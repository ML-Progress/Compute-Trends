{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCD_trends_ML_visualization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZL__N9PWIeR"
      },
      "source": [
        "# Visualization of Parameter, Compute and Data Trends in Machine Learning\n",
        "\n",
        "This is an accompanying notebook to [this dataset](https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0).\n",
        "\n",
        "Please cite us as *Parameter, Compute and Data Trends in Machine Learning* \n",
        "by Jaime Sevilla, Pablo Villalobos, Juan Felipe CerÃ³n, Matthew Burtell, Lennart Heim, Amogh B. Nanjajjar, Anson Ho, Tamay Besiroglu and Marius Hobbhahn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RNjdCJ1Nd1I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "b9c985f1-29d9-4b1d-9980-5ae8575125c2",
        "cellView": "form"
      },
      "source": [
        "#@title Retrieve data and plot interactive graph\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "import altair as alt\n",
        "\n",
        "\n",
        "## PARAMETERS\n",
        "x_axis = 'Publication date' #@param ['Publication date', 'Parameters', 'Training compute (FLOPs)', 'Inference compute (FLOPs)', 'Training compute per parameter (FLOPs)', 'Training compute times parameters']\n",
        "y_axis = 'Training compute (FLOPs)' #@param ['Parameters', 'Training compute (FLOPs)', 'Inference compute (FLOPs)', 'Training compute per parameter (FLOPs)', 'Training compute times parameters', 'Training cost (2020 USD)']\n",
        "date_start = '1950-01-01'# '2017-12-06' # '2012-09-29'  #'2009-12-30' #  2015-09-01' # #@param \n",
        "date_end = '2022-02-20' # '2017-12-31'# #@param \n",
        "\n",
        "#@markdown Era options\n",
        "start_dl_era = '2009-12-31' # \"2012-09-30\"# #@param \n",
        "start_large_scale_era = '2012-09-30' # '2017-12-06' #'2015-09-01' # #@param\n",
        "split_dl_era = True #@param {'type':'boolean'}\n",
        "split_large_scale_era = True #@param {'type':'boolean'}\n",
        "\n",
        "#@markdown Data options\n",
        "citation_threshold = 0 #@param\n",
        "separate_categories = False #@param {'type':'boolean'}\n",
        "other_domain_threshold = 10\n",
        "outliers_action = 'label' #@param ['ignore', 'label', 'remove']\n",
        "large_scale_action = 'ignore' #@param ['ignore', 'label', 'isolate']\n",
        "big_alphago_action = 'ignore' #@param ['ignore', 'label', 'remove']\n",
        "record_setters_action = 'label' #@param ['ignore', 'label', 'isolate']\n",
        "\n",
        "#@markdown Choose Z-value thresholds to automatically detect outliers\n",
        "low_outliers_z_value_threshold = -2 #@param\n",
        "high_outliers_z_value_threshold = 0.76 #@param\n",
        "outlier_window_size = 2 #@param\n",
        "\n",
        "#@markdown Bootstrapping options\n",
        "bootstrap_sample_size = 1000 #@param\n",
        "adjust_for_estimate_uncertainty = True #@param {'type':'boolean'}\n",
        "\n",
        "#@markdown Visualization options \n",
        "label_points = False #@param {'type' : 'boolean'}\n",
        "plot_regressions = True #@param {'type' : 'boolean'}\n",
        "label_eras = True #@param {'type' : 'boolean'}\n",
        "\n",
        "## CODE\n",
        "def make_visualization(\n",
        "    x_axis = 'Publication date',\n",
        "    y_axis = 'Training compute (FLOPs)',\n",
        "    date_start = '1950-01-01',\n",
        "    date_end = '2022-02-01',\n",
        "\n",
        "    # Era options\n",
        "    start_dl_era = '2009-12-31',\n",
        "    start_large_scale_era = '2015-09-01',\n",
        "    split_dl_era = True,\n",
        "    split_large_scale_era = True,\n",
        "\n",
        "    # Data options\n",
        "    citation_threshold = 0,\n",
        "    separate_categories = False,\n",
        "    other_domain_threshold = 10,\n",
        "    outliers_action = 'remove',\n",
        "    large_scale_action = 'label',\n",
        "    big_alphago_action = 'label',\n",
        "    record_setters_action = 'ignore',\n",
        "    low_outliers_z_value_threshold = -2,\n",
        "    high_outliers_z_value_threshold = 0.76,\n",
        "    outlier_window_size = 2,\n",
        "\n",
        "    # Bootstrapping options\n",
        "    bootstrap_sample_size = 500,\n",
        "    adjust_for_estimate_uncertainty = True,\n",
        "\n",
        "    # Visualization options \n",
        "    label_points = False,\n",
        "    plot_regressions = True,\n",
        "    label_eras = False,\n",
        "  ):\n",
        "\n",
        "  # Preprocess parameters\n",
        "  date_start = pd.to_datetime(date_start)\n",
        "  date_end = pd.to_datetime(date_end)\n",
        "  start_dl_era = np.datetime64(start_dl_era)\n",
        "  start_large_scale_era = np.datetime64(start_large_scale_era)\n",
        "\n",
        "  df = make_df(x_axis, y_axis, date_start, date_end, citation_threshold, separate_categories, other_domain_threshold)\n",
        "\n",
        "  # Filter outliers\n",
        "  df = filter_outliers(\n",
        "    df, x_axis, y_axis,\n",
        "\n",
        "    outliers_action, large_scale_action, \n",
        "    big_alphago_action, record_setters_action,\n",
        "\n",
        "    low_outliers_z_value_threshold, high_outliers_z_value_threshold,\n",
        "    outlier_window_size, start_large_scale_era\n",
        "    )\n",
        "\n",
        "  # Adjust date start and date end based on available data\n",
        "  date_start = df['Publication date'].min()\n",
        "  date_end = df['Publication date'].max()\n",
        "\n",
        "  # Make dataframe with era information\n",
        "  eras_df = make_eras_df(date_start, start_dl_era, start_large_scale_era, date_end)\n",
        "\n",
        "  # Add eras info to dataset\n",
        "  df = add_era_info(df, eras_df, split_dl_era, split_large_scale_era)\n",
        "\n",
        "  # Regress on each domain\n",
        "  df_reg, df_results =\\\n",
        "    regress_data(df, eras_df, x_axis, y_axis, bootstrap_sample_size, adjust_for_estimate_uncertainty)\n",
        "\n",
        "  # Make charts\n",
        "  chart =\\\n",
        "    make_chart(\n",
        "      df, \n",
        "      eras_df,\n",
        "      df_reg, \n",
        "      x_axis, \n",
        "      y_axis, \n",
        "      date_start,\n",
        "      date_end,\n",
        "      label_points, \n",
        "      plot_regressions, \n",
        "      label_eras,\n",
        "      separate_categories,\n",
        "      )\n",
        "\n",
        "  # Return results\n",
        "  return chart, df_results\n",
        "\n",
        "def make_df(\n",
        "    x_axis,\n",
        "    y_axis,\n",
        "    date_start, \n",
        "    date_end, \n",
        "    citation_threshold, \n",
        "    separate_categories,\n",
        "    other_domain_threshold,\n",
        "    ):\n",
        "  # Download dataset\n",
        "  df = pd.read_csv('https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/export?format=csv#gid=0')\n",
        "\n",
        "  # Preprocess columns\n",
        "  df['Publication date'] = pd.to_datetime(df['Publication date'], \n",
        "                                          errors='coerce', \n",
        "                                          dayfirst=True)\n",
        "\n",
        "  df['Training compute per parameter (FLOPs)'] = \\\n",
        "    df['Training compute (FLOPs)'] / df['Parameters']\n",
        "\n",
        "  df['Training compute times parameters'] = \\\n",
        "    df['Training compute (FLOPs)'] * df['Parameters']\n",
        "\n",
        "  # Filter systems before and after date threshold\n",
        "  df = df[df['Publication date'] > date_start]\n",
        "  df = df[df['Publication date'] < date_end]\n",
        "\n",
        "  # Filter systems below citation threshold\n",
        "  df['Citations'] = df['Citations'].fillna(0)\n",
        "  df = df[df['Citations'] >= citation_threshold]\n",
        "  #df = df[(df['Citations'] >= citation_treshold) | df['Citations'].isna()]\n",
        "\n",
        "  # Mask nans\n",
        "  mask = df[x_axis].isna() | df[y_axis].isna() | (df[y_axis] == 0)\n",
        "  df = df[~mask]\n",
        "\n",
        "  # If not separating categories, delete all domain information\n",
        "  if not separate_categories:\n",
        "    df['Domain'] = \"All\"\n",
        "\n",
        "  # Recode low count categories as \"other\"\n",
        "  vc = df['Domain'].value_counts()\n",
        "  sparse_domains = [domain for domain in vc.index if vc[domain]<other_domain_threshold]\n",
        "  mask = df['Domain'].isin(sparse_domains)\n",
        "  df.loc[mask,('Domain',)] = 'Other'\n",
        "\n",
        "  return df\n",
        "\n",
        "def filter_outliers(\n",
        "    df, \n",
        "    x_axis,\n",
        "    y_axis,\n",
        "\n",
        "    outliers_action, \n",
        "    large_scale_action, \n",
        "    big_alphago_action,\n",
        "    record_setters_action,\n",
        "\n",
        "    low_outliers_z_value_threshold,\n",
        "    high_outliers_z_value_threshold,\n",
        "    outlier_window_size,\n",
        "    start_large_scale_era,\n",
        "    ):\n",
        "  \"\"\"\n",
        "  Label, isolate or ignore outliers of various kinds\n",
        "  \"\"\"\n",
        "  outliers_idx = set()\n",
        "  large_scale_idx = set()\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    # Filter entries in a 3-year window around the paper\n",
        "    window_size = pd.Timedelta(f'{outlier_window_size*52*7} days')\n",
        "    half_window_size = window_size / 2\n",
        "    mask = ( row['Publication date'] - half_window_size <= df['Publication date'] ) &\\\n",
        "          ( df['Publication date'] <= row['Publication date'] + half_window_size )\n",
        "    window_df = df[mask].copy()\n",
        "\n",
        "    if len(window_df) < 2: continue\n",
        "\n",
        "    for axis in (x_axis, y_axis):\n",
        "      if axis == 'Publication date': continue\n",
        "      window_df[f'{axis} z scores'] = stats.zscore(np.log10(window_df[axis].values))\n",
        "      if window_df.loc[index, f'{axis} z scores'] < low_outliers_z_value_threshold: \n",
        "        outliers_idx.add(index)\n",
        "      if window_df.loc[index, f'{axis} z scores'] > high_outliers_z_value_threshold: \n",
        "        large_scale_idx.add(index)\n",
        "\n",
        "  # Drop low-scale outliers\n",
        "  if outliers_action == 'remove':\n",
        "    df = df.drop(outliers_idx)\n",
        "  elif outliers_action == 'label':\n",
        "    df.loc[outliers_idx, \"Domain\"] = \"Outlier\"\n",
        "\n",
        "  # Drop large-scale outliers\n",
        "  large_scale_mask = df.index.isin(large_scale_idx) & \\\n",
        "                    (df['Publication date'] > start_large_scale_era)\n",
        "  if large_scale_action == 'label':\n",
        "    df.loc[large_scale_mask, \"Domain\"] = \"Large Scale\"\n",
        "  if large_scale_action == 'isolate':\n",
        "    df = df[large_scale_mask]\n",
        "    df[\"Domain\"] = \"Large Scale\"\n",
        "\n",
        "  # Drop alphago zero\n",
        "  if big_alphago_action == 'remove':\n",
        "    df = df[~(df['System'] == \"AlphaGo Zero\")]\n",
        "    df = df[~(df['System'] == \"AlphaGo Master\")]\n",
        "  elif big_alphago_action == 'label':\n",
        "    df.loc[df['System'] == \"AlphaGo Zero\", \"Domain\"] = 'AlphaGo Zero'\n",
        "    df.loc[df['System'] == \"AlphaGo Master\", \"Domain\"] = 'AlphaGo Zero'\n",
        "    \n",
        "  # Filter record setter models\n",
        "  record_setters_mask =\\\n",
        "    df.apply(lambda row : row[y_axis] >= df[(df['Publication date'] <= row['Publication date']) & \n",
        "                                             (df['Domain'] == row['Domain'])\n",
        "                                            ][y_axis].max(), axis = 1)\n",
        "  if record_setters_action == 'isolate':\n",
        "    df = df[record_setters_mask]\n",
        "  elif record_setters_action == 'label':\n",
        "    df.loc[record_setters_mask, \"Domain\"] = \"Record\"\n",
        "  \n",
        "  return df\n",
        "\n",
        "def make_eras_df(date_start, start_dl_era, start_large_scale_era, date_end):\n",
        "  eras = [\n",
        "    ('Pre Deep Learning Era', date_start, start_dl_era),\n",
        "    (    'Deep Learning Era', start_dl_era, start_large_scale_era),\n",
        "    (      'Large Scale Era', start_large_scale_era, date_end),\n",
        "  ]\n",
        "  eras_df = pd.DataFrame(eras, columns = ['Era', 'start', 'stop'])\n",
        "\n",
        "  ## Remove eras outside of the considered timespan\n",
        "  mask =\\\n",
        "  (eras_df['stop']  > date_start) & \\\n",
        "  (eras_df['start'] < date_end)\n",
        "  eras_df = eras_df[mask]\n",
        "\n",
        "  ## Modify eras start-stop to fit the considered timespan\n",
        "  eras_df.loc[eras_df['start']  < date_start, 'start'] = date_start\n",
        "  eras_df.loc[eras_df['stop']   > date_end, 'stop']  = date_end\n",
        "\n",
        "  return eras_df\n",
        "\n",
        "def find_era(eras_df, date):\n",
        "    \"\"\" Find the era that corresponds to a given date\n",
        "    \"\"\"\n",
        "    mask = (eras_df['start'] <= date) & (date <= eras_df['stop'])\n",
        "    era = eras_df[mask]['Era'].iloc[0]\n",
        "    return era\n",
        "\n",
        "def add_era_info(df, eras_df, split_dl_era, split_large_scale_era):\n",
        "  # Add era labels to each domain\n",
        "  if split_dl_era:\n",
        "    df['Era'] = df['Publication date'].apply(lambda date: find_era(eras_df, date))\n",
        "  else:\n",
        "    df['Era'] = 'Machine Learning Era'\n",
        "\n",
        "  ## Merge large scale and DL era\n",
        "  if not split_large_scale_era:\n",
        "    mask = df['Era'] == 'Large Scale Era'\n",
        "    df.loc[mask, 'Era'] = 'Deep Learning Era'\n",
        "\n",
        "  ## Make the era label categorical so it can be sorted\n",
        "  df['Era'] = pd.Categorical(df['Era'], eras_df['Era'])\n",
        "\n",
        "  return df\n",
        "\n",
        "def julian_date_to_datetime(jd):\n",
        "  # https://stackoverflow.com/questions/41154423/convert-from-to-julian-date-to-np-datetime64\n",
        "  epoch = pd.to_datetime(0, unit='s').to_julian_date()\n",
        "  dt = pd.to_datetime(jd - epoch, unit='D')\n",
        "  return dt.strftime('%Y-%m-%d')\n",
        "julian_date_to_datetime = np.vectorize(julian_date_to_datetime)\n",
        "\n",
        "def regress_data(\n",
        "    df, \n",
        "    eras_df, \n",
        "    x_axis,\n",
        "    y_axis,\n",
        "    bootstrap_sample_size,\n",
        "    adjust_for_estimate_uncertainty\n",
        "    ):\n",
        "  \"\"\" Prepares a dataframe with interpolations of the data\n",
        "      and a dataset of summary statistics of the interpolations\n",
        "  \"\"\"\n",
        "  regression_data = []\n",
        "  results = []\n",
        "  for (era, domain), group in df.groupby(['Era', 'Domain'], sort=True):\n",
        "    # Extract data matrices\n",
        "    if x_axis == 'Publication date':\n",
        "      X = group['Publication date'].apply(lambda x : x.to_julian_date())\n",
        "    else:\n",
        "      X = group[x_axis]\n",
        "    X = X.values.reshape(-1, 1)\n",
        "    Y = group[y_axis].values\n",
        "\n",
        "    # Preprocessing\n",
        "    if x_axis != 'Publication date':\n",
        "      X = np.log10(X) \n",
        "    Y = np.log10(Y)\n",
        "\n",
        "    assert not np.any(np.isnan(X)) and not np.any(np.isnan(Y))\n",
        "\n",
        "    if len(X) <= 2: continue\n",
        "\n",
        "    # Fit linear model\n",
        "    model = linear_model.LinearRegression().fit(X, Y)\n",
        "    \n",
        "    # Bootstrap\n",
        "    \n",
        "    bootstrap_slopes = []\n",
        "    bootstrap_start_values = []\n",
        "    bootstrap_end_values = []\n",
        "    if x_axis == 'Publication date': \n",
        "      bootstrap_doubling_times = []\n",
        "    for bootstrap in range(bootstrap_sample_size):\n",
        "      idx_bootstrap = np.random.choice(range(len(X)), size = len(X), replace=True)\n",
        "      # We need at least 3 distinct points to do a linear regression\n",
        "      if len(np.unique(idx_bootstrap)) <= 2: continue\n",
        "      X_bootstrap = X[idx_bootstrap].reshape(-1, 1)\n",
        "      Y_bootstrap = Y[idx_bootstrap]\n",
        "      assert X_bootstrap.shape == X.shape\n",
        "      assert Y_bootstrap.shape == Y.shape\n",
        "      if adjust_for_estimate_uncertainty:\n",
        "        noise = np.random.uniform(np.log10(1/2), np.log10(2), size=Y.shape)\n",
        "        Y_bootstrap += noise\n",
        "      bootstrap_model = linear_model.LinearRegression().fit(X_bootstrap, Y_bootstrap)\n",
        "\n",
        "      ## Append slope\n",
        "      bootstrap_slopes.append(bootstrap_model.coef_[0])\n",
        "\n",
        "      ## Append doubling time\n",
        "      if x_axis == 'Publication date':\n",
        "        DAYS_PER_MONTH = 30\n",
        "        bootstrap_doubling_times.append(np.log10(2) / bootstrap_model.coef_[0] / DAYS_PER_MONTH)\n",
        "      \n",
        "      ## Append start and end value\n",
        "      y_start = 10**bootstrap_model.predict(X.min().reshape(1, -1))[0]\n",
        "      y_end = 10**bootstrap_model.predict(X.max().reshape(1, -1))[0]\n",
        "      bootstrap_start_values.append(y_start)\n",
        "      bootstrap_end_values.append(y_end)\n",
        "\n",
        "    # Collect information about the fit\n",
        "    row = {}\n",
        "    row['Trend'] = f\"{domain} {era}\"\n",
        "    row['n'] = len(X)\n",
        "\n",
        "    ## Start and end value\n",
        "    low_q = 0.025\n",
        "    median_q = 0.5\n",
        "    high_q = 0.975\n",
        "    \n",
        "    y_start = 10**model.predict(X.min().reshape(1, -1))[0]\n",
        "    y_start_low = np.quantile(bootstrap_start_values, q=low_q)\n",
        "    y_start_median = np.quantile(bootstrap_start_values, q=median_q)\n",
        "    y_start_high = np.quantile(bootstrap_start_values, q=high_q)\n",
        "\n",
        "    y_end = 10**model.predict(X.max().reshape(1, -1))[0]\n",
        "    y_end_low = np.quantile(bootstrap_end_values, q=low_q)\n",
        "    y_end_median = np.quantile(bootstrap_end_values, q=median_q)\n",
        "    y_end_high = np.quantile(bootstrap_end_values, q=high_q)\n",
        "\n",
        "    # row['start'] = f\"{y_start:1.0e} [{y_start_low:1.0e} ; {y_start_median:1.0e} ; {y_start_high:1.0e}]\"                       \n",
        "    # row['end'] =   f\"{y_end  :1.0e} [{y_end_low  :1.0e} ; {y_end_median  :1.0e} ; {y_end_high  :1.0e}]\"\n",
        "    row['Scale (start / end)'] = f'{y_start:1.0e} / {y_end:1.0e}'\n",
        "\n",
        "    ## Slope\n",
        "    best_slope = model.coef_[0]\n",
        "    low_slope = np.quantile(bootstrap_slopes, q=low_q)\n",
        "    median_slope = np.quantile(bootstrap_slopes, q=median_q)\n",
        "    high_slope = np.quantile(bootstrap_slopes, q=high_q)\n",
        "    \n",
        "    if x_axis == 'Publication date':\n",
        "      DAYS_PER_YEAR = 365\n",
        "      best_slope *= DAYS_PER_YEAR\n",
        "      low_slope *= DAYS_PER_YEAR\n",
        "      median_slope *= DAYS_PER_YEAR\n",
        "      high_slope *= DAYS_PER_YEAR\n",
        "      slope_summary = f\"{best_slope:0.1f} OOMs/year [{low_slope:0.1f} ; {median_slope:0.1f} ; {high_slope:0.1f}]\"  \n",
        "    else:\n",
        "      slope_summary = f\"{best_slope:0.1e} [{low_slope:0.1e} ; {median_slope:0.1e} ; {high_slope:0.1e}]\"\n",
        "    \n",
        "    row[\"Slope\"] = slope_summary\n",
        "\n",
        "    ## Doubling time\n",
        "    if x_axis == 'Publication date': \n",
        "      best_doubling_time = np.log10(2) / model.coef_[0] / DAYS_PER_MONTH\n",
        "      low_doubling_time = np.quantile(bootstrap_doubling_times, q=low_q)\n",
        "      median_doubling_time = np.quantile(bootstrap_doubling_times, q=median_q)\n",
        "      high_doubling_time = np.quantile(bootstrap_doubling_times, q=high_q)\n",
        "      doubling_time_summary = f\"{best_doubling_time:0.1f} months [{low_doubling_time:0.1f} ; {median_doubling_time:0.1f} ; {high_doubling_time:0.1f}]\"\n",
        "      row[\"Doubling time\"] = doubling_time_summary \n",
        "    results.append(row)\n",
        "\n",
        "    ## R2\n",
        "    row[\"R2\"] = f\"{model.score(X,Y):0.2f}\"\n",
        "\n",
        "    # Extract predictions\n",
        "    if x_axis == 'Publication date':\n",
        "      # Stretch datapoints to cover the corresponding era\n",
        "      eps = 10\n",
        "      min_t = julian_date_to_datetime(X.min()+eps)\n",
        "      min_t = pd.to_datetime(min_t)\n",
        "      min_era = eras_df[(eras_df['start'] <= min_t) & (min_t <= eras_df['stop'])].iloc[0]\n",
        "      max_t = julian_date_to_datetime(X.max()-eps)\n",
        "      max_t = pd.to_datetime(max_t)\n",
        "      max_era = eras_df[(eras_df['start'] <= max_t) & (max_t <= eras_df['stop'])].iloc[0]\n",
        "      x_pred = np.linspace(min_era['start'].to_julian_date(), max_era['stop'].to_julian_date(),200)\n",
        "    else:\n",
        "      x_pred = np.linspace(min(X), max(X))\n",
        "    y_pred = model.predict(x_pred.reshape(-1, 1))\n",
        "    # Postprocessing\n",
        "    if x_axis == 'Publication date': \n",
        "      x_pred = julian_date_to_datetime(x_pred)\n",
        "    else:\n",
        "      x_pred = 10.0**x_pred\n",
        "\n",
        "    y_pred = 10.0**y_pred\n",
        "\n",
        "    # Store in dataframe\n",
        "    regression_data += [{x_axis : x,\n",
        "                        y_axis : y,\n",
        "                        'Domain' : domain} \n",
        "                        for x,y in zip(x_pred, y_pred)]\n",
        "\n",
        "  df_results = pd.DataFrame(results)\n",
        "  df_reg = pd.DataFrame(regression_data)\n",
        "\n",
        "  return df_reg, df_results\n",
        "\n",
        "# plot the dataset, referencing dataframe column names\n",
        "def make_chart(\n",
        "    df, \n",
        "    eras_df,\n",
        "    df_reg, \n",
        "    x_axis, \n",
        "    y_axis, \n",
        "    date_start,\n",
        "    date_end,\n",
        "    label_points, \n",
        "    plot_regressions, \n",
        "    label_eras,\n",
        "    separate_categories,\n",
        "    log_axis=True,\n",
        "    ):\n",
        "  \"\"\" Makes an altair chart of the data\n",
        "  \"\"\"\n",
        "  alt.themes.enable('fivethirtyeight')\n",
        "  selection = alt.selection_multi(fields=['Domain'], bind='legend')\n",
        "\n",
        "  domain_key =  [\n",
        "      ('Vision', '#6d904f', 'cross'), \n",
        "      ('Language', '#b96db8', 'square'),   \n",
        "      ('Games', '#30a2da', 'circle'),\n",
        "      ('Drawing', '#8b8b8b', 'M0,.5L.6,.8L.5,.1L1,-.3L.3,-.4L0,-1L-.3,-.4L-1,-.3L-.5,.1L-.6,.8L0,.5Z'),\n",
        "      ('Speech', '#ff9e27', 'triangle-down'),\n",
        "      ('Other', '#e5ae38', 'diamond'), \n",
        "      ('Large Scale', '#fc4f30', 'triangle'),     \n",
        "      ('All', '#30a2da', 'circle'),\n",
        "      ('Outlier', '#56cc60', 'triangle-left'),\n",
        "      ('AlphaGo Zero', '#ff9e27', 'M0,.5L.6,.8L.5,.1L1,-.3L.3,-.4L0,-1L-.3,-.4L-1,-.3L-.5,.1L-.6,.8L0,.5Z'),\n",
        "      ('Record', '#ff9e27', 'M0,.5L.6,.8L.5,.1L1,-.3L.3,-.4L0,-1L-.3,-.4L-1,-.3L-.5,.1L-.6,.8L0,.5Z')\n",
        "  ]\n",
        "  domain_df = pd.DataFrame(domain_key, columns = ['Domain', 'Color', 'Shape'])\n",
        "\n",
        "  # Filter domains that are present \n",
        "  mask = domain_df['Domain'].apply(lambda domain : domain in df['Domain'].unique())\n",
        "  domain_df = domain_df[mask]\n",
        "\n",
        "  color_scale = alt.Scale(\n",
        "          domain=domain_df['Domain'].values,\n",
        "          range=domain_df['Color'].values,\n",
        "          )\n",
        "\n",
        "  shape_scale = alt.Scale(\n",
        "      domain= domain_df['Domain'].values,\n",
        "      range=domain_df['Shape'].values,\n",
        "  )\n",
        "\n",
        "  make_domain_color = lambda legend : alt.Color(\n",
        "      'Domain', \n",
        "      legend= legend,\n",
        "      scale= color_scale\n",
        "      )\n",
        "\n",
        "  ## Chart with historical data\n",
        "  chart = alt.Chart(df, width=1100, height=600,)\\\n",
        "  .properties(\n",
        "      title={\n",
        "        \"text\": [\n",
        "                f\"{y_axis} of milestone Machine Learning systems over time\"\\\n",
        "                if x_axis == 'Publication date' else\\\n",
        "                f\"{y_axis} vs {x_axis} of milestone Machine Learning systems\"\n",
        "                ], \n",
        "        \"subtitle\": [f\"n = {len(df)}\"]\n",
        "      }\n",
        "  )\\\n",
        "  .mark_point(size=120, filled=False)\\\n",
        "  .encode(\n",
        "    x=alt.X(f'{x_axis}:{\"T\" if x_axis == \"Publication date\" else \"Q\"}',\n",
        "            scale=alt.Scale(type='time' if x_axis == \"Publication date\" else 'linear' if not log_axis else 'log', \n",
        "                            domain=(df[x_axis].min(), df[x_axis].max())), \n",
        "            axis=alt.Axis(\n",
        "                format=\"%Y\" if x_axis == \"Publication date\" else \".1e\",\n",
        "\n",
        "                values = [pd.to_datetime(f'{year}-01-01') for year in range(date_start.year, date_end.year + 2)] \\\n",
        "                        if x_axis == \"Publication date\" else alt.Undefined\n",
        "                )\n",
        "            ),\n",
        "    y=alt.Y(f'{y_axis}:Q',\n",
        "            scale=alt.Scale(type='log' if log_axis else 'linear', \n",
        "                            domain=(df[y_axis].min(), df[y_axis].max())), \n",
        "            axis=alt.Axis(format=\"e\", grid = True)\n",
        "            ),\n",
        "    color= make_domain_color(legend = None if not separate_categories else alt.Legend()),\n",
        "    shape = alt.Shape('Domain', \n",
        "                      legend = None if not separate_categories else alt.Legend(),\n",
        "                      scale = shape_scale,\n",
        "                      ),\n",
        "    tooltip=['System', \n",
        "            'Reference', \n",
        "            'Publication date', \n",
        "            alt.Tooltip('Parameters', format=\".1e\"), \n",
        "            alt.Tooltip('Training compute (FLOPs)', format=\".1e\"), \n",
        "            alt.Tooltip('Inference compute (FLOPs)', format=\".1e\"), \n",
        "            'Domain'\n",
        "            ],\n",
        "    opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
        "  )\n",
        "\n",
        "  ## Labels\n",
        "  if label_points:\n",
        "\n",
        "    # Define text constructor\n",
        "    make_text = lambda df_slice, text_align: alt.Chart(df_slice).mark_text(\n",
        "        align='left' if text_align == 'right' else 'right',\n",
        "        baseline='middle',\n",
        "        dx=7 if text_align == 'right' else -7\n",
        "    ).encode(\n",
        "        x=alt.X(f'{x_axis}:{\"T\" if x_axis == \"Publication date\" else \"Q\"}',),\n",
        "        y=alt.Y(f'{y_axis}:Q',),\n",
        "        text='System',\n",
        "        color=make_domain_color(legend=None),\n",
        "        opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
        "    )\n",
        "\n",
        "    # Label left and right points appropriately\n",
        "    midpoint = pd.to_datetime(f\"{(date_start.year + date_end.year) // 2}-01-01\")\n",
        "    left_text = make_text(df[df['Publication date'] < midpoint],'right')\n",
        "    right_text = make_text(df[df['Publication date'] > midpoint],'left')\n",
        "\n",
        "    # Combine with main chart\n",
        "    chart = (chart + left_text + right_text)\n",
        "\n",
        "  ## Eras\n",
        "\n",
        "  if x_axis == 'Publication date' and label_eras:\n",
        "    # Define era colors\n",
        "    era_color = alt.Color(\n",
        "        'Era', \n",
        "        legend = None, \n",
        "        scale=alt.Scale(\n",
        "            domain=['Pre Deep Learning Era', 'Deep Learning Era', 'Large Scale Era'],\n",
        "            range=['#e5ae38', '#30a2da', '#fc4f30']\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    # Create era backgrounds\n",
        "    eras_chart = alt.Chart(\n",
        "        eras_df\n",
        "    ).mark_rect(\n",
        "        opacity=0.1\n",
        "    ).encode(\n",
        "        x=alt.X('start', title = x_axis),\n",
        "        x2=alt.X2('stop'),\n",
        "        y=alt.value(0),  # pixels from top\n",
        "        y2=alt.value(1000),  # pixels from top\n",
        "        color=era_color\n",
        "    )\n",
        "\n",
        "    # Era label constructor\n",
        "    make_era_labels = lambda df_slice, mode : \\\n",
        "      alt.Chart(df_slice).mark_text(\n",
        "          align='right' if mode == 'high' else 'left',\n",
        "          baseline='top',\n",
        "          size = 18,\n",
        "          fontWeight='bold',\n",
        "          angle = 270,\n",
        "          dx = -20,#120,\n",
        "          dy=20,#25\n",
        "      ).encode(\n",
        "        text='Era',\n",
        "        x=alt.X('start', title = x_axis),\n",
        "        y=alt.value(0 if mode ==\"high\" else 570),  # pixels from top\n",
        "        color=era_color\n",
        "      )\n",
        "\n",
        "    # Instantiate left and right era labels\n",
        "    midpoint = pd.to_datetime(f\"{(date_start.year + date_end.year) // 2}-01-01\")\n",
        "    left_era_labels = make_era_labels(eras_df[eras_df['start'] < midpoint], \"high\")\n",
        "    right_era_labels = make_era_labels(eras_df[eras_df['start'] > midpoint], \"low\")\n",
        "\n",
        "    # Combine with main chart\n",
        "    chart = (chart + eras_chart + left_era_labels + right_era_labels)\n",
        "\n",
        "  ## Chart with regressions\n",
        "  if plot_regressions:\n",
        "    reg_chart = alt.Chart(df_reg).mark_line(point=False, strokeDash=[10,5], clip=True)\\\n",
        "    .encode(\n",
        "        x=f'{x_axis}:{\"T\" if x_axis == \"Publication date\" else \"Q\"}',\n",
        "        y=y_axis,\n",
        "        opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
        "        color=make_domain_color(legend=None),\n",
        "        )\n",
        "    chart = (chart + reg_chart)\n",
        "\n",
        "  # Final touches\n",
        "  chart = chart\\\n",
        "  .add_selection(selection)\\\n",
        "  .configure_title(fontSize=42)\\\n",
        "  .configure(background='#FFFFFF')\\\n",
        "  .configure_axis(\n",
        "      labelFontSize=20,titleFontSize=26)\\\n",
        "  .configure_legend(\n",
        "      titleFontSize=20,\n",
        "      labelFontSize =18,\n",
        "      gradientLength=400,\n",
        "      gradientThickness=30,\n",
        "      symbolSize = 130,)\\\n",
        "  .resolve_scale(shape='independent', color='independent')\\\n",
        "  .interactive()\n",
        "\n",
        "  return chart\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "chart, df_results = \\\n",
        "  make_visualization(\n",
        "      x_axis,\n",
        "      y_axis,\n",
        "      date_start,\n",
        "      date_end,\n",
        "\n",
        "      # Era options\n",
        "      start_dl_era,\n",
        "      start_large_scale_era,\n",
        "      split_dl_era,\n",
        "      split_large_scale_era,\n",
        "\n",
        "      # Data options\n",
        "      citation_threshold,\n",
        "      separate_categories,\n",
        "      other_domain_threshold,\n",
        "      outliers_action,\n",
        "      large_scale_action,\n",
        "      big_alphago_action,\n",
        "      record_setters_action,\n",
        "      low_outliers_z_value_threshold,\n",
        "      high_outliers_z_value_threshold,\n",
        "      outlier_window_size,\n",
        "\n",
        "      # Bootstrapping options\n",
        "      bootstrap_sample_size,\n",
        "      adjust_for_estimate_uncertainty,\n",
        "\n",
        "      # Visualization options \n",
        "      label_points,\n",
        "      plot_regressions,\n",
        "      label_eras,\n",
        "  )\n",
        "\n",
        "display(df_results)\n",
        "chart"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f1c3e3e2-0bc4-4946-9608-b139f0f98f2e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Trend</th>\n",
              "      <th>n</th>\n",
              "      <th>Scale (start / end)</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Doubling time</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>All Pre Deep Learning Era</td>\n",
              "      <td>9</td>\n",
              "      <td>4e+03 / 1e+14</td>\n",
              "      <td>0.2 OOMs/year [0.2 ; 0.2 ; 0.3]</td>\n",
              "      <td>17.5 months [12.2 ; 17.3 ; 21.9]</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Record Pre Deep Learning Era</td>\n",
              "      <td>10</td>\n",
              "      <td>1e+05 / 3e+14</td>\n",
              "      <td>0.2 OOMs/year [0.1 ; 0.2 ; 0.2]</td>\n",
              "      <td>19.9 months [14.7 ; 20.3 ; 30.9]</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All Deep Learning Era</td>\n",
              "      <td>5</td>\n",
              "      <td>3e+14 / 7e+15</td>\n",
              "      <td>0.6 OOMs/year [0.1 ; 0.6 ; 2.5]</td>\n",
              "      <td>6.2 months [1.3 ; 6.0 ; 23.6]</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>All Large Scale Era</td>\n",
              "      <td>85</td>\n",
              "      <td>1e+16 / 4e+22</td>\n",
              "      <td>0.7 OOMs/year [0.6 ; 0.7 ; 0.9]</td>\n",
              "      <td>5.1 months [4.3 ; 5.1 ; 6.2]</td>\n",
              "      <td>0.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Record Large Scale Era</td>\n",
              "      <td>12</td>\n",
              "      <td>6e+18 / 9e+24</td>\n",
              "      <td>0.8 OOMs/year [0.6 ; 0.8 ; 1.6]</td>\n",
              "      <td>4.7 months [2.3 ; 4.6 ; 6.3]</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1c3e3e2-0bc4-4946-9608-b139f0f98f2e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1c3e3e2-0bc4-4946-9608-b139f0f98f2e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1c3e3e2-0bc4-4946-9608-b139f0f98f2e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                          Trend   n  ...                     Doubling time    R2\n",
              "0     All Pre Deep Learning Era   9  ...  17.5 months [12.2 ; 17.3 ; 21.9]  0.90\n",
              "1  Record Pre Deep Learning Era  10  ...  19.9 months [14.7 ; 20.3 ; 30.9]  0.83\n",
              "2         All Deep Learning Era   5  ...     6.2 months [1.3 ; 6.0 ; 23.6]  0.70\n",
              "3           All Large Scale Era  85  ...      5.1 months [4.3 ; 5.1 ; 6.2]  0.53\n",
              "4        Record Large Scale Era  12  ...      4.7 months [2.3 ; 4.6 ; 6.3]  0.78\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.LayerChart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-35f95f782ae24a14b314c8f4fb2d8a17\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-35f95f782ae24a14b314c8f4fb2d8a17\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-35f95f782ae24a14b314c8f4fb2d8a17\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"usermeta\": {\"embedOptions\": {\"theme\": \"fivethirtyeight\"}}, \"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 20, \"titleFontSize\": 26}, \"background\": \"#FFFFFF\", \"legend\": {\"gradientLength\": 400, \"gradientThickness\": 30, \"labelFontSize\": 18, \"symbolSize\": 130, \"titleFontSize\": 20}}, \"layer\": [{\"data\": {\"name\": \"data-644faba72efb326897c156d0852ff6dc\"}, \"mark\": {\"type\": \"point\", \"filled\": false, \"size\": 120}, \"encoding\": {\"color\": {\"field\": \"Domain\", \"legend\": null, \"scale\": {\"domain\": [\"All\", \"Record\"], \"range\": [\"#30a2da\", \"#ff9e27\"]}, \"type\": \"nominal\"}, \"opacity\": {\"condition\": {\"value\": 1, \"selection\": \"selector001\"}, \"value\": 0.2}, \"shape\": {\"field\": \"Domain\", \"legend\": null, \"scale\": {\"domain\": [\"All\", \"Record\"], \"range\": [\"circle\", \"M0,.5L.6,.8L.5,.1L1,-.3L.3,-.4L0,-1L-.3,-.4L-1,-.3L-.5,.1L-.6,.8L0,.5Z\"]}, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"System\", \"type\": \"nominal\"}, {\"field\": \"Reference\", \"type\": \"nominal\"}, {\"field\": \"Publication date\", \"type\": \"temporal\"}, {\"field\": \"Parameters\", \"format\": \".1e\", \"type\": \"quantitative\"}, {\"field\": \"Training compute (FLOPs)\", \"format\": \".1e\", \"type\": \"quantitative\"}, {\"field\": \"Inference compute (FLOPs)\", \"format\": \".1e\", \"type\": \"quantitative\"}, {\"field\": \"Domain\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"format\": \"%Y\", \"values\": [\"1952-01-01T00:00:00\", \"1953-01-01T00:00:00\", \"1954-01-01T00:00:00\", \"1955-01-01T00:00:00\", \"1956-01-01T00:00:00\", \"1957-01-01T00:00:00\", \"1958-01-01T00:00:00\", \"1959-01-01T00:00:00\", \"1960-01-01T00:00:00\", \"1961-01-01T00:00:00\", \"1962-01-01T00:00:00\", \"1963-01-01T00:00:00\", \"1964-01-01T00:00:00\", \"1965-01-01T00:00:00\", \"1966-01-01T00:00:00\", \"1967-01-01T00:00:00\", \"1968-01-01T00:00:00\", \"1969-01-01T00:00:00\", \"1970-01-01T00:00:00\", \"1971-01-01T00:00:00\", \"1972-01-01T00:00:00\", \"1973-01-01T00:00:00\", \"1974-01-01T00:00:00\", \"1975-01-01T00:00:00\", \"1976-01-01T00:00:00\", \"1977-01-01T00:00:00\", \"1978-01-01T00:00:00\", \"1979-01-01T00:00:00\", \"1980-01-01T00:00:00\", \"1981-01-01T00:00:00\", \"1982-01-01T00:00:00\", \"1983-01-01T00:00:00\", \"1984-01-01T00:00:00\", \"1985-01-01T00:00:00\", \"1986-01-01T00:00:00\", \"1987-01-01T00:00:00\", \"1988-01-01T00:00:00\", \"1989-01-01T00:00:00\", \"1990-01-01T00:00:00\", \"1991-01-01T00:00:00\", \"1992-01-01T00:00:00\", \"1993-01-01T00:00:00\", \"1994-01-01T00:00:00\", \"1995-01-01T00:00:00\", \"1996-01-01T00:00:00\", \"1997-01-01T00:00:00\", \"1998-01-01T00:00:00\", \"1999-01-01T00:00:00\", \"2000-01-01T00:00:00\", \"2001-01-01T00:00:00\", \"2002-01-01T00:00:00\", \"2003-01-01T00:00:00\", \"2004-01-01T00:00:00\", \"2005-01-01T00:00:00\", \"2006-01-01T00:00:00\", \"2007-01-01T00:00:00\", \"2008-01-01T00:00:00\", \"2009-01-01T00:00:00\", \"2010-01-01T00:00:00\", \"2011-01-01T00:00:00\", \"2012-01-01T00:00:00\", \"2013-01-01T00:00:00\", \"2014-01-01T00:00:00\", \"2015-01-01T00:00:00\", \"2016-01-01T00:00:00\", \"2017-01-01T00:00:00\", \"2018-01-01T00:00:00\", \"2019-01-01T00:00:00\", \"2020-01-01T00:00:00\", \"2021-01-01T00:00:00\", \"2022-01-01T00:00:00\", \"2023-01-01T00:00:00\"]}, \"field\": \"Publication date\", \"scale\": {\"domain\": [\"1952-01-01T00:00:00\", \"2022-01-20T00:00:00\"], \"type\": \"time\"}, \"type\": \"temporal\"}, \"y\": {\"axis\": {\"format\": \"e\", \"grid\": true}, \"field\": \"Training compute (FLOPs)\", \"scale\": {\"domain\": [40.0, 1.35e+24], \"type\": \"log\"}, \"type\": \"quantitative\"}}, \"height\": 600, \"selection\": {\"selector001\": {\"type\": \"multi\", \"fields\": [\"Domain\"], \"bind\": \"legend\"}, \"selector002\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": {\"text\": [\"Training compute (FLOPs) of milestone Machine Learning systems over time\"], \"subtitle\": [\"n = 123\"]}, \"width\": 1100}, {\"data\": {\"name\": \"data-1505ebec624515b18852e84cd646fcb0\"}, \"mark\": {\"type\": \"rect\", \"opacity\": 0.1}, \"encoding\": {\"color\": {\"field\": \"Era\", \"legend\": null, \"scale\": {\"domain\": [\"Pre Deep Learning Era\", \"Deep Learning Era\", \"Large Scale Era\"], \"range\": [\"#e5ae38\", \"#30a2da\", \"#fc4f30\"]}, \"type\": \"nominal\"}, \"x\": {\"field\": \"start\", \"title\": \"Publication date\", \"type\": \"temporal\"}, \"x2\": {\"field\": \"stop\"}, \"y\": {\"value\": 0}, \"y2\": {\"value\": 1000}}}, {\"data\": {\"name\": \"data-ed0c35b2fcb8cb62e6d6e06ec64a1081\"}, \"mark\": {\"type\": \"text\", \"align\": \"right\", \"angle\": 270, \"baseline\": \"top\", \"dx\": -20, \"dy\": 20, \"fontWeight\": \"bold\", \"size\": 18}, \"encoding\": {\"color\": {\"field\": \"Era\", \"legend\": null, \"scale\": {\"domain\": [\"Pre Deep Learning Era\", \"Deep Learning Era\", \"Large Scale Era\"], \"range\": [\"#e5ae38\", \"#30a2da\", \"#fc4f30\"]}, \"type\": \"nominal\"}, \"text\": {\"field\": \"Era\", \"type\": \"nominal\"}, \"x\": {\"field\": \"start\", \"title\": \"Publication date\", \"type\": \"temporal\"}, \"y\": {\"value\": 0}}}, {\"data\": {\"name\": \"data-e3f5f1d78994eed643e4fee58918f893\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"angle\": 270, \"baseline\": \"top\", \"dx\": -20, \"dy\": 20, \"fontWeight\": \"bold\", \"size\": 18}, \"encoding\": {\"color\": {\"field\": \"Era\", \"legend\": null, \"scale\": {\"domain\": [\"Pre Deep Learning Era\", \"Deep Learning Era\", \"Large Scale Era\"], \"range\": [\"#e5ae38\", \"#30a2da\", \"#fc4f30\"]}, \"type\": \"nominal\"}, \"text\": {\"field\": \"Era\", \"type\": \"nominal\"}, \"x\": {\"field\": \"start\", \"title\": \"Publication date\", \"type\": \"temporal\"}, \"y\": {\"value\": 570}}}, {\"data\": {\"name\": \"data-89aa337f71f6c54ca6ff78582f6ba4a7\"}, \"mark\": {\"type\": \"line\", \"clip\": true, \"point\": false, \"strokeDash\": [10, 5]}, \"encoding\": {\"color\": {\"field\": \"Domain\", \"legend\": null, \"scale\": {\"domain\": [\"All\", \"Record\"], \"range\": [\"#30a2da\", \"#ff9e27\"]}, \"type\": \"nominal\"}, \"opacity\": {\"condition\": {\"value\": 1, \"selection\": \"selector001\"}, \"value\": 0.2}, \"x\": {\"field\": \"Publication date\", \"type\": \"temporal\"}, \"y\": {\"field\": \"Training compute (FLOPs)\", \"type\": \"quantitative\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-644faba72efb326897c156d0852ff6dc\": [{\"System\": \"Theseus\", \"Domain\": \"Record\", \"Task\": \"Maze solving\", \"Organization(s)\": \"Bell Laboratories\", \"Author(s)\": \"Claude Shannon\", \"Publication date\": \"1952-01-01T00:00:00\", \"Year\": \"1952\", \"Reference\": \"Mighty Mouse\", \"Link\": \"https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 40.0, \"Training compute (FLOPs)\": 40.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": 1.0, \"Training compute times parameters\": 1600.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"Samuel Neural Checkers\", \"Domain\": \"Record\", \"Task\": \"Checkers\", \"Organization(s)\": \"IBM\", \"Author(s)\": \"Arthur L. Samuel\", \"Publication date\": \"1959-01-01T00:00:00\", \"Year\": \"1959\", \"Reference\": \"Some studies in machine learning using the game of checkers\", \"Link\": \"https://ieeexplore.ieee.org/abstract/document/5392560\", \"Citations\": 4150.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 16.0, \"Training compute (FLOPs)\": 428000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 9.0, \"Inference time (ms)\": \"7.5\", \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 26750000.0, \"Training compute times parameters\": 6848000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"ADALINE\", \"Domain\": \"All\", \"Task\": \"Pattern recognition\", \"Organization(s)\": \"Standford University\", \"Author(s)\": \"Widrow and Hoff\", \"Publication date\": \"1960-01-01T00:00:00\", \"Year\": \"1960\", \"Reference\": \"Adaptive switching circuits\", \"Link\": \"https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf\", \"Citations\": 6330.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 17.0, \"Training compute (FLOPs)\": 9900.0, \"Inference compute (FLOPs)\": 33.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"1.00E+02\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Median\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 582.3529411764706, \"Training compute times parameters\": 168300.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"Neocognitron\", \"Domain\": \"All\", \"Task\": \"Character recognition\", \"Organization(s)\": \"NHK Broadcasting Science Research Laboratories\", \"Author(s)\": \"K Fukushima, S Miyake\", \"Publication date\": \"1980-01-01T00:00:00\", \"Year\": \"1980\", \"Reference\": \"Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position\", \"Link\": \"https://link.springer.com/article/10.1007/BF00344251\", \"Citations\": 5780.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 1140000.0, \"Training compute (FLOPs)\": 228000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"5.00E+00\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 200.0, \"Training compute times parameters\": 259920000000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"System 11\", \"Domain\": \"All\", \"Task\": \"Face recognition\", \"Organization(s)\": \"Carnegie Mellon University\", \"Author(s)\": \"HA Rowley, S Baluja, T Kanade\", \"Publication date\": \"1996-01-01T00:00:00\", \"Year\": \"1996\", \"Reference\": \"Neural Network-Based Face Detection\", \"Link\": \"https://www.ri.cmu.edu/pub_files/pub1/rowley_henry_1996_3/rowley_henry_1996_3.pdf\", \"Citations\": 6010.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 6450.0, \"Training compute (FLOPs)\": 958000000.0, \"Inference compute (FLOPs)\": 12900.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"2.38E+04\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 148527.13178294574, \"Training compute times parameters\": 6179100000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"LSTM\", \"Domain\": \"Record\", \"Task\": \"Sequence recognition (?)\", \"Organization(s)\": \"The Technical University of Munich\", \"Author(s)\": \"Sepp Hochreiter ; Jurgen Schmidhuber\", \"Publication date\": \"1997-01-01T00:00:00\", \"Year\": \"1997\", \"Reference\": \"Long short-term memory\", \"Link\": \"http://www.bioinf.jku.at/publications/older/2604.pdf\", \"Citations\": 52000.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 10500.0, \"Training compute (FLOPs)\": 21000000000000.0, \"Inference compute (FLOPs)\": 42000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": \"Company\", \"Training compute per parameter (FLOPs)\": 2000000000.0, \"Training compute times parameters\": 2.205e+17, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"DLRM-2021\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook AI \", \"Author(s)\": \"D Mudigere, Y Hao, J Huang, A Tulloch\", \"Publication date\": \"2020-01-01T00:00:00\", \"Year\": \"2020\", \"Reference\": null, \"Link\": null, \"Citations\": 2.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1000000000000.0, \"Training compute (FLOPs)\": 3e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": \"Possibly first theoretical suggestion of NNs\", \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 300000000.0, \"Training compute times parameters\": 3e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"NEO (DL:RM-2022)\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook\", \"Author(s)\": \"D Mudigere, Y Hao, J Huang, A Tulloch\", \"Publication date\": \"2021-01-01T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models\", \"Link\": \"https://arxiv.org/abs/2104.05158\", \"Citations\": 2.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 3000000000000.0, \"Training compute (FLOPs)\": 1.1e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 366666666.6666667, \"Training compute times parameters\": 3.3e+33, \"Era\": \"Large Scale Era\"}, {\"System\": \"Primer\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Research, Brain Team\", \"Author(s)\": \"DavidR.So, WojciechMan \\u0301ke, HanxiaoLiu, ZihangDai, NoamShazeer, QuocV.Le\", \"Publication date\": \"2021-01-01T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Primer: Searching for Efficient Transformers for Language Modeling\", \"Link\": \"http://arxiv.org/abs/2109.08668\", \"Citations\": 13.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1900000000.0, \"Training compute (FLOPs)\": 7.1e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3736842105263.1577, \"Training compute times parameters\": 1.349e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"Perceptron Mark I\", \"Domain\": \"Record\", \"Task\": \"Binary classification\", \"Organization(s)\": \"Cornell Aeronautical Laboratory\", \"Author(s)\": \"F Rosenblatt\", \"Publication date\": \"1957-01-01T00:00:00\", \"Year\": \"1957\", \"Reference\": \"The Perceptron\\u2014a perceiving and recognizing automaton\", \"Link\": \"https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf\", \"Citations\": 1610.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Historical relevance\", \"Hidden layers\": null, \"Parameters\": 400.0, \"Training compute (FLOPs)\": 695000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1737.5, \"Training compute times parameters\": 278000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"Pandemonium (morse)\", \"Domain\": \"Record\", \"Task\": \"Morse translation\", \"Organization(s)\": \"Massachusetts Institute of Technology\", \"Author(s)\": \"OG Selfridge\", \"Publication date\": \"1959-02-01T00:00:00\", \"Year\": \"1959\", \"Reference\": \"Pandemonium: A Paradigm for Learning\", \"Link\": \"https://aitopics.org/doc/classics:504E1BAC/\", \"Citations\": 1450.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 3000.0, \"Training compute (FLOPs)\": 600000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 200000.0, \"Training compute times parameters\": 1800000000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"Back-propagation\", \"Domain\": \"All\", \"Task\": \"Learning to complete triples\", \"Organization(s)\": \"University of California\", \"Author(s)\": \"Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J.\", \"Publication date\": \"1986-10-09T00:00:00\", \"Year\": \"1986\", \"Reference\": \"Learning representations by back-propagating errors\", \"Link\": \"https://www.nature.com/articles/323533a0\", \"Citations\": 25300.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 144.0, \"Training compute (FLOPs)\": 124000000.0, \"Inference compute (FLOPs)\": 288.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"1.44E+02\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 861111.1111111111, \"Training compute times parameters\": 17856000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"NetTalk\", \"Domain\": \"Record\", \"Task\": \"Speech synthesis\", \"Organization(s)\": \"Princeton University\", \"Author(s)\": \"TJ Sejnowski, CR Rosenberg\", \"Publication date\": \"1987-06-06T00:00:00\", \"Year\": \"1987\", \"Reference\": \"Parallel Networks that Learn to Pronounce English Text\", \"Link\": \"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf\", \"Citations\": 2560.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 1.0, \"Parameters\": 18600.0, \"Training compute (FLOPs)\": 81200000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 4365591.397849462, \"Training compute times parameters\": 1510320000000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"Zip CNN\", \"Domain\": \"All\", \"Task\": \"Character recognition\", \"Organization(s)\": \"AT&T Bell Laboratories\", \"Author(s)\": \"Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel\", \"Publication date\": \"1989-12-01T00:00:00\", \"Year\": \"1989\", \"Reference\": \"Backpropagation applied to handwritten zip code recognition\", \"Link\": \"https://ieeexplore.ieee.org/document/6795724\", \"Citations\": 9050.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 3.0, \"Parameters\": 9760.0, \"Training compute (FLOPs)\": 43400000000.0, \"Inference compute (FLOPs)\": 129000.0, \"Training dataset\": \"Buffalo zips\", \"Training dataset size (datapoints)\": \"7291\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 4446721.31147541, \"Training compute times parameters\": 423584000000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"ALVINN\", \"Domain\": \"Record\", \"Task\": null, \"Organization(s)\": \"Carnegie Mellon University \", \"Author(s)\": \"DA Pomerleau\", \"Publication date\": \"1989-12-01T00:00:00\", \"Year\": \"1989\", \"Reference\": \"ALVINN: an autonomous land vehicle in a neural network\", \"Link\": \"https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html\", \"Citations\": 1580.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 1.0, \"Parameters\": 3990.0, \"Training compute (FLOPs)\": 81200000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 20350877.192982458, \"Training compute times parameters\": 323988000000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"TD-Gammon\", \"Domain\": \"Record\", \"Task\": \"Backgammon\", \"Organization(s)\": \"IBM\", \"Author(s)\": \"G Tesauro\", \"Publication date\": \"1992-05-01T00:00:00\", \"Year\": \"1992\", \"Reference\": \"Practical Issues in Temporal Difference Learning\", \"Link\": \"https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf\", \"Citations\": 1340.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 1.0, \"Parameters\": 25000.0, \"Training compute (FLOPs)\": 18200000000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 728000000.0, \"Training compute times parameters\": 4.55e+17, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"RNN for speech\", \"Domain\": \"All\", \"Task\": \"Speech synthesis\", \"Organization(s)\": \"National Chiao Tung University\", \"Author(s)\": \"SH Chen, SH Hwang, YR Wang\", \"Publication date\": \"1998-05-15T00:00:00\", \"Year\": \"1998\", \"Reference\": \"An RNN-based prosodic information synthesizer for Mandarin text-to-speech\", \"Link\": \"https://ieeexplore.ieee.org/abstract/document/668817\", \"Citations\": 231.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 7510.0, \"Training compute (FLOPs)\": 227000000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 30226364.84687084, \"Training compute times parameters\": 1704770000000000.0, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"LeNet-5\", \"Domain\": \"All\", \"Task\": \"Character recognition\", \"Organization(s)\": \"AT&T Labs\", \"Author(s)\": \"Yann LeCun, L\\u00e9on Bottou, Yoshua Bengio, Patrick Haffner\", \"Publication date\": \"1998-11-01T00:00:00\", \"Year\": \"1998\", \"Reference\": \"Gradient-based Learning Applied to Document Recognition\", \"Link\": \"http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf\", \"Citations\": 38600.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Historical relevance\", \"Hidden layers\": 6.0, \"Parameters\": 60000.0, \"Training compute (FLOPs)\": 2810000000000.0, \"Inference compute (FLOPs)\": 781000.0, \"Training dataset\": \"MNIST\", \"Training dataset size (datapoints)\": \"6.00E+04\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": \"0.021\", \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 46833333.333333336, \"Training compute times parameters\": 1.686e+17, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"Decision tree (classification)\", \"Domain\": \"Record\", \"Task\": \"Face recognition\", \"Organization(s)\": \"Mitsubishi Electric Research Labs and Compaq CRL\", \"Author(s)\": \"P. Viola, M. Jones\", \"Publication date\": \"2001-12-08T00:00:00\", \"Year\": \"2001\", \"Reference\": \"Rapid object detection using a boosted cascade of simple features\", \"Link\": \"https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf\", \"Citations\": 23400.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 120000000.0, \"Training compute (FLOPs)\": 63000000000000.0, \"Inference compute (FLOPs)\": 67000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"1.94E+04\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 525000.0, \"Training compute times parameters\": 7.56e+21, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"NPLM\", \"Domain\": \"Record\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Universit\\u00e9 de Montr\\u00e9al\", \"Author(s)\": \"Yoshua Bengio, R\\u00e9jean Ducharme, Pascal Vincent, Christian Jauvin\", \"Publication date\": \"2003-03-15T00:00:00\", \"Year\": \"2003\", \"Reference\": \"A Neural Probabilistic Language Model\", \"Link\": \"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\", \"Citations\": 7630.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 11900000.0, \"Training compute (FLOPs)\": 1300000000000000.0, \"Inference compute (FLOPs)\": 21700000.0, \"Training dataset\": \"Brown corpus\", \"Training dataset size (datapoints)\": \"1.00E+06\", \"Equivalent training time (hours)\": 20160.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 109243697.4789916, \"Training compute times parameters\": 1.547e+22, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"BiLSTM for Speech\", \"Domain\": \"All\", \"Task\": \"Speech recognition\", \"Organization(s)\": \"IDSIA and TU Munich\", \"Author(s)\": \"A Graves, J Schmidhuber\", \"Publication date\": \"2005-08-01T00:00:00\", \"Year\": \"2005\", \"Reference\": \"Framewise phoneme classification with bidirectional LSTM and other neural network architectures\", \"Link\": \"https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206\", \"Citations\": 3390.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 152000.0, \"Training compute (FLOPs)\": 24100000000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 158552631.57894737, \"Training compute times parameters\": 3.6632e+18, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"GPU DBNs\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Stanford\", \"Author(s)\": \"R Raina, A Madhavan, AY Ng\", \"Publication date\": \"2009-06-15T00:00:00\", \"Year\": \"2009\", \"Reference\": \"Large-scale Deep Unsupervised Learning using Graphics Processors\", \"Link\": \"http://www.machinelearning.org/archive/icml2009/papers/218.pdf\", \"Citations\": 789.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 100000000.0, \"Training compute (FLOPs)\": 1000000000000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 10000000.0, \"Training compute times parameters\": 1e+23, \"Era\": \"Pre Deep Learning Era\"}, {\"System\": \"6-layer MLP (MNIST)\", \"Domain\": \"All\", \"Task\": \"Character recognition\", \"Organization(s)\": \"IDSIA ; University of Lugano & SUPSI\", \"Author(s)\": \"Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, Juergen Schmidhuber\", \"Publication date\": \"2010-03-01T00:00:00\", \"Year\": \"2010\", \"Reference\": \"Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition\", \"Link\": \"https://arxiv.org/abs/1003.0358\", \"Citations\": 1260.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 12100000.0, \"Training compute (FLOPs)\": 131000000000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"MNIST\", \"Training dataset size (datapoints)\": \"6.00E+04\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 10826446.280991735, \"Training compute times parameters\": 1.5851e+21, \"Era\": \"Deep Learning Era\"}, {\"System\": \"Feedforward NN\", \"Domain\": \"All\", \"Task\": \"Digit recognition\", \"Organization(s)\": \"University of Montreal, Microsoft Research\", \"Author(s)\": \"X Glorot, Y Bengio\", \"Publication date\": \"2010-05-13T00:00:00\", \"Year\": \"2010\", \"Reference\": \"Understanding the difficulty of training deep feedforward neural networks\", \"Link\": \"https://web.archive.org/web/20211123180817/http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\", \"Citations\": 13300.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 7080000.0, \"Training compute (FLOPs)\": 350000000000000.0, \"Inference compute (FLOPs)\": 14000000.0, \"Training dataset\": \"MNIST\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 49435028.24858757, \"Training compute times parameters\": 2.478e+21, \"Era\": \"Deep Learning Era\"}, {\"System\": \"RNN 500/10 + RT09 LM (NIST RT05)\", \"Domain\": \"All\", \"Task\": \"Transcription\", \"Organization(s)\": \"Brno University of Technology, Johns Hopkins University\", \"Author(s)\": \"T. Mikolov, M. Karafiat, L. Burget, J. Cernock \\u00b4 y, and S. Khudanpur\", \"Publication date\": \"2010-09-26T00:00:00\", \"Year\": \"2010\", \"Reference\": \"Recurrent neural network based language model.\", \"Link\": \"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\", \"Citations\": 5670.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 5270000.0, \"Training compute (FLOPs)\": 3410000000000000.0, \"Inference compute (FLOPs)\": 10500000.0, \"Training dataset\": \"NIST RT05\", \"Training dataset size (datapoints)\": \"5.40E+06\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 647058823.5294118, \"Training compute times parameters\": 1.79707e+22, \"Era\": \"Deep Learning Era\"}, {\"System\": \"KN5 LM + RNN 400/10 (WSJ)\", \"Domain\": \"Record\", \"Task\": \"Transcription\", \"Organization(s)\": \"Brno University of Technology, Johns Hopkins University\", \"Author(s)\": \"T. Mikolov, M. Karafiat, L. Burget, J. Cernock \\u00b4 y, and S. Khudanpur\", \"Publication date\": \"2010-09-26T00:00:00\", \"Year\": \"2010\", \"Reference\": \"Recurrent neural network based language model.\", \"Link\": \"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\", \"Citations\": 5670.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 80000000.0, \"Training compute (FLOPs)\": 6.14e+16, \"Inference compute (FLOPs)\": 160000000.0, \"Training dataset\": \"WSJ\", \"Training dataset size (datapoints)\": \"6.40E+06\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 767500000.0, \"Training compute times parameters\": 4.912e+24, \"Era\": \"Deep Learning Era\"}, {\"System\": \"MCDNN (MNIST)\", \"Domain\": \"All\", \"Task\": \"Character recognition\", \"Organization(s)\": \"IDSIA\", \"Author(s)\": \"D Ciregan, U Meier, J Schmidhuber\", \"Publication date\": \"2012-02-13T00:00:00\", \"Year\": \"2012\", \"Reference\": \"Multi-column Deep Neural Networks for Image Classification\", \"Link\": \"https://arxiv.org/abs/1202.2745v1\", \"Citations\": 4830.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 3.0, \"Parameters\": 1990000.0, \"Training compute (FLOPs)\": 3730000000000000.0, \"Inference compute (FLOPs)\": 25900000.0, \"Training dataset\": \"MNIST\", \"Training dataset size (datapoints)\": \"6.00E+04\", \"Equivalent training time (hours)\": 490.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": \"0.021\", \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1874371859.2964823, \"Training compute times parameters\": 7.4227e+21, \"Era\": \"Deep Learning Era\"}, {\"System\": \"Dropout (MNIST)\", \"Domain\": \"All\", \"Task\": \"Character recognition\", \"Organization(s)\": \"University of Toronto\", \"Author(s)\": \"GE Hinton, N Srivastava, A Krizhevsky\", \"Publication date\": \"2012-06-03T00:00:00\", \"Year\": \"2012\", \"Reference\": \"Improving neural networks by preventing co-adaptation of feature detectors\", \"Link\": \"https://arxiv.org/abs/1207.0580\", \"Citations\": 6680.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 2.0, \"Parameters\": 5590000.0, \"Training compute (FLOPs)\": 6040000000000000.0, \"Inference compute (FLOPs)\": 11200000.0, \"Training dataset\": \"MNIST\", \"Training dataset size (datapoints)\": \"6.00E+04\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": \"0.021\", \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1080500894.454383, \"Training compute times parameters\": 3.37636e+22, \"Era\": \"Deep Learning Era\"}, {\"System\": \"AlexNet\", \"Domain\": \"Record\", \"Task\": \"Image classification\", \"Organization(s)\": \"University of Toronto\", \"Author(s)\": \"Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton\", \"Publication date\": \"2012-09-30T00:00:00\", \"Year\": \"2012\", \"Reference\": \"ImageNet Classification with Deep Convolutional Neural Networks\", \"Link\": \"https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\", \"Citations\": 85100.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 8.0, \"Parameters\": 60000000.0, \"Training compute (FLOPs)\": 4.7e+17, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 7833333333.333333, \"Training compute times parameters\": 2.82e+25, \"Era\": \"Deep Learning Era\"}, {\"System\": \"DQN\", \"Domain\": \"All\", \"Task\": \"Atari\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"V Mnih, K Kavukcuoglu, D Silver, A Graves\", \"Publication date\": \"2013-01-01T00:00:00\", \"Year\": \"2013\", \"Reference\": \"Playing Atari with Deep Reinforcement Learning\", \"Link\": \"https://arxiv.org/pdf/1312.5602.pdf\", \"Citations\": 6680.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 836000.0, \"Training compute (FLOPs)\": 2300000000000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2751196172.2488036, \"Training compute times parameters\": 1.9228e+21, \"Era\": \"Large Scale Era\"}, {\"System\": \"Word2Vec (large)\", \"Domain\": \"All\", \"Task\": \"Semantic embedding\", \"Organization(s)\": \"Google\", \"Author(s)\": \"T Mikolov, I Sutskever, K Chen, GS Corrado\", \"Publication date\": \"2013-10-16T00:00:00\", \"Year\": \"2013\", \"Reference\": \"Distributed Representations of Words and Phrases and their Compositionality\", \"Link\": \"https://arxiv.org/abs/1310.4546\", \"Citations\": 28700.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 6920000000.0, \"Training compute (FLOPs)\": 3.89e+16, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 5621387.283236994, \"Training compute times parameters\": 2.69188e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"Visualizing CNNs\", \"Domain\": \"Record\", \"Task\": null, \"Organization(s)\": \"NYU\", \"Author(s)\": \"MD Zeiler, R Fergus\", \"Publication date\": \"2013-11-12T00:00:00\", \"Year\": \"2013\", \"Reference\": \"Visualizing and Understanding Convolutional Networks\", \"Link\": \"https://arxiv.org/abs/1311.2901\", \"Citations\": 13000.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 5.32e+17, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"Variational Autoencoders\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Univeristy of Amsterdam\", \"Author(s)\": \"DP Kingma, M Welling\", \"Publication date\": \"2013-12-20T00:00:00\", \"Year\": \"2013\", \"Reference\": \"Auto-Encoding Variational Bayes\", \"Link\": \"https://arxiv.org/abs/1312.6114\", \"Citations\": 15600.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 475200000000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"GANs\", \"Domain\": \"All\", \"Task\": \"Image generation\", \"Organization(s)\": \"Universite de Montr\\u00e9al\", \"Author(s)\": \"Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\", \"Publication date\": \"2014-06-10T00:00:00\", \"Year\": \"2014\", \"Reference\": \"Generative Adversarial Networks\", \"Link\": \"https://arxiv.org/abs/1406.2661\", \"Citations\": 36900.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 5.18e+17, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": \"Unsupervised\", \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"RNNsearch-50*\", \"Domain\": \"Record\", \"Task\": \"Translation\", \"Organization(s)\": \"Universite de Montr\\u00e9al\", \"Author(s)\": \"D Bahdanau, K Cho, Y Bengio\", \"Publication date\": \"2014-09-01T00:00:00\", \"Year\": \"2014\", \"Reference\": \"Neural Machine Translation by Jointly Learning to Align and Translate\", \"Link\": \"https://arxiv.org/abs/1409.0473\", \"Citations\": 19200.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.56e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"VGG16\", \"Domain\": \"Record\", \"Task\": null, \"Organization(s)\": \"University of Oxford\", \"Author(s)\": \"Karen Simonyan; Andrew Zisserman\", \"Publication date\": \"2014-09-04T00:00:00\", \"Year\": \"2014\", \"Reference\": \"Very Deep Convolutional Networks for Large-Scale Image Recognition\", \"Link\": \"https://arxiv.org/abs/1409.1556\", \"Citations\": 61300.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 16.0, \"Parameters\": 138000000.0, \"Training compute (FLOPs)\": 8.524e+18, \"Inference compute (FLOPs)\": 15300000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 61768115942.028984, \"Training compute times parameters\": 1.176312e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"Seq2Seq\", \"Domain\": \"All\", \"Task\": \"Translation\", \"Organization(s)\": \"Google\", \"Author(s)\": \"I Sutskever, O Vinyals, QV Le\", \"Publication date\": \"2014-09-10T00:00:00\", \"Year\": \"2014\", \"Reference\": \"Sequence to Sequence Learning with Neural Networks\", \"Link\": \"https://arxiv.org/abs/1409.3215\", \"Citations\": 15700.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 384000000.0, \"Training compute (FLOPs)\": 7.3e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"WMT'14 dataset\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 19010416666.666668, \"Training compute times parameters\": 2.8032e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"ADAM (CIFAR-10)\", \"Domain\": \"All\", \"Task\": \"Image classification\", \"Organization(s)\": \"Univeristy of Amsterdam, University of Toronto\", \"Author(s)\": \"DP Kingma, J Ba\", \"Publication date\": \"2014-12-22T00:00:00\", \"Year\": \"2014\", \"Reference\": \"Adam: A Method for Stochastic Optimization\", \"Link\": \"https://arxiv.org/abs/1412.6980\", \"Citations\": 81100.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 6.05e+16, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"MSRA (C, PReLU)\", \"Domain\": \"Record\", \"Task\": null, \"Organization(s)\": null, \"Author(s)\": null, \"Publication date\": \"2015-02-06T00:00:00\", \"Year\": \"2015\", \"Reference\": \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", \"Link\": \"https://arxiv.org/pdf/1502.01852v1.pdf\", \"Citations\": 14100.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 2.4e+19, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Imagenet-1k\", \"Training dataset size (datapoints)\": \"1280000\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"GoogLeNet / InceptionV1\", \"Domain\": \"All\", \"Task\": \"Image classification\", \"Organization(s)\": \"Google, University of Michigan, University of North Carolina\", \"Author(s)\": \"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich\", \"Publication date\": \"2015-06-07T00:00:00\", \"Year\": \"2015\", \"Reference\": \"Going deeper with convolutions\", \"Link\": \"https://ieeexplore.ieee.org/document/7298594\", \"Citations\": 32800.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 22.0, \"Parameters\": 6800000.0, \"Training compute (FLOPs)\": 1.56e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 229411764705.88235, \"Training compute times parameters\": 1.0608e+25, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaGo Fan\", \"Domain\": \"Record\", \"Task\": \"Go\", \"Organization(s)\": \"Google DeepMind\", \"Author(s)\": \"TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez\", \"Publication date\": \"2015-09-09T00:00:00\", \"Year\": \"2015\", \"Reference\": \"Continuous control with deep reinforcement learning\", \"Link\": \"https://arxiv.org/abs/1509.02971\", \"Citations\": 6350.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 8210000.0, \"Training compute (FLOPs)\": 3.8e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 46285018270401.945, \"Training compute times parameters\": 3.1198e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"DeepSpeech2\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Baidu Research- Silicon Valley AI Lab\", \"Author(s)\": \"D Amodei, S Ananthanarayanan\", \"Publication date\": \"2015-12-08T00:00:00\", \"Year\": \"2015\", \"Reference\": \"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\", \"Link\": \"https://arxiv.org/abs/1512.02595\", \"Citations\": 2210.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 11.0, \"Parameters\": 38000000.0, \"Training compute (FLOPs)\": 2.6e+19, \"Inference compute (FLOPs)\": 1800000000.0, \"Training dataset\": \"Is this supposed to be in this row? \", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 684210526315.7894, \"Training compute times parameters\": 9.88e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"ResNet-152 (ImageNet)\", \"Domain\": \"All\", \"Task\": \"Image classification\", \"Organization(s)\": \"Microsoft\", \"Author(s)\": \"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\", \"Publication date\": \"2015-12-10T00:00:00\", \"Year\": \"2015\", \"Reference\": \"Deep Residual Learning for Image Recognition\", \"Link\": \"https://arxiv.org/abs/1512.03385\", \"Citations\": 85800.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 152.0, \"Parameters\": 60000000.0, \"Training compute (FLOPs)\": 1.21e+19, \"Inference compute (FLOPs)\": 22600000000.0, \"Training dataset\": \"ILSVRC 2012\", \"Training dataset size (datapoints)\": \"1.20E+06\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": \"138\", \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 201666666666.66666, \"Training compute times parameters\": 7.26e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaGo Lee\", \"Domain\": \"Record\", \"Task\": \"Go\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"D Silver, A Huang, CJ Maddison, A Guez, L Sifre\", \"Publication date\": \"2016-01-27T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Mastering the game of Go with deep neural networks and tree search\", \"Link\": \"https://www.nature.com/articles/nature16961\", \"Citations\": 10800.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.9e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"2.94E+07\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"GNMT\", \"Domain\": \"Record\", \"Task\": \"Translation\", \"Organization(s)\": \"Google\", \"Author(s)\": \"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \\u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean\", \"Publication date\": \"2016-09-26T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\", \"Link\": \"https://research.google/pubs/pub45610/\", \"Citations\": 4500.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 278000000.0, \"Training compute (FLOPs)\": 6.9e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 24820143884892.086, \"Training compute times parameters\": 1.9182e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"Xception\", \"Domain\": \"All\", \"Task\": \"Image classification\", \"Organization(s)\": \"Google\", \"Author(s)\": \"Fran\\u00e7ois Chollet\", \"Publication date\": \"2016-10-07T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Xception: Deep Learning with Depthwise Separable Convolutions\", \"Link\": \"https://arxiv.org/abs/1610.02357\", \"Citations\": 5840.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 22900000.0, \"Training compute (FLOPs)\": 4.4e+19, \"Inference compute (FLOPs)\": 16800000000.0, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1921397379912.6638, \"Training compute times parameters\": 1.0076e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"NASv3 (CIFAR-10)\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Brain\", \"Author(s)\": \"Barret Zoph, Quoc V. Le\", \"Publication date\": \"2016-11-05T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Neural Architecture Search with Reinforcement Learning\", \"Link\": \"https://arxiv.org/abs/1611.01578\", \"Citations\": 2970.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 39.0, \"Parameters\": 37400000.0, \"Training compute (FLOPs)\": 2.2e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 58823529411764.7, \"Training compute times parameters\": 8.228e+28, \"Era\": \"Large Scale Era\"}, {\"System\": \"Libratus\", \"Domain\": \"All\", \"Task\": \"Poker\", \"Organization(s)\": \"Carnagie Mellon University\", \"Author(s)\": \"N Brown, T Sandholm, S Machine\", \"Publication date\": \"2017-01-01T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Libratus: The Superhuman AI for No-Limit Poker\", \"Link\": \"https://www.cs.cmu.edu/~noamb/papers/17-IJCAI-Libratus.pdf\", \"Citations\": 64.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA improvement\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.15e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 3000000.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaGo Master\", \"Domain\": \"Record\", \"Task\": \"Go\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"D Silver, J Schrittwieser, K Simonyan, I Antonoglou\", \"Publication date\": \"2017-01-01T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Mastering the game of Go without human knowledge\", \"Link\": \"https://www.nature.com/articles/nature24270\", \"Citations\": 5810.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.5e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"DeepStack\", \"Domain\": \"All\", \"Task\": \"Poker\", \"Organization(s)\": \"University of Alberta, Charles University, Czech Technical University\", \"Author(s)\": null, \"Publication date\": \"2017-01-06T00:00:00\", \"Year\": \"2017\", \"Reference\": \"DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker\", \"Link\": \"https://arxiv.org/abs/1701.01724\", \"Citations\": 618.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 2500000.0, \"Training compute (FLOPs)\": 150000000000000.0, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 60000000.0, \"Training compute times parameters\": 3.75e+20, \"Era\": \"Large Scale Era\"}, {\"System\": \"MoE\", \"Domain\": \"All\", \"Task\": \"Language modelling / Machine translation\", \"Organization(s)\": \"Google Brain, Jagiellonian University, Cracow\", \"Author(s)\": \"N Shazeer, A Mirhoseini, K Maziarz, A Davis\", \"Publication date\": \"2017-01-23T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\", \"Link\": \"https://arxiv.org/abs/1701.06538\", \"Citations\": 687.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 8700000000.0, \"Training compute (FLOPs)\": 9.39391e+19, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 10797597701.149426, \"Training compute times parameters\": 8.1727017e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"Transformer\", \"Domain\": \"All\", \"Task\": \"Translation\", \"Organization(s)\": \"Google Brain ; Google Research\", \"Author(s)\": \"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\", \"Publication date\": \"2017-06-12T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Attention Is All You Need\", \"Link\": \"https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\", \"Citations\": 25200.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 213000000.0, \"Training compute (FLOPs)\": 7.42452e+18, \"Inference compute (FLOPs)\": 54000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 672.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 34856901408.45071, \"Training compute times parameters\": 1.58142276e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"JFT\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Research, CMU\", \"Author(s)\": \"ChenSun,AbhinavShrivastava,SaurabhSingh,andAbhinavGupta\", \"Publication date\": \"2017-08-04T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.\", \"Link\": \"https://arxiv.org/pdf/1707.02968.pdf\", \"Citations\": 1140.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 4.79e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"JFT-300M\", \"Training dataset size (datapoints)\": \"3.00E+08\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"OpenAI TI7 DOTA 1v1\", \"Domain\": \"All\", \"Task\": \"DOTA\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": null, \"Publication date\": \"2017-08-11T00:00:00\", \"Year\": \"2017\", \"Reference\": null, \"Link\": null, \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 6.05e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaGo Zero\", \"Domain\": \"Record\", \"Task\": \"Go\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"D Silver, J Schrittwieser, K Simonyan, I Antonoglou\", \"Publication date\": \"2017-10-19T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Mastering the game of Go without human knowledge\", \"Link\": \"https://www.nature.com/articles/nature24270\", \"Citations\": 5810.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 46400000.0, \"Training compute (FLOPs)\": 3.41e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 7349137931034483.0, \"Training compute times parameters\": 1.58224e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaZero\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"D Silver, T Hubert, J Schrittwieser, I Antonoglou\", \"Publication date\": \"2017-12-05T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\", \"Link\": \"https://arxiv.org/abs/1712.01815\", \"Citations\": 1080.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 3.67e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"IMPALA\", \"Domain\": \"All\", \"Task\": \"Atari\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu\", \"Publication date\": \"2018-02-05T00:00:00\", \"Year\": \"2018\", \"Reference\": \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\", \"Link\": \"https://arxiv.org/abs/1802.01561\", \"Citations\": 675.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1600000.0, \"Training compute (FLOPs)\": 1.68e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 105000000000000.0, \"Training compute times parameters\": 2.688e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"YOLOv3\", \"Domain\": \"All\", \"Task\": \"Object detection\", \"Organization(s)\": \"University of Washington\", \"Author(s)\": \"Joseph Redmon, Ali Farhadi\", \"Publication date\": \"2018-04-08T00:00:00\", \"Year\": \"2018\", \"Reference\": \"YOLOv3: An Incremental Improvement\", \"Link\": \"https://arxiv.org/abs/1804.02767\", \"Citations\": 7710.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 106000000.0, \"Training compute (FLOPs)\": 5.09e+19, \"Inference compute (FLOPs)\": 71000000000.0, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": \"1.28E+06\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 480188679245.283, \"Training compute times parameters\": 5.3954e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"A Radford, K Narasimhan, T Salimans, I Sutskever\", \"Publication date\": \"2018-06-01T00:00:00\", \"Year\": \"2018\", \"Reference\": \"Improving Language Understanding by Generative Pre-Training\", \"Link\": \"https://openai.com/blog/language-unsupervised/\", \"Citations\": 2260.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 117000000.0, \"Training compute (FLOPs)\": 1.1e+19, \"Inference compute (FLOPs)\": 30000000000.0, \"Training dataset\": \"BooksCorpus\", \"Training dataset size (datapoints)\": \"6.25E+08\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 94017094017.09402, \"Training compute times parameters\": 1.287e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"Population-based DRL\", \"Domain\": \"All\", \"Task\": \"Capture the flag\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel\", \"Publication date\": \"2018-07-03T00:00:00\", \"Year\": \"2018\", \"Reference\": \"Human-level performance in first-person multiplayer games with population-based deep reinforcement learning\", \"Link\": \"https://arxiv.org/abs/1807.01281\", \"Citations\": 434.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 122000000.0, \"Training compute (FLOPs)\": 3.49e+19, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 286065573770.4918, \"Training compute times parameters\": 4.2578e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"BigGAN-deep 512x512\", \"Domain\": \"All\", \"Task\": \"Image generation\", \"Organization(s)\": \"Heriot-Watt University, DeepMind\", \"Author(s)\": \"A Brock, J Donahue, K Simonyan\", \"Publication date\": \"2018-09-28T00:00:00\", \"Year\": \"2018\", \"Reference\": \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\", \"Link\": \"https://arxiv.org/abs/1809.11096\", \"Citations\": 1980.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 113000000.0, \"Training compute (FLOPs)\": 3e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": \"Perplexity... sorta\", \"Milestone\": \">5000 citations\", \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 26548672566371.68, \"Training compute times parameters\": 3.39e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"BERT-Large\", \"Domain\": \"All\", \"Task\": \"Next sentence prediction\", \"Organization(s)\": \"Google AI\", \"Author(s)\": \"J Devlin, MW Chang, K Lee, K Toutanova\", \"Publication date\": \"2018-10-11T00:00:00\", \"Year\": \"2018\", \"Reference\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"Link\": \"https://arxiv.org/abs/1810.04805\", \"Citations\": 23800.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 340000000.0, \"Training compute (FLOPs)\": 2.85e+20, \"Inference compute (FLOPs)\": 79000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 838235294117.6471, \"Training compute times parameters\": 9.69e+28, \"Era\": \"Large Scale Era\"}, {\"System\": \"Hanabi 4 player\", \"Domain\": \"All\", \"Task\": \"Hanabi\", \"Organization(s)\": \"DeepMind, University of Oxford, Google Brain, Carnegie Mellon University, \", \"Author(s)\": null, \"Publication date\": \"2019-02-01T00:00:00\", \"Year\": \"2019\", \"Reference\": \"The Hanabi Challenge: A New Frontier for AI Research\", \"Link\": \"https://arxiv.org/abs/1902.00506\", \"Citations\": 115.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 764000.0, \"Training compute (FLOPs)\": 9.16e+16, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 119895287958.11519, \"Training compute times parameters\": 6.99824e+22, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT-2\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"A Radford, J Wu, R Child, D Luan, D Amodei\", \"Publication date\": \"2019-02-14T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Language Models are Unsupervised Multitask Learners\", \"Link\": \"https://openai.com/blog/better-language-models/\", \"Citations\": 1700.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 1500000000.0, \"Training compute (FLOPs)\": 2.49e+21, \"Inference compute (FLOPs)\": 3400000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"5.00E+09\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": \"40\", \"Approach\": \"Supervised\", \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1660000000000.0, \"Training compute times parameters\": 3.735e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"ProxylessNAS\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"MIT\", \"Author(s)\": \"Han Cai, Ligeng Zhu, and Song Han\", \"Publication date\": \"2019-02-23T00:00:00\", \"Year\": \"2019\", \"Reference\": \"ProxylessNAS: Direct neural architecture search on target task and hardware\", \"Link\": \"https://arxiv.org/pdf/1812.00332.pdf\", \"Citations\": 996.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 3.70656e+19, \"Inference compute (FLOPs)\": 263000000000.0, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": \"1.28E+06\", \"Equivalent training time (hours)\": 200.0, \"Inference time (ms)\": \"5.1\", \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"MnasNet-A1 + SSDLite\", \"Domain\": \"All\", \"Task\": \"Performing image classification and object detection on mobile devices\", \"Organization(s)\": \"Google \", \"Author(s)\": \"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le\", \"Publication date\": \"2019-05-29T00:00:00\", \"Year\": \"2019\", \"Reference\": \"MnasNet: Platform-Aware Neural Architecture Search for Mobile\", \"Link\": \"https://arxiv.org/pdf/1807.11626.pdf\", \"Citations\": 1430.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 4900000.0, \"Training compute (FLOPs)\": 1.5e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"MS COCO\", \"Training dataset size (datapoints)\": \"1.18E+05\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"https://proceedings.neurips.cc/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 306122448979591.8, \"Training compute times parameters\": 7.35e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"MnasNet-A3\", \"Domain\": \"All\", \"Task\": \"Performing image classification and object detection on mobile devices\", \"Organization(s)\": \"Google \", \"Author(s)\": \"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le\", \"Publication date\": \"2019-05-29T00:00:00\", \"Year\": \"2019\", \"Reference\": \"MnasNet: Platform-Aware Neural Architecture Search for Mobile\", \"Link\": \"https://arxiv.org/pdf/1807.11626.pdf\", \"Citations\": 1430.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 5200000.0, \"Training compute (FLOPs)\": 1.5e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LessWrong\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 288461538461538.44, \"Training compute times parameters\": 7.8e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"DLRM-2020\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook AI\", \"Author(s)\": \"M Naumov, D Mudigere, HJM Shi, J Huang\", \"Publication date\": \"2019-05-31T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Deep Learning Recommendation Model for Personalization and Recommendation Systems\", \"Link\": \"https://arxiv.org/abs/1906.00091\", \"Citations\": 140.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 100000000000.0, \"Training compute (FLOPs)\": 4e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": \"Supervised\", \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 40000000.0, \"Training compute times parameters\": 4e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"ObjectNet\", \"Domain\": \"All\", \"Task\": \"Object recognition\", \"Organization(s)\": \"MIT\", \"Author(s)\": \"Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfre- und, Josh Tenenbaum, and Boris Katz\", \"Publication date\": \"2019-09-06T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models\", \"Link\": \"https://papers.nips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf\", \"Citations\": 2390.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 38000000.0, \"Training compute (FLOPs)\": 1.94e+19, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Internal data\", \"Training dataset size (datapoints)\": \"19M utterences\", \"Equivalent training time (hours)\": 108.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 510526315789.4737, \"Training compute times parameters\": 7.372e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"Hide and Seek\", \"Domain\": \"All\", \"Task\": \"Hide and Seek\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"B Baker, I Kanitscheider, T Markov, Y Wu\", \"Publication date\": \"2019-09-17T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Emergent Tool Use From Multi-Agent Autocurricula\", \"Link\": \"https://openai.com/blog/emergent-tool-use/\", \"Citations\": 224.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1600000.0, \"Training compute (FLOPs)\": 3.04e+17, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 190000000000.0, \"Training compute times parameters\": 4.864e+23, \"Era\": \"Large Scale Era\"}, {\"System\": \"Megatron-LM\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"NVIDIA\", \"Author(s)\": \"M Shoeybi, M Patwary, R Puri, P LeGresley\", \"Publication date\": \"2019-09-17T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\", \"Link\": \"https://arxiv.org/abs/1909.08053\", \"Citations\": 246.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 8300000000.0, \"Training compute (FLOPs)\": 9.1e+21, \"Inference compute (FLOPs)\": 18000000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1096385542168.6747, \"Training compute times parameters\": 7.553e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"Megatron-BERT\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"NVIDIA\", \"Author(s)\": \"M Shoeybi, M Patwary, R Puri, P LeGresley\", \"Publication date\": \"2019-09-17T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\", \"Link\": \"https://arxiv.org/abs/1909.08053\", \"Citations\": 246.0, \"Highly influential citations\": 47.0, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 3900000000.0, \"Training compute (FLOPs)\": 5.68e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 14564102564102.564, \"Training compute times parameters\": 2.2152e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaX-1\", \"Domain\": \"All\", \"Task\": \"Neural architecture search for computer vision\", \"Organization(s)\": \"Brown and Facebook AI Research\", \"Author(s)\": \"Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1\", \"Publication date\": \"2019-10-02T00:00:00\", \"Year\": \"2019\", \"Reference\": \"AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search\", \"Link\": \"https://arxiv.org/pdf/1903.11059.pdf\", \"Citations\": 50.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 579000000.0, \"Training compute (FLOPs)\": 7.6e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 13126079447.322971, \"Training compute times parameters\": 4.4004e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"Rubik's cube\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Open AI\", \"Author(s)\": \"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang\\n\", \"Publication date\": \"2019-10-15T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Solving Rubik\\u2019s Cube with a Robot Hand\", \"Link\": \"https://openai.com/blog/solving-rubiks-cube/\", \"Citations\": 227.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 27800000.0, \"Training compute (FLOPs)\": 8.54e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 30719424460431.656, \"Training compute times parameters\": 2.37412e+28, \"Era\": \"Large Scale Era\"}, {\"System\": \"T5-3B\", \"Domain\": \"All\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Google\", \"Author(s)\": \"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu\", \"Publication date\": \"2019-10-23T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"Link\": \"https://arxiv.org/abs/1910.10683\", \"Citations\": 1540.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 3000000000.0, \"Training compute (FLOPs)\": 1.04e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Colossal Clean Crawled Corpus (C4)\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3466666666666.6665, \"Training compute times parameters\": 3.12e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"T5-11B\", \"Domain\": \"All\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Google\", \"Author(s)\": \"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu\", \"Publication date\": \"2019-10-23T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"Link\": \"https://arxiv.org/abs/1910.10683\", \"Citations\": 1540.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 11000000000.0, \"Training compute (FLOPs)\": 4.05e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Colossal Clean Crawled Corpus (C4)\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3681818181818.182, \"Training compute times parameters\": 4.4550000000000004e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaStar\", \"Domain\": \"All\", \"Task\": \"StarCraft\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Micha\\u00ebl Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,R\\u00e9mi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario W\\u00fcnsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver\", \"Publication date\": \"2019-10-30T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Grandmaster level in StarCraft II using multi-agent reinforcement learning\", \"Link\": \"https://www.nature.com/articles/s41586-019-1724-z.epdf\", \"Citations\": 1040.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 139000000.0, \"Training compute (FLOPs)\": 2.02e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1453237410071942.5, \"Training compute times parameters\": 2.8078e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"OpenAI Five\", \"Domain\": \"All\", \"Task\": \"DotA\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,\\nPrzemys\\u0142aw \\u201cPsyho\\\" D\\u0119biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal J\\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pond\\u00e9 de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang\", \"Publication date\": \"2019-12-13T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Dota 2 with Large Scale Deep Reinforcement Learning\", \"Link\": \"https://cdn.openai.com/dota-2.pdf\", \"Citations\": 349.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA improvement\", \"Hidden layers\": null, \"Parameters\": 159000000.0, \"Training compute (FLOPs)\": 1.3e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 81761006289308.17, \"Training compute times parameters\": 2.067e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"OpenAI Five\", \"Domain\": \"All\", \"Task\": \"DOTA 5v5\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"J Raiman, S Zhang, F Wolski\", \"Publication date\": \"2019-12-13T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Dota 2 with Large Scale Deep Reinforcement Learning\", \"Link\": \"https://arxiv.org/abs/1912.06680\", \"Citations\": 454.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA improvement\", \"Hidden layers\": null, \"Parameters\": 10000000.0, \"Training compute (FLOPs)\": 6.7e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 6700000000000000.0, \"Training compute times parameters\": 6.7e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaFold\", \"Domain\": \"All\", \"Task\": \"Protein folding prediction\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin \\u017d\\u00eddek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu & Demis Hassabis\", \"Publication date\": \"2020-01-15T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Improved protein structure prediction using potentials from deep learning\", \"Link\": \"https://www.nature.com/articles/s41586-019-1923-7\", \"Citations\": 840.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 69000000.0, \"Training compute (FLOPs)\": 1e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1449275362318.8406, \"Training compute times parameters\": 6.9e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"Meena\", \"Domain\": \"All\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Google AI\", \"Author(s)\": \"Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, Quoc V. Le\", \"Publication date\": \"2020-01-28T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Towards a Human-like Open-Domain Chatbot\", \"Link\": \"https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html\", \"Citations\": 257.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 2600000000.0, \"Training compute (FLOPs)\": 1.12e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 43076923076923.08, \"Training compute times parameters\": 2.912e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"Turing NLG\", \"Domain\": \"All\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Microsoft\", \"Author(s)\": \"C Rosset\", \"Publication date\": \"2020-02-13T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Turing-NLG: A 17-billion-parameter language model by Microsoft\", \"Link\": \"https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\", \"Citations\": 34.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 17000000000.0, \"Training compute (FLOPs)\": 1.57e+22, \"Inference compute (FLOPs)\": 36000000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 923529411764.7058, \"Training compute times parameters\": 2.669e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"ProGen\", \"Domain\": \"All\", \"Task\": \"Protein generation\", \"Organization(s)\": \"Salesforce research, Stanford\", \"Author(s)\": \"A Madani, B McCann, N Naik, NS Keskar\", \"Publication date\": \"2020-03-13T00:00:00\", \"Year\": \"2020\", \"Reference\": \"ProGen: Language Modeling for Protein Generation\", \"Link\": \"https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2\", \"Citations\": 46.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1200000000.0, \"Training compute (FLOPs)\": 2.7e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 225000000000.0, \"Training compute times parameters\": 3.24e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT-3 175B\", \"Domain\": \"All\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\", \"Publication date\": \"2020-04-28T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Language Models are Few-Shot Learners\", \"Link\": \"https://arxiv.org/abs/2005.14165\", \"Citations\": 1530.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 175000000000.0, \"Training compute (FLOPs)\": 3.14e+23, \"Inference compute (FLOPs)\": 740000000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": \"45TB\", \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1794285714285.7144, \"Training compute times parameters\": 5.495e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"Once for All\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"MIT-IBM Watson AI Lab, \", \"Author(s)\": \"Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han\", \"Publication date\": \"2020-04-29T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Once for all: Train one network and specialize it for efficient deployment.\", \"Link\": \"https://arxiv.org/pdf/1908.09791.pdf\", \"Citations\": 371.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 7700000.0, \"Training compute (FLOPs)\": 1.78e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Imagenet\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 231168831168831.16, \"Training compute times parameters\": 1.3706e+28, \"Era\": \"Large Scale Era\"}, {\"System\": \"iGPT-L\", \"Domain\": \"All\", \"Task\": \"Image completion\", \"Organization(s)\": \"Open AI\", \"Author(s)\": \"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever\", \"Publication date\": \"2020-06-17T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Generative Pretraining from Pixels\", \"Link\": \"https://openai.com/blog/image-gpt/\", \"Citations\": 182.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1360000000.0, \"Training compute (FLOPs)\": 8.91e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 6551470588235.295, \"Training compute times parameters\": 1.21176e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"iGPT-XL\", \"Domain\": \"All\", \"Task\": \"Image completion\", \"Organization(s)\": \"Open AI\", \"Author(s)\": \"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever\", \"Publication date\": \"2020-06-17T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Generative Pretraining from Pixels\", \"Link\": \"https://openai.com/blog/image-gpt/\", \"Citations\": 182.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 6800000000.0, \"Training compute (FLOPs)\": 3.3e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 4852941176470.588, \"Training compute times parameters\": 2.244e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"GShard (600B)\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Brain\", \"Author(s)\": \"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\", \"Publication date\": \"2020-06-30T00:00:00\", \"Year\": \"2020\", \"Reference\": \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\", \"Link\": \"https://arxiv.org/abs/2006.16668\", \"Citations\": 91.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 600000000000.0, \"Training compute (FLOPs)\": 1.33e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 22166666666.666664, \"Training compute times parameters\": 7.98e+33, \"Era\": \"Large Scale Era\"}, {\"System\": \"GShard (dense)\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Brain\", \"Author(s)\": \"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\", \"Publication date\": \"2020-06-30T00:00:00\", \"Year\": \"2020\", \"Reference\": \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\", \"Link\": \"https://arxiv.org/abs/2006.16668\", \"Citations\": 91.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 2300000000.0, \"Training compute (FLOPs)\": 2.6e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 11304347826086.957, \"Training compute times parameters\": 5.98e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"wave2vec 2.0 LARGE\", \"Domain\": \"All\", \"Task\": \"Speech completion\", \"Organization(s)\": \"Facebook\", \"Author(s)\": \"Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli\", \"Publication date\": \"2020-10-22T00:00:00\", \"Year\": \"2020\", \"Reference\": \"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\", \"Link\": \"https://arxiv.org/pdf/2006.11477.pdf\", \"Citations\": 410.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA Improvement\", \"Hidden layers\": null, \"Parameters\": 317000000.0, \"Training compute (FLOPs)\": 4.34e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"LibriSpeech\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1369085173501.5774, \"Training compute times parameters\": 1.37578e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"CPM-Large\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Tsinghua University, BAAI\", \"Author(s)\": \"Z Zhang, X Han, H Zhou, P Ke, Y Gu, D Ye, Y Qin, Y Su\", \"Publication date\": \"2020-12-01T00:00:00\", \"Year\": \"2020\", \"Reference\": \"CPM: A Large-scale Generative Chinese Pre-trained Language Model\", \"Link\": \"https://arxiv.org/abs/2012.00413\", \"Citations\": 10.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 2600000000.0, \"Training compute (FLOPs)\": 1.8e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 692307692307.6923, \"Training compute times parameters\": 4.68e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"AraGPT2-Mega\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"American University of Beirut\", \"Author(s)\": \"W Antoun, F Baly, H Hajj\", \"Publication date\": \"2020-12-31T00:00:00\", \"Year\": \"2020\", \"Reference\": \"AraGPT2: Pre-Trained Transformer for Arabic Language Generation\", \"Link\": \"https://arxiv.org/abs/2012.15520v1\", \"Citations\": 4.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1500000000.0, \"Training compute (FLOPs)\": 2e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1333333333333.3333, \"Training compute times parameters\": 3e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"CLIP (ViT L/14@336px)\", \"Domain\": \"All\", \"Task\": \"Zero-shot image classification\", \"Organization(s)\": \"Open AI\", \"Author(s)\": \"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\", \"Publication date\": \"2021-01-05T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Learning Transferable Visual Models From Natural Language Supervision\", \"Link\": \"https://arxiv.org/abs/2103.00020\", \"Citations\": 130.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 370000000.0, \"Training compute (FLOPs)\": 1.05e+22, \"Inference compute (FLOPs)\": 110000000.0, \"Training dataset\": \"Custom image-text pairs from the internet\", \"Training dataset size (datapoints)\": \"4.00E+08\", \"Equivalent training time (hours)\": 86016.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 28378378378378.375, \"Training compute times parameters\": 3.8849999999999994e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"DALL-E\", \"Domain\": \"All\", \"Task\": \"Text-to-image\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever\", \"Publication date\": \"2021-01-05T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Zero-Shot Text-to-Image Generation\", \"Link\": \"https://openai.com/blog/dall-e/\", \"Citations\": 80.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 12000000000.0, \"Training compute (FLOPs)\": 4.7e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3916666666666.667, \"Training compute times parameters\": 5.64e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"Switch\", \"Domain\": \"All\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Google Brain\", \"Author(s)\": \"William Fedus, Barret Zoph, Noam Shazeer\", \"Publication date\": \"2021-01-11T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\", \"Link\": \"https://arxiv.org/abs/2101.03961\", \"Citations\": 80.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1600000000000.0, \"Training compute (FLOPs)\": 8.22e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 51374999999.99999, \"Training compute times parameters\": 1.3152e+35, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT-Neo\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": null, \"Author(s)\": null, \"Publication date\": \"2021-03-21T00:00:00\", \"Year\": \"2021\", \"Reference\": \"GPT-Neo\", \"Link\": \"https://www.eleuther.ai/projects/gpt-neo/\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 2700000000.0, \"Training compute (FLOPs)\": 7.9e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2925925925925.926, \"Training compute times parameters\": 2.133e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"PanGu-\\u03b1\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"PanGu-\\u03b1 team\", \"Author(s)\": \"Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi LiaoZhiwei WangXin JiangZhenzhang YangKaisheng WangXiaoda ZhangChen LiZiyan GongYifan YaoXinjing HuangJun WangJianfeng YuQi GuoYue YuYan ZhangJin WangHengtao TaoDasen YanZexuan YiFang PengFangqing JiangHan ZhangLingfeng DengYehong ZhangZhe LinChao ZhangShaojie ZhangMingyue GuoShanzhi GuGaojun FanYaowei WangXuefeng JinQun LiuYonghong Tian\", \"Publication date\": \"2021-04-25T00:00:00\", \"Year\": \"2021\", \"Reference\": \"PanGu-\\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation\", \"Link\": \"https://arxiv.org/abs/2104.12369\", \"Citations\": 5.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 207000000000.0, \"Training compute (FLOPs)\": 5.83e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 281642512077.2947, \"Training compute times parameters\": 1.2068100000000001e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT-J-6B\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": null, \"Author(s)\": null, \"Publication date\": \"2021-05-01T00:00:00\", \"Year\": \"2021\", \"Reference\": null, \"Link\": \"https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 6050000000.0, \"Training compute (FLOPs)\": 1.5e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2479338842975.2065, \"Training compute times parameters\": 9.075e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"ProtT5-XXL\", \"Domain\": \"All\", \"Task\": \"Proteins\", \"Organization(s)\": \"Technical Univeristy of Munich, Med AI Technology, Google AI, NVIDIA, Oak Ridge National Laboratory\", \"Author(s)\": \"A Elnaggar, M Heinzinger, C Dallago, G Rihawi\", \"Publication date\": \"2021-05-04T00:00:00\", \"Year\": \"2021\", \"Reference\": \"ProtTrans: Towards Cracking the Language of Life\\u2019s Code Through Self-Supervised Learning\", \"Link\": \"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3\", \"Citations\": 57.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 11000000000.0, \"Training compute (FLOPs)\": 7.37e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 6700000000000.0, \"Training compute times parameters\": 8.107e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"HyperClova\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": null, \"Author(s)\": null, \"Publication date\": \"2021-05-25T00:00:00\", \"Year\": \"2021\", \"Reference\": null, \"Link\": \"https://www.navercorp.com/promotion/pressReleasesView/30546\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 204000000000.0, \"Training compute (FLOPs)\": 6.3e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 308823529411.7647, \"Training compute times parameters\": 1.2852e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"CogView\", \"Domain\": \"All\", \"Task\": \"Text-to-image\", \"Organization(s)\": \"Tsinghua University, DAMO academy Alibaba\", \"Author(s)\": \"M Ding, Z Yang, W Hong, W Zheng, C Zhou\", \"Publication date\": \"2021-05-26T00:00:00\", \"Year\": \"2021\", \"Reference\": \"CogView: Mastering Text-to-Image Generation via Transformers\", \"Link\": \"https://arxiv.org/abs/2105.13290\", \"Citations\": 1.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 4000000000.0, \"Training compute (FLOPs)\": 2.68e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 6700000000000.0, \"Training compute times parameters\": 1.072e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"ViT-G/14\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Research, Brain Team\", \"Author(s)\": \"X Zhai, A Kolesnikov, N Houlsby, L Beyer\", \"Publication date\": \"2021-06-08T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Scaling Vision Transformers\", \"Link\": \"https://arxiv.org/abs/2106.04560\", \"Citations\": 6.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1800000000.0, \"Training compute (FLOPs)\": 3.4e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1888888888888.889, \"Training compute times parameters\": 6.12e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"LSUN Bedroom model\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"UC Berkeley\", \"Author(s)\": \"Jonathan Ho, Ajay Jain, Pieter Abbeel\", \"Publication date\": \"2021-06-11T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Denoising Diffusion Probabilistic Models\", \"Link\": \"https://arxiv.org/pdf/2006.11239.pdf\", \"Citations\": 136.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA Improvement\", \"Hidden layers\": null, \"Parameters\": 256000000.0, \"Training compute (FLOPs)\": 1.6e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"CIFAR-10\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 6250000000.0, \"Training compute times parameters\": 4.096e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"ERNIE 3.0\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Baidu Inc. \", \"Author(s)\": \"Y Sun, S Wang, S Feng, S Ding, C Pang\", \"Publication date\": \"2021-07-05T00:00:00\", \"Year\": \"2021\", \"Reference\": \"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\", \"Link\": \"http://research.baidu.com/Blog/index-view?id=160\", \"Citations\": 1.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 10000000000.0, \"Training compute (FLOPs)\": 2.35e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 235000000.0, \"Training compute times parameters\": 2.35e+28, \"Era\": \"Large Scale Era\"}, {\"System\": \"HuBERT\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook AI Research\", \"Author(s)\": \"Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed\", \"Publication date\": \"2021-07-27T00:00:00\", \"Year\": \"2021\", \"Reference\": \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"Link\": \"https://arxiv.org/pdf/2106.07447.pdf\", \"Citations\": 37.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA Improvement\", \"Hidden layers\": null, \"Parameters\": 1000000000.0, \"Training compute (FLOPs)\": 5.54e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"LibriSpeech\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 5540000000000.0, \"Training compute times parameters\": 5.54e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"SEER\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook AI Research, Inria\", \"Author(s)\": \"Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski\", \"Publication date\": \"2021-07-29T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Self-supervised Pretraining of Visual Features in the Wild\", \"Link\": \"https://arxiv.org/pdf/2103.01988.pdf\", \"Citations\": 39.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Important context\", \"Hidden layers\": null, \"Parameters\": 1300000000.0, \"Training compute (FLOPs)\": 4.42e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3400000000000.0, \"Training compute times parameters\": 5.746e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"Jurassic-1-Jumbo\", \"Domain\": \"Record\", \"Task\": null, \"Organization(s)\": \"AI21 Labs\", \"Author(s)\": null, \"Publication date\": \"2021-08-11T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Jurassic-1: Technical Details and Evaluation\", \"Link\": \"https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf\", \"Citations\": 9.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 178000000000.0, \"Training compute (FLOPs)\": 3.7e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2078651685393.2585, \"Training compute times parameters\": 6.586e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"M6-10T\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Alibaba\", \"Author(s)\": \"Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang\", \"Publication date\": \"2021-10-08T00:00:00\", \"Year\": \"2021\", \"Reference\": \"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining\", \"Link\": \"https://arxiv.org/abs/2110.03888\", \"Citations\": 14.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 10000000000000.0, \"Training compute (FLOPs)\": 5.53e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 122880.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 553000000.0, \"Training compute times parameters\": 5.5299999999999995e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"Megatron-Turing NLG 530B\", \"Domain\": \"Record\", \"Task\": null, \"Organization(s)\": \"Microsoft\", \"Author(s)\": null, \"Publication date\": \"2021-10-11T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World\\u2019s Largest and Most Powerful Generative Language Model\", \"Link\": \"https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 530000000000.0, \"Training compute (FLOPs)\": 1.35e+24, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2547169811320.755, \"Training compute times parameters\": 7.155e+35, \"Era\": \"Large Scale Era\"}, {\"System\": \"Yuan 1.0\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Inspur\", \"Author(s)\": \"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu\", \"Publication date\": \"2021-11-10T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\", \"Link\": \"https://arxiv.org/abs/2110.04725\", \"Citations\": 4.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 245000000000.0, \"Training compute (FLOPs)\": 4.1e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1673469387755.102, \"Training compute times parameters\": 1.0045e+35, \"Era\": \"Large Scale Era\"}, {\"System\": \"Gopher\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu and Geoffrey Irving\", \"Publication date\": \"2021-12-08T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Scaling Language Models: Methods, Analysis & Insights from Training Gopher\", \"Link\": \"https://deepmind.com/blog/article/language-modelling-at-scale\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 280000000000.0, \"Training compute (FLOPs)\": 6.31e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2253571428571.4287, \"Training compute times parameters\": 1.7668000000000003e+35, \"Era\": \"Large Scale Era\"}, {\"System\": \"LaMDA\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google\", \"Author(s)\": \"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le\", \"Publication date\": \"2022-01-20T00:00:00\", \"Year\": \"2022\", \"Reference\": \"LaMDA: Language Models for Dialog Applications\", \"Link\": \"https://arxiv.org/abs/2201.08239\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 137000000000.0, \"Training compute (FLOPs)\": 3.55e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Infiniset\", \"Training dataset size (datapoints)\": \"1.56E+09\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2591240875912.4087, \"Training compute times parameters\": 4.8635e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"Mitosis\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"IDSIA\", \"Author(s)\": \"Dan\\u00a0C.\\u00a0Cire\\u015fan, Alessandro\\u00a0Giusti, Luca\\u00a0M.\\u00a0Gambardella, J\\u00fcrgen\\u00a0Schmidhuber\", \"Publication date\": \"2013-01-01T00:00:00\", \"Year\": \"2013\", \"Reference\": \"\\nMitosis Detection in Breast Cancer Histology Images with Deep Neural Networks\", \"Link\": \"https://link.springer.com/chapter/10.1007/978-3-642-40763-5_51\", \"Citations\": 1460.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"ICPR 2012 mitosis detection competition winner\", \"Hidden layers\": null, \"Parameters\": 37200.0, \"Training compute (FLOPs)\": 1.37e+17, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3682795698924.731, \"Training compute times parameters\": 5.0964e+21, \"Era\": \"Large Scale Era\"}, {\"System\": \"ALIGN\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google AI\", \"Author(s)\": \"ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocV.Le,YunhsuanSung, Zhen Li, and Tom Duerig\", \"Publication date\": \"2021-06-11T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Scaling up visual and vision-language representation learning with noisy text supervision\", \"Link\": \"https://arxiv.org/pdf/2102.05918.pdf\", \"Citations\": 75.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 820000000.0, \"Training compute (FLOPs)\": 2.15e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"1.60E+09\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Scaling Vision Transformers\", \"Type of developer\": \"Company\", \"Training compute per parameter (FLOPs)\": 262195121951219.5, \"Training compute times parameters\": 1.763e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"Meta Pseudo Labels\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google AI, Brain team\", \"Author(s)\": \"Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le\", \"Publication date\": \"2021-03-01T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Meta pseudo labels\", \"Link\": \"https://arxiv.org/pdf/2003.10580.pdf\", \"Citations\": 131.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA Improvement\", \"Hidden layers\": null, \"Parameters\": 480000000.0, \"Training compute (FLOPs)\": 2.12e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": \"1.30E+08\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Scaling Vision Transformers\", \"Type of developer\": \"Company\", \"Training compute per parameter (FLOPs)\": 441666666666666.6, \"Training compute times parameters\": 1.0176e+32, \"Era\": \"Large Scale Era\"}, {\"System\": null, \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"University of Freiburg\", \"Author(s)\": \"Ilya Loshchilov and Frank Hutter\", \"Publication date\": \"2019-01-04T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Decoupled weight decay regularization.\", \"Link\": \"https://arxiv.org/pdf/1711.05101.pdf\", \"Citations\": 2060.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 36500000.0, \"Training compute (FLOPs)\": 2.47e+18, \"Inference compute (FLOPs)\": 1730000000.0, \"Training dataset\": \"CIFAR-10\", \"Training dataset size (datapoints)\": \"5.00E+04\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Automated Concatenation of Embeddings for Structured Prediction\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": 67671232876.712326, \"Training compute times parameters\": 9.0155e+25, \"Era\": \"Large Scale Era\"}, {\"System\": null, \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Tel Aviv University, MIT\", \"Author(s)\": \"Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson.\", \"Publication date\": \"2019-04-04T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.\", \"Link\": \"https://arxiv.org/pdf/1902.09492.pdf\", \"Citations\": 129.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 2.56e+18, \"Inference compute (FLOPs)\": 3660000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Automated Concatenation of Embeddings for Structured Prediction\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"R-FCN\", \"Domain\": \"All\", \"Task\": \"Object detection\", \"Organization(s)\": \"Microsoft research, Tsinghua university\", \"Author(s)\": \"Jifeng Dai, Y. Li, Kaiming He, and Jian Sun\", \"Publication date\": \"2016-06-21T00:00:00\", \"Year\": \"2016\", \"Reference\": \"R-fcn: Object detection via region-based fully convolutional networks.\", \"Link\": \"https://arxiv.org/pdf/1605.06409.pdf\", \"Citations\": 4490.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 6.14929e+16, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"PASCAL VOC (2007 and 2012 vesrions) + MS COCO\", \"Training dataset size (datapoints)\": \"9.44E+04\", \"Equivalent training time (hours)\": 12.06567222, \"Inference time (ms)\": \"170\", \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Dynamic Head: Unifying Object Detection Heads with Attentions\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"TransE\", \"Domain\": \"Record\", \"Task\": \"Entity embedding\", \"Organization(s)\": \"CNRS, Google\", \"Author(s)\": \"Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko\", \"Publication date\": \"2013-12-05T00:00:00\", \"Year\": \"2013\", \"Reference\": \"Translating Embeddings for Modeling Multi- relational Data\", \"Link\": \"https://papers.nips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\", \"Citations\": 4000.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.34093e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"ALBERT-xxlarge\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google research, Toyota Technological Institute at Chicago\", \"Author(s)\": \"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut\", \"Publication date\": \"2020-02-09T00:00:00\", \"Year\": \"2020\", \"Reference\": \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\", \"Link\": \"https://arxiv.org/pdf/1909.11942.pdf\", \"Citations\": 2180.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 235000000.0, \"Training compute (FLOPs)\": 2.54373e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 17408.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 10824382978723.404, \"Training compute times parameters\": 5.9777655000000005e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"Part-of-sentence tagging model\", \"Domain\": \"All\", \"Task\": \"POS tagging\", \"Organization(s)\": \"University of Toronto\", \"Author(s)\": \"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton\", \"Publication date\": \"2016-07-21T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Layer Normalization.\", \"Link\": \"https://arxiv.org/pdf/1607.06450.pdf\", \"Citations\": 4130.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.45411e+17, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 12.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"Named Entity Recognition model\", \"Domain\": \"All\", \"Task\": \"Named Entity Recognition model\", \"Organization(s)\": \"University of Toronto\", \"Author(s)\": \"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton\", \"Publication date\": \"2016-07-21T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Layer Normalization.\", \"Link\": \"https://arxiv.org/pdf/1607.06450.pdf\", \"Citations\": 4130.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 9.69408e+16, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 8.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": null, \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Tsinghua University, Princeton, Mila- Quebec AI, University de Montreal, HEC, CIFAR\", \"Author(s)\": \"Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang.\", \"Publication date\": \"2020-11-23T00:00:00\", \"Year\": \"2020\", \"Reference\": \"KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.\", \"Link\": \"https://arxiv.org/pdf/1911.06136.pdf\", \"Citations\": 96.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 110000000.0, \"Training compute (FLOPs)\": 1.24e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Wikipedia+BookCorpus\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": 1127272727272.7273, \"Training compute times parameters\": 1.364e+28, \"Era\": \"Large Scale Era\"}], \"data-1505ebec624515b18852e84cd646fcb0\": [{\"Era\": \"Pre Deep Learning Era\", \"start\": \"1952-01-01T00:00:00\", \"stop\": \"2009-12-31T00:00:00\"}, {\"Era\": \"Deep Learning Era\", \"start\": \"2009-12-31T00:00:00\", \"stop\": \"2012-09-30T00:00:00\"}, {\"Era\": \"Large Scale Era\", \"start\": \"2012-09-30T00:00:00\", \"stop\": \"2022-01-20T00:00:00\"}], \"data-ed0c35b2fcb8cb62e6d6e06ec64a1081\": [{\"Era\": \"Pre Deep Learning Era\", \"start\": \"1952-01-01T00:00:00\", \"stop\": \"2009-12-31T00:00:00\"}], \"data-e3f5f1d78994eed643e4fee58918f893\": [{\"Era\": \"Deep Learning Era\", \"start\": \"2009-12-31T00:00:00\", \"stop\": \"2012-09-30T00:00:00\"}, {\"Era\": \"Large Scale Era\", \"start\": \"2012-09-30T00:00:00\", \"stop\": \"2022-01-20T00:00:00\"}], \"data-89aa337f71f6c54ca6ff78582f6ba4a7\": [{\"Publication date\": \"1952-01-01\", \"Training compute (FLOPs)\": 85.09732154505848, \"Domain\": \"All\"}, {\"Publication date\": \"1952-04-16\", \"Training compute (FLOPs)\": 97.96309948855348, \"Domain\": \"All\"}, {\"Publication date\": \"1952-07-31\", \"Training compute (FLOPs)\": 112.77404138193528, \"Domain\": \"All\"}, {\"Publication date\": \"1952-11-15\", \"Training compute (FLOPs)\": 129.82423459444715, \"Domain\": \"All\"}, {\"Publication date\": \"1953-03-01\", \"Training compute (FLOPs)\": 149.45222926756514, \"Domain\": \"All\"}, {\"Publication date\": \"1953-06-16\", \"Training compute (FLOPs)\": 172.0477605957595, \"Domain\": \"All\"}, {\"Publication date\": \"1953-09-30\", \"Training compute (FLOPs)\": 198.05948744345568, \"Domain\": \"All\"}, {\"Publication date\": \"1954-01-15\", \"Training compute (FLOPs)\": 228.00390095500205, \"Domain\": \"All\"}, {\"Publication date\": \"1954-05-01\", \"Training compute (FLOPs)\": 262.4755800475625, \"Domain\": \"All\"}, {\"Publication date\": \"1954-08-16\", \"Training compute (FLOPs)\": 302.1589974240703, \"Domain\": \"All\"}, {\"Publication date\": \"1954-11-30\", \"Training compute (FLOPs)\": 347.84211052215716, \"Domain\": \"All\"}, {\"Publication date\": \"1955-03-16\", \"Training compute (FLOPs)\": 400.4320072678843, \"Domain\": \"All\"}, {\"Publication date\": \"1955-07-01\", \"Training compute (FLOPs)\": 460.97291729252277, \"Domain\": \"All\"}, {\"Publication date\": \"1955-10-15\", \"Training compute (FLOPs)\": 530.666946248656, \"Domain\": \"All\"}, {\"Publication date\": \"1956-01-30\", \"Training compute (FLOPs)\": 610.8979449270332, \"Domain\": \"All\"}, {\"Publication date\": \"1956-05-15\", \"Training compute (FLOPs)\": 703.258987118085, \"Domain\": \"All\"}, {\"Publication date\": \"1956-08-30\", \"Training compute (FLOPs)\": 809.5840018281393, \"Domain\": \"All\"}, {\"Publication date\": \"1956-12-14\", \"Training compute (FLOPs)\": 931.9841879333301, \"Domain\": \"All\"}, {\"Publication date\": \"1957-03-31\", \"Training compute (FLOPs)\": 1072.8899343315848, \"Domain\": \"All\"}, {\"Publication date\": \"1957-07-15\", \"Training compute (FLOPs)\": 1235.0990779609438, \"Domain\": \"All\"}, {\"Publication date\": \"1957-10-30\", \"Training compute (FLOPs)\": 1421.8324578935935, \"Domain\": \"All\"}, {\"Publication date\": \"1958-02-13\", \"Training compute (FLOPs)\": 1636.797868601928, \"Domain\": \"All\"}, {\"Publication date\": \"1958-05-30\", \"Training compute (FLOPs)\": 1884.263682255673, \"Domain\": \"All\"}, {\"Publication date\": \"1958-09-14\", \"Training compute (FLOPs)\": 2169.1436018904815, \"Domain\": \"All\"}, {\"Publication date\": \"1958-12-29\", \"Training compute (FLOPs)\": 2497.094228337889, \"Domain\": \"All\"}, {\"Publication date\": \"1959-04-15\", \"Training compute (FLOPs)\": 2874.6273781770083, \"Domain\": \"All\"}, {\"Publication date\": \"1959-07-30\", \"Training compute (FLOPs)\": 3309.2393829547814, \"Domain\": \"All\"}, {\"Publication date\": \"1959-11-14\", \"Training compute (FLOPs)\": 3809.5599369993724, \"Domain\": \"All\"}, {\"Publication date\": \"1960-02-28\", \"Training compute (FLOPs)\": 4385.523449386971, \"Domain\": \"All\"}, {\"Publication date\": \"1960-06-14\", \"Training compute (FLOPs)\": 5048.566302461697, \"Domain\": \"All\"}, {\"Publication date\": \"1960-09-28\", \"Training compute (FLOPs)\": 5811.853933625167, \"Domain\": \"All\"}, {\"Publication date\": \"1961-01-13\", \"Training compute (FLOPs)\": 6690.5422494549075, \"Domain\": \"All\"}, {\"Publication date\": \"1961-04-29\", \"Training compute (FLOPs)\": 7702.078562708002, \"Domain\": \"All\"}, {\"Publication date\": \"1961-08-13\", \"Training compute (FLOPs)\": 8866.548027692104, \"Domain\": \"All\"}, {\"Publication date\": \"1961-11-28\", \"Training compute (FLOPs)\": 10207.072452895402, \"Domain\": \"All\"}, {\"Publication date\": \"1962-03-14\", \"Training compute (FLOPs)\": 11750.269409600825, \"Domain\": \"All\"}, {\"Publication date\": \"1962-06-29\", \"Training compute (FLOPs)\": 13526.780752794153, \"Domain\": \"All\"}, {\"Publication date\": \"1962-10-13\", \"Training compute (FLOPs)\": 15571.881048510706, \"Domain\": \"All\"}, {\"Publication date\": \"1963-01-28\", \"Training compute (FLOPs)\": 17926.17798870965, \"Domain\": \"All\"}, {\"Publication date\": \"1963-05-14\", \"Training compute (FLOPs)\": 20636.41870125694, \"Domain\": \"All\"}, {\"Publication date\": \"1963-08-29\", \"Training compute (FLOPs)\": 23756.417964909167, \"Domain\": \"All\"}, {\"Publication date\": \"1963-12-13\", \"Training compute (FLOPs)\": 27348.126760438532, \"Domain\": \"All\"}, {\"Publication date\": \"1964-03-28\", \"Training compute (FLOPs)\": 31482.862374713455, \"Domain\": \"All\"}, {\"Publication date\": \"1964-07-13\", \"Training compute (FLOPs)\": 36242.72448301518, \"Domain\": \"All\"}, {\"Publication date\": \"1964-10-27\", \"Training compute (FLOPs)\": 41722.22532747718, \"Domain\": \"All\"}, {\"Publication date\": \"1965-02-11\", \"Training compute (FLOPs)\": 48030.166360469135, \"Domain\": \"All\"}, {\"Publication date\": \"1965-05-28\", \"Training compute (FLOPs)\": 55291.798615906475, \"Domain\": \"All\"}, {\"Publication date\": \"1965-09-12\", \"Training compute (FLOPs)\": 63651.30970480562, \"Domain\": \"All\"}, {\"Publication date\": \"1965-12-27\", \"Training compute (FLOPs)\": 73274.68681714291, \"Domain\": \"All\"}, {\"Publication date\": \"1966-04-13\", \"Training compute (FLOPs)\": 84353.01257823149, \"Domain\": \"All\"}, {\"Publication date\": \"1966-07-28\", \"Training compute (FLOPs)\": 97106.25920211877, \"Domain\": \"All\"}, {\"Publication date\": \"1966-11-12\", \"Training compute (FLOPs)\": 111787.65627948023, \"Domain\": \"All\"}, {\"Publication date\": \"1967-02-26\", \"Training compute (FLOPs)\": 128688.71892643716, \"Domain\": \"All\"}, {\"Publication date\": \"1967-06-12\", \"Training compute (FLOPs)\": 148145.03613453766, \"Domain\": \"All\"}, {\"Publication date\": \"1967-09-27\", \"Training compute (FLOPs)\": 170542.9342556482, \"Domain\": \"All\"}, {\"Publication date\": \"1968-01-11\", \"Training compute (FLOPs)\": 196327.1479315562, \"Domain\": \"All\"}, {\"Publication date\": \"1968-04-27\", \"Training compute (FLOPs)\": 226009.65078506595, \"Domain\": \"All\"}, {\"Publication date\": \"1968-08-11\", \"Training compute (FLOPs)\": 260179.82121220278, \"Domain\": \"All\"}, {\"Publication date\": \"1968-11-26\", \"Training compute (FLOPs)\": 299516.14513306785, \"Domain\": \"All\"}, {\"Publication date\": \"1969-03-12\", \"Training compute (FLOPs)\": 344799.6880674789, \"Domain\": \"All\"}, {\"Publication date\": \"1969-06-27\", \"Training compute (FLOPs)\": 396929.60404037626, \"Domain\": \"All\"}, {\"Publication date\": \"1969-10-11\", \"Training compute (FLOPs)\": 456940.9892649932, \"Domain\": \"All\"}, {\"Publication date\": \"1970-01-26\", \"Training compute (FLOPs)\": 526025.435102122, \"Domain\": \"All\"}, {\"Publication date\": \"1970-05-12\", \"Training compute (FLOPs)\": 605554.688406185, \"Domain\": \"All\"}, {\"Publication date\": \"1970-08-26\", \"Training compute (FLOPs)\": 697107.889050486, \"Domain\": \"All\"}, {\"Publication date\": \"1970-12-11\", \"Training compute (FLOPs)\": 802502.9254669163, \"Domain\": \"All\"}, {\"Publication date\": \"1971-03-27\", \"Training compute (FLOPs)\": 923832.5308021189, \"Domain\": \"All\"}, {\"Publication date\": \"1971-07-12\", \"Training compute (FLOPs)\": 1063505.8364064982, \"Domain\": \"All\"}, {\"Publication date\": \"1971-10-26\", \"Training compute (FLOPs)\": 1224296.2077646842, \"Domain\": \"All\"}, {\"Publication date\": \"1972-02-10\", \"Training compute (FLOPs)\": 1409396.3126826272, \"Domain\": \"All\"}, {\"Publication date\": \"1972-05-26\", \"Training compute (FLOPs)\": 1622481.5151793244, \"Domain\": \"All\"}, {\"Publication date\": \"1972-09-10\", \"Training compute (FLOPs)\": 1867782.8538452662, \"Domain\": \"All\"}, {\"Publication date\": \"1972-12-25\", \"Training compute (FLOPs)\": 2150171.0537078977, \"Domain\": \"All\"}, {\"Publication date\": \"1973-04-11\", \"Training compute (FLOPs)\": 2475253.239789263, \"Domain\": \"All\"}, {\"Publication date\": \"1973-07-26\", \"Training compute (FLOPs)\": 2849484.272669774, \"Domain\": \"All\"}, {\"Publication date\": \"1973-11-09\", \"Training compute (FLOPs)\": 3280294.916767252, \"Domain\": \"All\"}, {\"Publication date\": \"1974-02-24\", \"Training compute (FLOPs)\": 3776239.386254751, \"Domain\": \"All\"}, {\"Publication date\": \"1974-06-10\", \"Training compute (FLOPs)\": 4347165.198288782, \"Domain\": \"All\"}, {\"Publication date\": \"1974-09-25\", \"Training compute (FLOPs)\": 5004408.706185308, \"Domain\": \"All\"}, {\"Publication date\": \"1975-01-09\", \"Training compute (FLOPs)\": 5761020.195052145, \"Domain\": \"All\"}, {\"Publication date\": \"1975-04-26\", \"Training compute (FLOPs)\": 6632023.009380268, \"Domain\": \"All\"}, {\"Publication date\": \"1975-08-10\", \"Training compute (FLOPs)\": 7634711.857935819, \"Domain\": \"All\"}, {\"Publication date\": \"1975-11-25\", \"Training compute (FLOPs)\": 8788996.218986597, \"Domain\": \"All\"}, {\"Publication date\": \"1976-03-10\", \"Training compute (FLOPs)\": 10117795.664687425, \"Domain\": \"All\"}, {\"Publication date\": \"1976-06-24\", \"Training compute (FLOPs)\": 11647494.954125538, \"Domain\": \"All\"}, {\"Publication date\": \"1976-10-09\", \"Training compute (FLOPs)\": 13408467.931382068, \"Domain\": \"All\"}, {\"Publication date\": \"1977-01-23\", \"Training compute (FLOPs)\": 15435680.631337378, \"Domain\": \"All\"}, {\"Publication date\": \"1977-05-10\", \"Training compute (FLOPs)\": 17769385.56827056, \"Domain\": \"All\"}, {\"Publication date\": \"1977-08-24\", \"Training compute (FLOPs)\": 20455920.99339158, \"Domain\": \"All\"}, {\"Publication date\": \"1977-12-09\", \"Training compute (FLOPs)\": 23548630.99124268, \"Domain\": \"All\"}, {\"Publication date\": \"1978-03-25\", \"Training compute (FLOPs)\": 27108924.684460882, \"Domain\": \"All\"}, {\"Publication date\": \"1978-07-10\", \"Training compute (FLOPs)\": 31207495.57889167, \"Domain\": \"All\"}, {\"Publication date\": \"1978-10-24\", \"Training compute (FLOPs)\": 35925725.25992841, \"Domain\": \"All\"}, {\"Publication date\": \"1979-02-08\", \"Training compute (FLOPs)\": 41357299.31256778, \"Domain\": \"All\"}, {\"Publication date\": \"1979-05-25\", \"Training compute (FLOPs)\": 47610067.550645135, \"Domain\": \"All\"}, {\"Publication date\": \"1979-09-08\", \"Training compute (FLOPs)\": 54808185.49207836, \"Domain\": \"All\"}, {\"Publication date\": \"1979-12-24\", \"Training compute (FLOPs)\": 63094579.60210218, \"Domain\": \"All\"}, {\"Publication date\": \"1980-04-08\", \"Training compute (FLOPs)\": 72633785.25346917, \"Domain\": \"All\"}, {\"Publication date\": \"1980-07-24\", \"Training compute (FLOPs)\": 83615213.75560546, \"Domain\": \"All\"}, {\"Publication date\": \"1980-11-07\", \"Training compute (FLOPs)\": 96256913.32205024, \"Domain\": \"All\"}, {\"Publication date\": \"1981-02-22\", \"Training compute (FLOPs)\": 110809898.65510811, \"Domain\": \"All\"}, {\"Publication date\": \"1981-06-08\", \"Training compute (FLOPs)\": 127563135.11612427, \"Domain\": \"All\"}, {\"Publication date\": \"1981-09-23\", \"Training compute (FLOPs)\": 146849276.44680175, \"Domain\": \"All\"}, {\"Publication date\": \"1982-01-07\", \"Training compute (FLOPs)\": 169051269.97176373, \"Domain\": \"All\"}, {\"Publication date\": \"1982-04-24\", \"Training compute (FLOPs)\": 194609960.43394908, \"Domain\": \"All\"}, {\"Publication date\": \"1982-08-08\", \"Training compute (FLOPs)\": 224032843.44689783, \"Domain\": \"All\"}, {\"Publication date\": \"1982-11-22\", \"Training compute (FLOPs)\": 257904142.37283254, \"Domain\": \"All\"}, {\"Publication date\": \"1983-03-09\", \"Training compute (FLOPs)\": 296896408.71264535, \"Domain\": \"All\"}, {\"Publication date\": \"1983-06-23\", \"Training compute (FLOPs)\": 341783876.348104, \"Domain\": \"All\"}, {\"Publication date\": \"1983-10-08\", \"Training compute (FLOPs)\": 393457834.7987333, \"Domain\": \"All\"}, {\"Publication date\": \"1984-01-22\", \"Training compute (FLOPs)\": 452944326.74394065, \"Domain\": \"All\"}, {\"Publication date\": \"1984-05-08\", \"Training compute (FLOPs)\": 521424521.2188541, \"Domain\": \"All\"}, {\"Publication date\": \"1984-08-22\", \"Training compute (FLOPs)\": 600258167.0080798, \"Domain\": \"All\"}, {\"Publication date\": \"1984-12-07\", \"Training compute (FLOPs)\": 691010591.9408213, \"Domain\": \"All\"}, {\"Publication date\": \"1985-03-23\", \"Training compute (FLOPs)\": 795483784.1768624, \"Domain\": \"All\"}, {\"Publication date\": \"1985-07-08\", \"Training compute (FLOPs)\": 915752172.6413748, \"Domain\": \"All\"}, {\"Publication date\": \"1985-10-22\", \"Training compute (FLOPs)\": 1054203817.0721874, \"Domain\": \"All\"}, {\"Publication date\": \"1986-02-05\", \"Training compute (FLOPs)\": 1213587825.5389402, \"Domain\": \"All\"}, {\"Publication date\": \"1986-05-23\", \"Training compute (FLOPs)\": 1397068940.9821048, \"Domain\": \"All\"}, {\"Publication date\": \"1986-09-06\", \"Training compute (FLOPs)\": 1608290380.6249945, \"Domain\": \"All\"}, {\"Publication date\": \"1986-12-22\", \"Training compute (FLOPs)\": 1851446176.0142066, \"Domain\": \"All\"}, {\"Publication date\": \"1987-04-07\", \"Training compute (FLOPs)\": 2131364450.0871336, \"Domain\": \"All\"}, {\"Publication date\": \"1987-07-23\", \"Training compute (FLOPs)\": 2453603284.7954717, \"Domain\": \"All\"}, {\"Publication date\": \"1987-11-06\", \"Training compute (FLOPs)\": 2824561082.8842597, \"Domain\": \"All\"}, {\"Publication date\": \"1988-02-21\", \"Training compute (FLOPs)\": 3251603615.135168, \"Domain\": \"All\"}, {\"Publication date\": \"1988-06-06\", \"Training compute (FLOPs)\": 3743210275.756603, \"Domain\": \"All\"}, {\"Publication date\": \"1988-09-20\", \"Training compute (FLOPs)\": 4309142449.997564, \"Domain\": \"All\"}, {\"Publication date\": \"1989-01-05\", \"Training compute (FLOPs)\": 4960637337.051105, \"Domain\": \"All\"}, {\"Publication date\": \"1989-04-21\", \"Training compute (FLOPs)\": 5710631076.900088, \"Domain\": \"All\"}, {\"Publication date\": \"1989-08-06\", \"Training compute (FLOPs)\": 6574015611.441617, \"Domain\": \"All\"}, {\"Publication date\": \"1989-11-20\", \"Training compute (FLOPs)\": 7567934380.194287, \"Domain\": \"All\"}, {\"Publication date\": \"1990-03-07\", \"Training compute (FLOPs)\": 8712122721.954891, \"Domain\": \"All\"}, {\"Publication date\": \"1990-06-21\", \"Training compute (FLOPs)\": 10029299741.419586, \"Domain\": \"All\"}, {\"Publication date\": \"1990-10-06\", \"Training compute (FLOPs)\": 11545619421.746225, \"Domain\": \"All\"}, {\"Publication date\": \"1991-01-20\", \"Training compute (FLOPs)\": 13291189940.346268, \"Domain\": \"All\"}, {\"Publication date\": \"1991-05-07\", \"Training compute (FLOPs)\": 15300671499.497894, \"Domain\": \"All\"}, {\"Publication date\": \"1991-08-21\", \"Training compute (FLOPs)\": 17613964542.398827, \"Domain\": \"All\"}, {\"Publication date\": \"1991-12-05\", \"Training compute (FLOPs)\": 20277002019.872257, \"Domain\": \"All\"}, {\"Publication date\": \"1992-03-21\", \"Training compute (FLOPs)\": 23342661439.121338, \"Domain\": \"All\"}, {\"Publication date\": \"1992-07-05\", \"Training compute (FLOPs)\": 26871814804.18107, \"Domain\": \"All\"}, {\"Publication date\": \"1992-10-20\", \"Training compute (FLOPs)\": 30934537295.702206, \"Domain\": \"All\"}, {\"Publication date\": \"1993-02-03\", \"Training compute (FLOPs)\": 35611498690.09955, \"Domain\": \"All\"}, {\"Publication date\": \"1993-05-21\", \"Training compute (FLOPs)\": 40995565145.578835, \"Domain\": \"All\"}, {\"Publication date\": \"1993-09-04\", \"Training compute (FLOPs)\": 47193643160.899506, \"Domain\": \"All\"}, {\"Publication date\": \"1993-12-20\", \"Training compute (FLOPs)\": 54328802320.239395, \"Domain\": \"All\"}, {\"Publication date\": \"1994-04-05\", \"Training compute (FLOPs)\": 62542718973.51654, \"Domain\": \"All\"}, {\"Publication date\": \"1994-07-21\", \"Training compute (FLOPs)\": 71998489374.85428, \"Domain\": \"All\"}, {\"Publication date\": \"1994-11-04\", \"Training compute (FLOPs)\": 82883868135.84714, \"Domain\": \"All\"}, {\"Publication date\": \"1995-02-18\", \"Training compute (FLOPs)\": 95414996297.96088, \"Domain\": \"All\"}, {\"Publication date\": \"1995-06-05\", \"Training compute (FLOPs)\": 109840693048.1077, \"Domain\": \"All\"}, {\"Publication date\": \"1995-09-19\", \"Training compute (FLOPs)\": 126447396293.97046, \"Domain\": \"All\"}, {\"Publication date\": \"1996-01-04\", \"Training compute (FLOPs)\": 145564850201.09982, \"Domain\": \"All\"}, {\"Publication date\": \"1996-04-19\", \"Training compute (FLOPs)\": 167572652621.45953, \"Domain\": \"All\"}, {\"Publication date\": \"1996-08-04\", \"Training compute (FLOPs)\": 192907792422.60767, \"Domain\": \"All\"}, {\"Publication date\": \"1996-11-18\", \"Training compute (FLOPs)\": 222073326376.16394, \"Domain\": \"All\"}, {\"Publication date\": \"1997-03-05\", \"Training compute (FLOPs)\": 255648367898.66565, \"Domain\": \"All\"}, {\"Publication date\": \"1997-06-19\", \"Training compute (FLOPs)\": 294299585978.1054, \"Domain\": \"All\"}, {\"Publication date\": \"1997-10-04\", \"Training compute (FLOPs)\": 338794442611.7769, \"Domain\": \"All\"}, {\"Publication date\": \"1998-01-18\", \"Training compute (FLOPs)\": 390016431600.7024, \"Domain\": \"All\"}, {\"Publication date\": \"1998-05-04\", \"Training compute (FLOPs)\": 448982621278.38824, \"Domain\": \"All\"}, {\"Publication date\": \"1998-08-19\", \"Training compute (FLOPs)\": 516863849511.84607, \"Domain\": \"All\"}, {\"Publication date\": \"1998-12-03\", \"Training compute (FLOPs)\": 595007971960.4579, \"Domain\": \"All\"}, {\"Publication date\": \"1999-03-20\", \"Training compute (FLOPs)\": 684966625216.4204, \"Domain\": \"All\"}, {\"Publication date\": \"1999-07-04\", \"Training compute (FLOPs)\": 788526036238.6409, \"Domain\": \"All\"}, {\"Publication date\": \"1999-10-19\", \"Training compute (FLOPs)\": 907742489832.1765, \"Domain\": \"All\"}, {\"Publication date\": \"2000-02-02\", \"Training compute (FLOPs)\": 1044983158422.1063, \"Domain\": \"All\"}, {\"Publication date\": \"2000-05-19\", \"Training compute (FLOPs)\": 1202973104838.2139, \"Domain\": \"All\"}, {\"Publication date\": \"2000-09-02\", \"Training compute (FLOPs)\": 1384849391398.5393, \"Domain\": \"All\"}, {\"Publication date\": \"2000-12-17\", \"Training compute (FLOPs)\": 1594223369703.541, \"Domain\": \"All\"}, {\"Publication date\": \"2001-04-03\", \"Training compute (FLOPs)\": 1835252387944.4465, \"Domain\": \"All\"}, {\"Publication date\": \"2001-07-18\", \"Training compute (FLOPs)\": 2112722339581.6414, \"Domain\": \"All\"}, {\"Publication date\": \"2001-11-02\", \"Training compute (FLOPs)\": 2432142692467.164, \"Domain\": \"All\"}, {\"Publication date\": \"2002-02-16\", \"Training compute (FLOPs)\": 2799855885315.5195, \"Domain\": \"All\"}, {\"Publication date\": \"2002-06-03\", \"Training compute (FLOPs)\": 3223163263742.5493, \"Domain\": \"All\"}, {\"Publication date\": \"2002-09-17\", \"Training compute (FLOPs)\": 3710470056412.6914, \"Domain\": \"All\"}, {\"Publication date\": \"2003-01-02\", \"Training compute (FLOPs)\": 4271452270012.87, \"Domain\": \"All\"}, {\"Publication date\": \"2003-04-18\", \"Training compute (FLOPs)\": 4917248816894.347, \"Domain\": \"All\"}, {\"Publication date\": \"2003-08-03\", \"Training compute (FLOPs)\": 5660682690281.045, \"Domain\": \"All\"}, {\"Publication date\": \"2003-11-17\", \"Training compute (FLOPs)\": 6516515578787.056, \"Domain\": \"All\"}, {\"Publication date\": \"2004-03-02\", \"Training compute (FLOPs)\": 7501740975776.87, \"Domain\": \"All\"}, {\"Publication date\": \"2004-06-17\", \"Training compute (FLOPs)\": 8635921603693.088, \"Domain\": \"All\"}, {\"Publication date\": \"2004-10-01\", \"Training compute (FLOPs)\": 9941577853197.975, \"Domain\": \"All\"}, {\"Publication date\": \"2005-01-16\", \"Training compute (FLOPs)\": 11444634950000.809, \"Domain\": \"All\"}, {\"Publication date\": \"2005-05-02\", \"Training compute (FLOPs)\": 13174937728493.018, \"Domain\": \"All\"}, {\"Publication date\": \"2005-08-17\", \"Training compute (FLOPs)\": 15166843233349.752, \"Domain\": \"All\"}, {\"Publication date\": \"2005-12-01\", \"Training compute (FLOPs)\": 17459902916098.213, \"Domain\": \"All\"}, {\"Publication date\": \"2006-03-18\", \"Training compute (FLOPs)\": 20099647972180.305, \"Domain\": \"All\"}, {\"Publication date\": \"2006-07-02\", \"Training compute (FLOPs)\": 23138493412404.48, \"Domain\": \"All\"}, {\"Publication date\": \"2006-10-17\", \"Training compute (FLOPs)\": 26636778820066.547, \"Domain\": \"All\"}, {\"Publication date\": \"2007-01-31\", \"Training compute (FLOPs)\": 30663966458957.25, \"Domain\": \"All\"}, {\"Publication date\": \"2007-05-17\", \"Training compute (FLOPs)\": 35300020522308.73, \"Domain\": \"All\"}, {\"Publication date\": \"2007-09-01\", \"Training compute (FLOPs)\": 40636994908772.52, \"Domain\": \"All\"}, {\"Publication date\": \"2007-12-16\", \"Training compute (FLOPs)\": 46780861052789.42, \"Domain\": \"All\"}, {\"Publication date\": \"2008-04-01\", \"Training compute (FLOPs)\": 53853612102825.64, \"Domain\": \"All\"}, {\"Publication date\": \"2008-07-16\", \"Training compute (FLOPs)\": 61995685227958.76, \"Domain\": \"All\"}, {\"Publication date\": \"2008-10-31\", \"Training compute (FLOPs)\": 71368750150753.0, \"Domain\": \"All\"}, {\"Publication date\": \"2009-02-14\", \"Training compute (FLOPs)\": 82158919275662.95, \"Domain\": \"All\"}, {\"Publication date\": \"2009-06-01\", \"Training compute (FLOPs)\": 94580443153103.88, \"Domain\": \"All\"}, {\"Publication date\": \"2009-09-15\", \"Training compute (FLOPs)\": 108879964657674.08, \"Domain\": \"All\"}, {\"Publication date\": \"2009-12-31\", \"Training compute (FLOPs)\": 125341416350245.75, \"Domain\": \"All\"}, {\"Publication date\": \"1952-01-01\", \"Training compute (FLOPs)\": 122489.22654245842, \"Domain\": \"Record\"}, {\"Publication date\": \"1952-04-16\", \"Training compute (FLOPs)\": 138577.50750007492, \"Domain\": \"Record\"}, {\"Publication date\": \"1952-07-31\", \"Training compute (FLOPs)\": 156778.8949854034, \"Domain\": \"Record\"}, {\"Publication date\": \"1952-11-15\", \"Training compute (FLOPs)\": 177370.93382790123, \"Domain\": \"Record\"}, {\"Publication date\": \"1953-03-01\", \"Training compute (FLOPs)\": 200667.62283221085, \"Domain\": \"Record\"}, {\"Publication date\": \"1953-06-16\", \"Training compute (FLOPs)\": 227024.2028054101, \"Domain\": \"Record\"}, {\"Publication date\": \"1953-09-30\", \"Training compute (FLOPs)\": 256842.57346551307, \"Domain\": \"Record\"}, {\"Publication date\": \"1954-01-15\", \"Training compute (FLOPs)\": 290577.4218309465, \"Domain\": \"Record\"}, {\"Publication date\": \"1954-05-01\", \"Training compute (FLOPs)\": 328743.15553925483, \"Domain\": \"Record\"}, {\"Publication date\": \"1954-08-16\", \"Training compute (FLOPs)\": 371921.74682050117, \"Domain\": \"Record\"}, {\"Publication date\": \"1954-11-30\", \"Training compute (FLOPs)\": 420771.60673038196, \"Domain\": \"Record\"}, {\"Publication date\": \"1955-03-16\", \"Training compute (FLOPs)\": 476037.6249684806, \"Domain\": \"Record\"}, {\"Publication date\": \"1955-07-01\", \"Training compute (FLOPs)\": 538562.5283669784, \"Domain\": \"Record\"}, {\"Publication date\": \"1955-10-15\", \"Training compute (FLOPs)\": 609299.7312556657, \"Domain\": \"Record\"}, {\"Publication date\": \"1956-01-30\", \"Training compute (FLOPs)\": 689327.873653052, \"Domain\": \"Record\"}, {\"Publication date\": \"1956-05-15\", \"Training compute (FLOPs)\": 779867.268963892, \"Domain\": \"Record\"}, {\"Publication date\": \"1956-08-30\", \"Training compute (FLOPs)\": 882298.511995168, \"Domain\": \"Record\"}, {\"Publication date\": \"1956-12-14\", \"Training compute (FLOPs)\": 998183.5310307724, \"Domain\": \"Record\"}, {\"Publication date\": \"1957-03-31\", \"Training compute (FLOPs)\": 1129289.4049740797, \"Domain\": \"Record\"}, {\"Publication date\": \"1957-07-15\", \"Training compute (FLOPs)\": 1277615.308750334, \"Domain\": \"Record\"}, {\"Publication date\": \"1957-10-30\", \"Training compute (FLOPs)\": 1445422.9978274012, \"Domain\": \"Record\"}, {\"Publication date\": \"1958-02-13\", \"Training compute (FLOPs)\": 1635271.2967191152, \"Domain\": \"Record\"}, {\"Publication date\": \"1958-05-30\", \"Training compute (FLOPs)\": 1850055.1173551579, \"Domain\": \"Record\"}, {\"Publication date\": \"1958-09-14\", \"Training compute (FLOPs)\": 2093049.6023008914, \"Domain\": \"Record\"}, {\"Publication date\": \"1958-12-29\", \"Training compute (FLOPs)\": 2367960.0659450046, \"Domain\": \"Record\"}, {\"Publication date\": \"1959-04-15\", \"Training compute (FLOPs)\": 2678978.495179138, \"Domain\": \"Record\"}, {\"Publication date\": \"1959-07-30\", \"Training compute (FLOPs)\": 3030847.4711409938, \"Domain\": \"Record\"}, {\"Publication date\": \"1959-11-14\", \"Training compute (FLOPs)\": 3428932.486713491, \"Domain\": \"Record\"}, {\"Publication date\": \"1960-02-28\", \"Training compute (FLOPs)\": 3879303.762523802, \"Domain\": \"Record\"}, {\"Publication date\": \"1960-06-14\", \"Training compute (FLOPs)\": 4388828.809037081, \"Domain\": \"Record\"}, {\"Publication date\": \"1960-09-28\", \"Training compute (FLOPs)\": 4965277.146150181, \"Domain\": \"Record\"}, {\"Publication date\": \"1961-01-13\", \"Training compute (FLOPs)\": 5617438.777132533, \"Domain\": \"Record\"}, {\"Publication date\": \"1961-04-29\", \"Training compute (FLOPs)\": 6355258.223463992, \"Domain\": \"Record\"}, {\"Publication date\": \"1961-08-13\", \"Training compute (FLOPs)\": 7189986.164389067, \"Domain\": \"Record\"}, {\"Publication date\": \"1961-11-28\", \"Training compute (FLOPs)\": 8134350.993523588, \"Domain\": \"Record\"}, {\"Publication date\": \"1962-03-14\", \"Training compute (FLOPs)\": 9202752.908411356, \"Domain\": \"Record\"}, {\"Publication date\": \"1962-06-29\", \"Training compute (FLOPs)\": 10411483.492751552, \"Domain\": \"Record\"}, {\"Publication date\": \"1962-10-13\", \"Training compute (FLOPs)\": 11778974.139446648, \"Domain\": \"Record\"}, {\"Publication date\": \"1963-01-28\", \"Training compute (FLOPs)\": 13326077.102679329, \"Domain\": \"Record\"}, {\"Publication date\": \"1963-05-14\", \"Training compute (FLOPs)\": 15076383.464663064, \"Domain\": \"Record\"}, {\"Publication date\": \"1963-08-29\", \"Training compute (FLOPs)\": 17056582.865466587, \"Domain\": \"Record\"}, {\"Publication date\": \"1963-12-13\", \"Training compute (FLOPs)\": 19296870.481456533, \"Domain\": \"Record\"}, {\"Publication date\": \"1964-03-28\", \"Training compute (FLOPs)\": 21831407.458079506, \"Domain\": \"Record\"}, {\"Publication date\": \"1964-07-13\", \"Training compute (FLOPs)\": 24698841.817819223, \"Domain\": \"Record\"}, {\"Publication date\": \"1964-10-27\", \"Training compute (FLOPs)\": 27942897.786748517, \"Domain\": \"Record\"}, {\"Publication date\": \"1965-02-11\", \"Training compute (FLOPs)\": 31613042.525619425, \"Domain\": \"Record\"}, {\"Publication date\": \"1965-05-28\", \"Training compute (FLOPs)\": 35765240.43259982, \"Domain\": \"Record\"}, {\"Publication date\": \"1965-09-12\", \"Training compute (FLOPs)\": 40462806.519335784, \"Domain\": \"Record\"}, {\"Publication date\": \"1965-12-27\", \"Training compute (FLOPs)\": 45777371.87330267, \"Domain\": \"Record\"}, {\"Publication date\": \"1966-04-13\", \"Training compute (FLOPs)\": 51789975.92829701, \"Domain\": \"Record\"}, {\"Publication date\": \"1966-07-28\", \"Training compute (FLOPs)\": 58592302.19846757, \"Domain\": \"Record\"}, {\"Publication date\": \"1966-11-12\", \"Training compute (FLOPs)\": 66288076.31946765, \"Domain\": \"Record\"}, {\"Publication date\": \"1967-02-26\", \"Training compute (FLOPs)\": 74994647.71411887, \"Domain\": \"Record\"}, {\"Publication date\": \"1967-06-12\", \"Training compute (FLOPs)\": 84844779.00157535, \"Domain\": \"Record\"}, {\"Publication date\": \"1967-09-27\", \"Training compute (FLOPs)\": 95988670.43508908, \"Domain\": \"Record\"}, {\"Publication date\": \"1968-01-11\", \"Training compute (FLOPs)\": 108596250.23862775, \"Domain\": \"Record\"}, {\"Publication date\": \"1968-04-27\", \"Training compute (FLOPs)\": 122859765.76654188, \"Domain\": \"Record\"}, {\"Publication date\": \"1968-08-11\", \"Training compute (FLOPs)\": 138996714.9973702, \"Domain\": \"Record\"}, {\"Publication date\": \"1968-11-26\", \"Training compute (FLOPs)\": 157253163.06375015, \"Domain\": \"Record\"}, {\"Publication date\": \"1969-03-12\", \"Training compute (FLOPs)\": 177907494.38959906, \"Domain\": \"Record\"}, {\"Publication date\": \"1969-06-27\", \"Training compute (FLOPs)\": 201274657.64946672, \"Domain\": \"Record\"}, {\"Publication date\": \"1969-10-11\", \"Training compute (FLOPs)\": 227710968.2812826, \"Domain\": \"Record\"}, {\"Publication date\": \"1970-01-26\", \"Training compute (FLOPs)\": 257619541.7799511, \"Domain\": \"Record\"}, {\"Publication date\": \"1970-05-12\", \"Training compute (FLOPs)\": 291456440.6266559, \"Domain\": \"Record\"}, {\"Publication date\": \"1970-08-26\", \"Training compute (FLOPs)\": 329737628.581617, \"Domain\": \"Record\"}, {\"Publication date\": \"1970-12-11\", \"Training compute (FLOPs)\": 373046838.3844922, \"Domain\": \"Record\"}, {\"Publication date\": \"1971-03-27\", \"Training compute (FLOPs)\": 422044472.83522415, \"Domain\": \"Record\"}, {\"Publication date\": \"1971-07-12\", \"Training compute (FLOPs)\": 477477674.98079103, \"Domain\": \"Record\"}, {\"Publication date\": \"1971-10-26\", \"Training compute (FLOPs)\": 540191720.966033, \"Domain\": \"Record\"}, {\"Publication date\": \"1972-02-10\", \"Training compute (FLOPs)\": 611142909.2724317, \"Domain\": \"Record\"}, {\"Publication date\": \"1972-05-26\", \"Training compute (FLOPs)\": 691413142.8853599, \"Domain\": \"Record\"}, {\"Publication date\": \"1972-09-10\", \"Training compute (FLOPs)\": 782226426.7517796, \"Domain\": \"Record\"}, {\"Publication date\": \"1972-12-25\", \"Training compute (FLOPs)\": 884967532.0823193, \"Domain\": \"Record\"}, {\"Publication date\": \"1973-04-11\", \"Training compute (FLOPs)\": 1001203112.1111429, \"Domain\": \"Record\"}, {\"Publication date\": \"1973-07-26\", \"Training compute (FLOPs)\": 1132705591.2926352, \"Domain\": \"Record\"}, {\"Publication date\": \"1973-11-09\", \"Training compute (FLOPs)\": 1281480192.2054942, \"Domain\": \"Record\"}, {\"Publication date\": \"1974-02-24\", \"Training compute (FLOPs)\": 1449795512.3023922, \"Domain\": \"Record\"}, {\"Publication date\": \"1974-06-10\", \"Training compute (FLOPs)\": 1640218116.7338717, \"Domain\": \"Record\"}, {\"Publication date\": \"1974-09-25\", \"Training compute (FLOPs)\": 1855651674.7589946, \"Domain\": \"Record\"}, {\"Publication date\": \"1975-01-09\", \"Training compute (FLOPs)\": 2099381236.4984176, \"Domain\": \"Record\"}, {\"Publication date\": \"1975-04-26\", \"Training compute (FLOPs)\": 2375123325.1973557, \"Domain\": \"Record\"}, {\"Publication date\": \"1975-08-10\", \"Training compute (FLOPs)\": 2687082608.830275, \"Domain\": \"Record\"}, {\"Publication date\": \"1975-11-25\", \"Training compute (FLOPs)\": 3040016015.1997046, \"Domain\": \"Record\"}, {\"Publication date\": \"1976-03-10\", \"Training compute (FLOPs)\": 3439305268.2119555, \"Domain\": \"Record\"}, {\"Publication date\": \"1976-06-24\", \"Training compute (FLOPs)\": 3891038951.3785653, \"Domain\": \"Record\"}, {\"Publication date\": \"1976-10-09\", \"Training compute (FLOPs)\": 4402105349.898328, \"Domain\": \"Record\"}, {\"Publication date\": \"1977-01-23\", \"Training compute (FLOPs)\": 4980297487.065275, \"Domain\": \"Record\"}, {\"Publication date\": \"1977-05-10\", \"Training compute (FLOPs)\": 5634431956.570184, \"Domain\": \"Record\"}, {\"Publication date\": \"1977-08-24\", \"Training compute (FLOPs)\": 6374483362.825513, \"Domain\": \"Record\"}, {\"Publication date\": \"1977-12-09\", \"Training compute (FLOPs)\": 7211736419.245972, \"Domain\": \"Record\"}, {\"Publication date\": \"1978-03-25\", \"Training compute (FLOPs)\": 8158958023.791633, \"Domain\": \"Record\"}, {\"Publication date\": \"1978-07-10\", \"Training compute (FLOPs)\": 9230591935.714975, \"Domain\": \"Record\"}, {\"Publication date\": \"1978-10-24\", \"Training compute (FLOPs)\": 10442979021.981829, \"Domain\": \"Record\"}, {\"Publication date\": \"1979-02-08\", \"Training compute (FLOPs)\": 11814606431.857763, \"Domain\": \"Record\"}, {\"Publication date\": \"1979-05-25\", \"Training compute (FLOPs)\": 13366389499.198477, \"Domain\": \"Record\"}, {\"Publication date\": \"1979-09-08\", \"Training compute (FLOPs)\": 15121990671.008472, \"Domain\": \"Record\"}, {\"Publication date\": \"1979-12-24\", \"Training compute (FLOPs)\": 17108180325.577045, \"Domain\": \"Record\"}, {\"Publication date\": \"1980-04-08\", \"Training compute (FLOPs)\": 19355244981.96169, \"Domain\": \"Record\"}, {\"Publication date\": \"1980-07-24\", \"Training compute (FLOPs)\": 21897449125.637577, \"Domain\": \"Record\"}, {\"Publication date\": \"1980-11-07\", \"Training compute (FLOPs)\": 24773557692.359932, \"Domain\": \"Record\"}, {\"Publication date\": \"1981-02-22\", \"Training compute (FLOPs)\": 28027427177.250946, \"Domain\": \"Record\"}, {\"Publication date\": \"1981-06-08\", \"Training compute (FLOPs)\": 31708674383.04078, \"Domain\": \"Record\"}, {\"Publication date\": \"1981-09-23\", \"Training compute (FLOPs)\": 35873433004.41315, \"Domain\": \"Record\"}, {\"Publication date\": \"1982-01-07\", \"Training compute (FLOPs)\": 40585209585.753365, \"Domain\": \"Record\"}, {\"Publication date\": \"1982-04-24\", \"Training compute (FLOPs)\": 45915851904.02304, \"Domain\": \"Record\"}, {\"Publication date\": \"1982-08-08\", \"Training compute (FLOPs)\": 51946644543.4408, \"Domain\": \"Record\"}, {\"Publication date\": \"1982-11-22\", \"Training compute (FLOPs)\": 58769548367.83871, \"Domain\": \"Record\"}, {\"Publication date\": \"1983-03-09\", \"Training compute (FLOPs)\": 66488602790.68855, \"Domain\": \"Record\"}, {\"Publication date\": \"1983-06-23\", \"Training compute (FLOPs)\": 75221512225.83122, \"Domain\": \"Record\"}, {\"Publication date\": \"1983-10-08\", \"Training compute (FLOPs)\": 85101440909.45245, \"Domain\": \"Record\"}, {\"Publication date\": \"1984-01-22\", \"Training compute (FLOPs)\": 96279043461.85744, \"Domain\": \"Record\"}, {\"Publication date\": \"1984-05-08\", \"Training compute (FLOPs)\": 108924762153.0029, \"Domain\": \"Record\"}, {\"Publication date\": \"1984-08-22\", \"Training compute (FLOPs)\": 123231425899.88863, \"Domain\": \"Record\"}, {\"Publication date\": \"1984-12-07\", \"Training compute (FLOPs)\": 139417190629.14737, \"Domain\": \"Record\"}, {\"Publication date\": \"1985-03-23\", \"Training compute (FLOPs)\": 157728865839.17844, \"Domain\": \"Record\"}, {\"Publication date\": \"1985-07-08\", \"Training compute (FLOPs)\": 178445678087.70135, \"Domain\": \"Record\"}, {\"Publication date\": \"1985-10-22\", \"Training compute (FLOPs)\": 201883528793.30765, \"Domain\": \"Record\"}, {\"Publication date\": \"1986-02-05\", \"Training compute (FLOPs)\": 228399811274.82007, \"Domain\": \"Record\"}, {\"Publication date\": \"1986-05-23\", \"Training compute (FLOPs)\": 258398860482.48386, \"Domain\": \"Record\"}, {\"Publication date\": \"1986-09-06\", \"Training compute (FLOPs)\": 292338118521.2134, \"Domain\": \"Record\"}, {\"Publication date\": \"1986-12-22\", \"Training compute (FLOPs)\": 330735109980.3765, \"Domain\": \"Record\"}, {\"Publication date\": \"1987-04-07\", \"Training compute (FLOPs)\": 374175333436.20905, \"Domain\": \"Record\"}, {\"Publication date\": \"1987-07-23\", \"Training compute (FLOPs)\": 423321189456.8192, \"Domain\": \"Record\"}, {\"Publication date\": \"1987-11-06\", \"Training compute (FLOPs)\": 478922081253.6359, \"Domain\": \"Record\"}, {\"Publication date\": \"1988-02-21\", \"Training compute (FLOPs)\": 541825841996.8624, \"Domain\": \"Record\"}, {\"Publication date\": \"1988-06-06\", \"Training compute (FLOPs)\": 612991663042.5286, \"Domain\": \"Record\"}, {\"Publication date\": \"1988-09-20\", \"Training compute (FLOPs)\": 693504720216.0164, \"Domain\": \"Record\"}, {\"Publication date\": \"1989-01-05\", \"Training compute (FLOPs)\": 784592721170.9496, \"Domain\": \"Record\"}, {\"Publication date\": \"1989-04-21\", \"Training compute (FLOPs)\": 887644626157.2521, \"Domain\": \"Record\"}, {\"Publication date\": \"1989-08-06\", \"Training compute (FLOPs)\": 1004231827654.4076, \"Domain\": \"Record\"}, {\"Publication date\": \"1989-11-20\", \"Training compute (FLOPs)\": 1136132111833.968, \"Domain\": \"Record\"}, {\"Publication date\": \"1990-03-07\", \"Training compute (FLOPs)\": 1285356767227.6538, \"Domain\": \"Record\"}, {\"Publication date\": \"1990-06-21\", \"Training compute (FLOPs)\": 1454181253965.3845, \"Domain\": \"Record\"}, {\"Publication date\": \"1990-10-06\", \"Training compute (FLOPs)\": 1645179901254.456, \"Domain\": \"Record\"}, {\"Publication date\": \"1991-01-20\", \"Training compute (FLOPs)\": 1861265162174.527, \"Domain\": \"Record\"}, {\"Publication date\": \"1991-05-07\", \"Training compute (FLOPs)\": 2105732024369.7317, \"Domain\": \"Record\"}, {\"Publication date\": \"1991-08-21\", \"Training compute (FLOPs)\": 2382308253854.8657, \"Domain\": \"Record\"}, {\"Publication date\": \"1991-12-05\", \"Training compute (FLOPs)\": 2695211238040.0137, \"Domain\": \"Record\"}, {\"Publication date\": \"1992-03-21\", \"Training compute (FLOPs)\": 3049212294799.751, \"Domain\": \"Record\"}, {\"Publication date\": \"1992-07-05\", \"Training compute (FLOPs)\": 3449709428164.9756, \"Domain\": \"Record\"}, {\"Publication date\": \"1992-10-20\", \"Training compute (FLOPs)\": 3902809640069.887, \"Domain\": \"Record\"}, {\"Publication date\": \"1993-02-03\", \"Training compute (FLOPs)\": 4415422053307.502, \"Domain\": \"Record\"}, {\"Publication date\": \"1993-05-21\", \"Training compute (FLOPs)\": 4995363265651.133, \"Domain\": \"Record\"}, {\"Publication date\": \"1993-09-04\", \"Training compute (FLOPs)\": 5651476541664.796, \"Domain\": \"Record\"}, {\"Publication date\": \"1993-12-20\", \"Training compute (FLOPs)\": 6393766659698.773, \"Domain\": \"Record\"}, {\"Publication date\": \"1994-04-05\", \"Training compute (FLOPs)\": 7233552470273.869, \"Domain\": \"Record\"}, {\"Publication date\": \"1994-07-21\", \"Training compute (FLOPs)\": 8183639492201.569, \"Domain\": \"Record\"}, {\"Publication date\": \"1994-11-04\", \"Training compute (FLOPs)\": 9258515178193.0, \"Domain\": \"Record\"}, {\"Publication date\": \"1995-02-18\", \"Training compute (FLOPs)\": 10474569827577.389, \"Domain\": \"Record\"}, {\"Publication date\": \"1995-06-05\", \"Training compute (FLOPs)\": 11850346514669.002, \"Domain\": \"Record\"}, {\"Publication date\": \"1995-09-19\", \"Training compute (FLOPs)\": 13406823843775.336, \"Domain\": \"Record\"}, {\"Publication date\": \"1996-01-04\", \"Training compute (FLOPs)\": 15167735842628.857, \"Domain\": \"Record\"}, {\"Publication date\": \"1996-04-19\", \"Training compute (FLOPs)\": 17159933871918.701, \"Domain\": \"Record\"}, {\"Publication date\": \"1996-08-04\", \"Training compute (FLOPs)\": 19413796069759.402, \"Domain\": \"Record\"}, {\"Publication date\": \"1996-11-18\", \"Training compute (FLOPs)\": 21963690574273.133, \"Domain\": \"Record\"}, {\"Publication date\": \"1997-03-05\", \"Training compute (FLOPs)\": 24848499588089.34, \"Domain\": \"Record\"}, {\"Publication date\": \"1997-06-19\", \"Training compute (FLOPs)\": 28112212275601.734, \"Domain\": \"Record\"}, {\"Publication date\": \"1997-10-04\", \"Training compute (FLOPs)\": 31804595534085.81, \"Domain\": \"Record\"}, {\"Publication date\": \"1998-01-18\", \"Training compute (FLOPs)\": 35981952866982.92, \"Domain\": \"Record\"}, {\"Publication date\": \"1998-05-04\", \"Training compute (FLOPs)\": 40707982930754.09, \"Domain\": \"Record\"}, {\"Publication date\": \"1998-08-19\", \"Training compute (FLOPs)\": 46054750847341.92, \"Domain\": \"Record\"}, {\"Publication date\": \"1998-12-03\", \"Training compute (FLOPs)\": 52103787093085.31, \"Domain\": \"Record\"}, {\"Publication date\": \"1999-03-20\", \"Training compute (FLOPs)\": 58947330720318.71, \"Domain\": \"Record\"}, {\"Publication date\": \"1999-07-04\", \"Training compute (FLOPs)\": 66689735869767.27, \"Domain\": \"Record\"}, {\"Publication date\": \"1999-10-19\", \"Training compute (FLOPs)\": 75449063019981.12, \"Domain\": \"Record\"}, {\"Publication date\": \"2000-02-02\", \"Training compute (FLOPs)\": 85358879239072.16, \"Domain\": \"Record\"}, {\"Publication date\": \"2000-05-19\", \"Training compute (FLOPs)\": 96570294889161.44, \"Domain\": \"Record\"}, {\"Publication date\": \"2000-09-02\", \"Training compute (FLOPs)\": 109254267840841.11, \"Domain\": \"Record\"}, {\"Publication date\": \"2000-12-17\", \"Training compute (FLOPs)\": 123604210333463.4, \"Domain\": \"Record\"}, {\"Publication date\": \"2001-04-03\", \"Training compute (FLOPs)\": 139838938231755.61, \"Domain\": \"Record\"}, {\"Publication date\": \"2001-07-18\", \"Training compute (FLOPs)\": 158206007651590.88, \"Domain\": \"Record\"}, {\"Publication date\": \"2001-11-02\", \"Training compute (FLOPs)\": 178985489832506.06, \"Domain\": \"Record\"}, {\"Publication date\": \"2002-02-16\", \"Training compute (FLOPs)\": 202494241818655.84, \"Domain\": \"Record\"}, {\"Publication date\": \"2002-06-03\", \"Training compute (FLOPs)\": 229090738070910.53, \"Domain\": \"Record\"}, {\"Publication date\": \"2002-09-17\", \"Training compute (FLOPs)\": 259180536683484.62, \"Domain\": \"Record\"}, {\"Publication date\": \"2003-01-02\", \"Training compute (FLOPs)\": 293222463558289.1, \"Domain\": \"Record\"}, {\"Publication date\": \"2003-04-18\", \"Training compute (FLOPs)\": 331735608836386.56, \"Domain\": \"Record\"}, {\"Publication date\": \"2003-08-03\", \"Training compute (FLOPs)\": 375307242271068.9, \"Domain\": \"Record\"}, {\"Publication date\": \"2003-11-17\", \"Training compute (FLOPs)\": 424601768243273.1, \"Domain\": \"Record\"}, {\"Publication date\": \"2004-03-02\", \"Training compute (FLOPs)\": 480370856965536.2, \"Domain\": \"Record\"}, {\"Publication date\": \"2004-06-17\", \"Training compute (FLOPs)\": 543464906367780.4, \"Domain\": \"Record\"}, {\"Publication date\": \"2004-10-01\", \"Training compute (FLOPs)\": 614846009433675.6, \"Domain\": \"Record\"}, {\"Publication date\": \"2005-01-16\", \"Training compute (FLOPs)\": 695602624726772.5, \"Domain\": \"Record\"}, {\"Publication date\": \"2005-05-02\", \"Training compute (FLOPs)\": 786966173811075.9, \"Domain\": \"Record\"}, {\"Publication date\": \"2005-08-17\", \"Training compute (FLOPs)\": 890329818645786.2, \"Domain\": \"Record\"}, {\"Publication date\": \"2005-12-01\", \"Training compute (FLOPs)\": 1007269705291959.8, \"Domain\": \"Record\"}, {\"Publication date\": \"2006-03-18\", \"Training compute (FLOPs)\": 1139568997861078.0, \"Domain\": \"Record\"}, {\"Publication date\": \"2006-07-02\", \"Training compute (FLOPs)\": 1289245069183346.2, \"Domain\": \"Record\"}, {\"Publication date\": \"2006-10-17\", \"Training compute (FLOPs)\": 1458580262831150.2, \"Domain\": \"Record\"}, {\"Publication date\": \"2007-01-31\", \"Training compute (FLOPs)\": 1650156695551358.8, \"Domain\": \"Record\"}, {\"Publication date\": \"2007-05-17\", \"Training compute (FLOPs)\": 1866895630815350.2, \"Domain\": \"Record\"}, {\"Publication date\": \"2007-09-01\", \"Training compute (FLOPs)\": 2112102023859463.5, \"Domain\": \"Record\"}, {\"Publication date\": \"2007-12-16\", \"Training compute (FLOPs)\": 2389514917466639.0, \"Domain\": \"Record\"}, {\"Publication date\": \"2008-04-01\", \"Training compute (FLOPs)\": 2703364456972604.5, \"Domain\": \"Record\"}, {\"Publication date\": \"2008-07-16\", \"Training compute (FLOPs)\": 3058436393846447.5, \"Domain\": \"Record\"}, {\"Publication date\": \"2008-10-31\", \"Training compute (FLOPs)\": 3460145061491149.5, \"Domain\": \"Record\"}, {\"Publication date\": \"2009-02-14\", \"Training compute (FLOPs)\": 3914615935991559.0, \"Domain\": \"Record\"}, {\"Publication date\": \"2009-06-01\", \"Training compute (FLOPs)\": 4428779040757059.0, \"Domain\": \"Record\"}, {\"Publication date\": \"2009-09-15\", \"Training compute (FLOPs)\": 5010474619365395.0, \"Domain\": \"Record\"}, {\"Publication date\": \"2009-12-31\", \"Training compute (FLOPs)\": 5668572687928308.0, \"Domain\": \"Record\"}, {\"Publication date\": \"2009-12-31\", \"Training compute (FLOPs)\": 265248046739076.88, \"Domain\": \"All\"}, {\"Publication date\": \"2010-01-05\", \"Training compute (FLOPs)\": 270276172304507.9, \"Domain\": \"All\"}, {\"Publication date\": \"2010-01-10\", \"Training compute (FLOPs)\": 275399612602743.06, \"Domain\": \"All\"}, {\"Publication date\": \"2010-01-15\", \"Training compute (FLOPs)\": 280620174449601.38, \"Domain\": \"All\"}, {\"Publication date\": \"2010-01-20\", \"Training compute (FLOPs)\": 285939698912353.6, \"Domain\": \"All\"}, {\"Publication date\": \"2010-01-25\", \"Training compute (FLOPs)\": 291360061957240.25, \"Domain\": \"All\"}, {\"Publication date\": \"2010-01-30\", \"Training compute (FLOPs)\": 296883175112203.06, \"Domain\": \"All\"}, {\"Publication date\": \"2010-02-04\", \"Training compute (FLOPs)\": 302510986140392.9, \"Domain\": \"All\"}, {\"Publication date\": \"2010-02-09\", \"Training compute (FLOPs)\": 308245479728323.2, \"Domain\": \"All\"}, {\"Publication date\": \"2010-02-14\", \"Training compute (FLOPs)\": 314088678183900.0, \"Domain\": \"All\"}, {\"Publication date\": \"2010-02-19\", \"Training compute (FLOPs)\": 320042642150517.06, \"Domain\": \"All\"}, {\"Publication date\": \"2010-02-24\", \"Training compute (FLOPs)\": 326109471334779.06, \"Domain\": \"All\"}, {\"Publication date\": \"2010-03-01\", \"Training compute (FLOPs)\": 332291305244985.3, \"Domain\": \"All\"}, {\"Publication date\": \"2010-03-06\", \"Training compute (FLOPs)\": 338590323947318.56, \"Domain\": \"All\"}, {\"Publication date\": \"2010-03-11\", \"Training compute (FLOPs)\": 345008748832919.4, \"Domain\": \"All\"}, {\"Publication date\": \"2010-03-16\", \"Training compute (FLOPs)\": 351548843403382.44, \"Domain\": \"All\"}, {\"Publication date\": \"2010-03-21\", \"Training compute (FLOPs)\": 358212914067597.7, \"Domain\": \"All\"}, {\"Publication date\": \"2010-03-26\", \"Training compute (FLOPs)\": 365003310955086.3, \"Domain\": \"All\"}, {\"Publication date\": \"2010-03-31\", \"Training compute (FLOPs)\": 371922428746312.1, \"Domain\": \"All\"}, {\"Publication date\": \"2010-04-05\", \"Training compute (FLOPs)\": 378972707514909.94, \"Domain\": \"All\"}, {\"Publication date\": \"2010-04-10\", \"Training compute (FLOPs)\": 386156633589297.0, \"Domain\": \"All\"}, {\"Publication date\": \"2010-04-15\", \"Training compute (FLOPs)\": 393476740430728.75, \"Domain\": \"All\"}, {\"Publication date\": \"2010-04-20\", \"Training compute (FLOPs)\": 400935609524337.6, \"Domain\": \"All\"}, {\"Publication date\": \"2010-04-26\", \"Training compute (FLOPs)\": 408535871291106.06, \"Domain\": \"All\"}, {\"Publication date\": \"2010-05-01\", \"Training compute (FLOPs)\": 416280206014669.6, \"Domain\": \"All\"}, {\"Publication date\": \"2010-05-06\", \"Training compute (FLOPs)\": 424171344788272.9, \"Domain\": \"All\"}, {\"Publication date\": \"2010-05-11\", \"Training compute (FLOPs)\": 432212070475317.06, \"Domain\": \"All\"}, {\"Publication date\": \"2010-05-16\", \"Training compute (FLOPs)\": 440405218692474.75, \"Domain\": \"All\"}, {\"Publication date\": \"2010-05-21\", \"Training compute (FLOPs)\": 448753678808792.06, \"Domain\": \"All\"}, {\"Publication date\": \"2010-05-26\", \"Training compute (FLOPs)\": 457260394966514.9, \"Domain\": \"All\"}, {\"Publication date\": \"2010-05-31\", \"Training compute (FLOPs)\": 465928367116567.7, \"Domain\": \"All\"}, {\"Publication date\": \"2010-06-05\", \"Training compute (FLOPs)\": 474760652078360.1, \"Domain\": \"All\"}, {\"Publication date\": \"2010-06-10\", \"Training compute (FLOPs)\": 483760364616827.1, \"Domain\": \"All\"}, {\"Publication date\": \"2010-06-15\", \"Training compute (FLOPs)\": 492930678542889.2, \"Domain\": \"All\"}, {\"Publication date\": \"2010-06-20\", \"Training compute (FLOPs)\": 502274827829707.06, \"Domain\": \"All\"}, {\"Publication date\": \"2010-06-25\", \"Training compute (FLOPs)\": 511796107754626.56, \"Domain\": \"All\"}, {\"Publication date\": \"2010-06-30\", \"Training compute (FLOPs)\": 521497876062917.1, \"Domain\": \"All\"}, {\"Publication date\": \"2010-07-05\", \"Training compute (FLOPs)\": 531383554148717.9, \"Domain\": \"All\"}, {\"Publication date\": \"2010-07-10\", \"Training compute (FLOPs)\": 541456628264296.44, \"Domain\": \"All\"}, {\"Publication date\": \"2010-07-15\", \"Training compute (FLOPs)\": 551720650746716.2, \"Domain\": \"All\"}, {\"Publication date\": \"2010-07-20\", \"Training compute (FLOPs)\": 562179241273962.2, \"Domain\": \"All\"}, {\"Publication date\": \"2010-07-25\", \"Training compute (FLOPs)\": 572836088139209.2, \"Domain\": \"All\"}, {\"Publication date\": \"2010-07-30\", \"Training compute (FLOPs)\": 583694949551468.8, \"Domain\": \"All\"}, {\"Publication date\": \"2010-08-04\", \"Training compute (FLOPs)\": 594759654963379.9, \"Domain\": \"All\"}, {\"Publication date\": \"2010-08-09\", \"Training compute (FLOPs)\": 606034106418059.6, \"Domain\": \"All\"}, {\"Publication date\": \"2010-08-15\", \"Training compute (FLOPs)\": 617522279926948.1, \"Domain\": \"All\"}, {\"Publication date\": \"2010-08-20\", \"Training compute (FLOPs)\": 629228226873951.8, \"Domain\": \"All\"}, {\"Publication date\": \"2010-08-25\", \"Training compute (FLOPs)\": 641156075440346.8, \"Domain\": \"All\"}, {\"Publication date\": \"2010-08-30\", \"Training compute (FLOPs)\": 653310032063161.4, \"Domain\": \"All\"}, {\"Publication date\": \"2010-09-04\", \"Training compute (FLOPs)\": 665694382917272.8, \"Domain\": \"All\"}, {\"Publication date\": \"2010-09-09\", \"Training compute (FLOPs)\": 678313495429731.0, \"Domain\": \"All\"}, {\"Publication date\": \"2010-09-14\", \"Training compute (FLOPs)\": 691171819815818.4, \"Domain\": \"All\"}, {\"Publication date\": \"2010-09-19\", \"Training compute (FLOPs)\": 704273890651197.9, \"Domain\": \"All\"}, {\"Publication date\": \"2010-09-24\", \"Training compute (FLOPs)\": 717624328469626.0, \"Domain\": \"All\"}, {\"Publication date\": \"2010-09-29\", \"Training compute (FLOPs)\": 731227841395407.9, \"Domain\": \"All\"}, {\"Publication date\": \"2010-10-04\", \"Training compute (FLOPs)\": 745089226799282.1, \"Domain\": \"All\"}, {\"Publication date\": \"2010-10-09\", \"Training compute (FLOPs)\": 759213372993210.8, \"Domain\": \"All\"}, {\"Publication date\": \"2010-10-14\", \"Training compute (FLOPs)\": 773605260952727.4, \"Domain\": \"All\"}, {\"Publication date\": \"2010-10-19\", \"Training compute (FLOPs)\": 788269966076738.1, \"Domain\": \"All\"}, {\"Publication date\": \"2010-10-24\", \"Training compute (FLOPs)\": 803212659972579.8, \"Domain\": \"All\"}, {\"Publication date\": \"2010-10-29\", \"Training compute (FLOPs)\": 818438612282160.8, \"Domain\": \"All\"}, {\"Publication date\": \"2010-11-03\", \"Training compute (FLOPs)\": 833953192542953.2, \"Domain\": \"All\"}, {\"Publication date\": \"2010-11-08\", \"Training compute (FLOPs)\": 849761872076502.9, \"Domain\": \"All\"}, {\"Publication date\": \"2010-11-13\", \"Training compute (FLOPs)\": 865870225922214.5, \"Domain\": \"All\"}, {\"Publication date\": \"2010-11-18\", \"Training compute (FLOPs)\": 882283934798976.8, \"Domain\": \"All\"}, {\"Publication date\": \"2010-11-23\", \"Training compute (FLOPs)\": 899008787113896.0, \"Domain\": \"All\"}, {\"Publication date\": \"2010-11-28\", \"Training compute (FLOPs)\": 916050680999081.6, \"Domain\": \"All\"}, {\"Publication date\": \"2010-12-04\", \"Training compute (FLOPs)\": 933415626396372.1, \"Domain\": \"All\"}, {\"Publication date\": \"2010-12-09\", \"Training compute (FLOPs)\": 951109747171079.4, \"Domain\": \"All\"}, {\"Publication date\": \"2010-12-14\", \"Training compute (FLOPs)\": 969139283275395.8, \"Domain\": \"All\"}, {\"Publication date\": \"2010-12-19\", \"Training compute (FLOPs)\": 987510592946979.4, \"Domain\": \"All\"}, {\"Publication date\": \"2010-12-24\", \"Training compute (FLOPs)\": 1006230154955347.5, \"Domain\": \"All\"}, {\"Publication date\": \"2010-12-29\", \"Training compute (FLOPs)\": 1025304570880512.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-01-03\", \"Training compute (FLOPs)\": 1044740567445150.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-01-08\", \"Training compute (FLOPs)\": 1064544998884698.6, \"Domain\": \"All\"}, {\"Publication date\": \"2011-01-13\", \"Training compute (FLOPs)\": 1084724849368986.2, \"Domain\": \"All\"}, {\"Publication date\": \"2011-01-18\", \"Training compute (FLOPs)\": 1105287235458621.6, \"Domain\": \"All\"}, {\"Publication date\": \"2011-01-23\", \"Training compute (FLOPs)\": 1126239408619093.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-01-28\", \"Training compute (FLOPs)\": 1147588757775754.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-02-02\", \"Training compute (FLOPs)\": 1169342811924359.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-02-07\", \"Training compute (FLOPs)\": 1191509242779074.2, \"Domain\": \"All\"}, {\"Publication date\": \"2011-02-12\", \"Training compute (FLOPs)\": 1214095867481427.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-02-17\", \"Training compute (FLOPs)\": 1237110651360957.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-02-22\", \"Training compute (FLOPs)\": 1260561710736690.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-02-27\", \"Training compute (FLOPs)\": 1284457315784419.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-03-04\", \"Training compute (FLOPs)\": 1308805893450630.2, \"Domain\": \"All\"}, {\"Publication date\": \"2011-03-09\", \"Training compute (FLOPs)\": 1333616030429770.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-03-14\", \"Training compute (FLOPs)\": 1358896476184264.2, \"Domain\": \"All\"}, {\"Publication date\": \"2011-03-19\", \"Training compute (FLOPs)\": 1384656146036921.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-03-25\", \"Training compute (FLOPs)\": 1410904124307872.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-03-30\", \"Training compute (FLOPs)\": 1437649667526831.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-04-04\", \"Training compute (FLOPs)\": 1464902207690224.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-04-09\", \"Training compute (FLOPs)\": 1492671355594975.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-04-14\", \"Training compute (FLOPs)\": 1520966904218700.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-04-19\", \"Training compute (FLOPs)\": 1549798832179322.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-04-24\", \"Training compute (FLOPs)\": 1579177307250933.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-04-29\", \"Training compute (FLOPs)\": 1609112689956117.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-05-04\", \"Training compute (FLOPs)\": 1639615537209831.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-05-09\", \"Training compute (FLOPs)\": 1670696606048888.2, \"Domain\": \"All\"}, {\"Publication date\": \"2011-05-14\", \"Training compute (FLOPs)\": 1702366857422101.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-05-19\", \"Training compute (FLOPs)\": 1734637460062832.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-05-24\", \"Training compute (FLOPs)\": 1767519794417120.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-05-29\", \"Training compute (FLOPs)\": 1801025456664114.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-06-03\", \"Training compute (FLOPs)\": 1835166262801862.2, \"Domain\": \"All\"}, {\"Publication date\": \"2011-06-08\", \"Training compute (FLOPs)\": 1869954252821964.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-06-13\", \"Training compute (FLOPs)\": 1905401694944129.2, \"Domain\": \"All\"}, {\"Publication date\": \"2011-06-18\", \"Training compute (FLOPs)\": 1941521089948192.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-06-23\", \"Training compute (FLOPs)\": 1978325175588812.2, \"Domain\": \"All\"}, {\"Publication date\": \"2011-06-28\", \"Training compute (FLOPs)\": 2015826931075438.2, \"Domain\": \"All\"}, {\"Publication date\": \"2011-07-03\", \"Training compute (FLOPs)\": 2054039581657538.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-07-08\", \"Training compute (FLOPs)\": 2092976603284384.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-07-14\", \"Training compute (FLOPs)\": 2132651727366167.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-07-19\", \"Training compute (FLOPs)\": 2173078945607988.8, \"Domain\": \"All\"}, {\"Publication date\": \"2011-07-24\", \"Training compute (FLOPs)\": 2214272514932346.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-07-29\", \"Training compute (FLOPs)\": 2256246962541058.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-08-03\", \"Training compute (FLOPs)\": 2299017091006654.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-08-08\", \"Training compute (FLOPs)\": 2342597983503760.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-08-13\", \"Training compute (FLOPs)\": 2387005009135769.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-08-18\", \"Training compute (FLOPs)\": 2432253828340285.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-08-23\", \"Training compute (FLOPs)\": 2478360398421555.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-08-28\", \"Training compute (FLOPs)\": 2525340979172871.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-09-02\", \"Training compute (FLOPs)\": 2573212138621221.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-09-07\", \"Training compute (FLOPs)\": 2621990758854403.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-09-12\", \"Training compute (FLOPs)\": 2671694041985037.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-09-17\", \"Training compute (FLOPs)\": 2722339516211557.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-09-22\", \"Training compute (FLOPs)\": 2773945042011004.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-09-27\", \"Training compute (FLOPs)\": 2826528818420701.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-10-02\", \"Training compute (FLOPs)\": 2880109389464488.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-10-07\", \"Training compute (FLOPs)\": 2934705650701615.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-10-12\", \"Training compute (FLOPs)\": 2990336855872461.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-10-17\", \"Training compute (FLOPs)\": 3047022623700390.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-10-22\", \"Training compute (FLOPs)\": 3104782944804219.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-10-27\", \"Training compute (FLOPs)\": 3163638188760986.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-11-02\", \"Training compute (FLOPs)\": 3223609111270100.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-11-07\", \"Training compute (FLOPs)\": 3284716861485800.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-11-12\", \"Training compute (FLOPs)\": 3346982989468851.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-11-17\", \"Training compute (FLOPs)\": 3410429453800276.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-11-22\", \"Training compute (FLOPs)\": 3475078629308007.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-11-27\", \"Training compute (FLOPs)\": 3540953314956852.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-12-02\", \"Training compute (FLOPs)\": 3608076741906896.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-12-07\", \"Training compute (FLOPs)\": 3676472581691780.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-12-12\", \"Training compute (FLOPs)\": 3746164954566271.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-12-17\", \"Training compute (FLOPs)\": 3817178438028058.0, \"Domain\": \"All\"}, {\"Publication date\": \"2011-12-22\", \"Training compute (FLOPs)\": 3889538075461849.5, \"Domain\": \"All\"}, {\"Publication date\": \"2011-12-27\", \"Training compute (FLOPs)\": 3963269384986573.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-01-01\", \"Training compute (FLOPs)\": 4038398368446430.5, \"Domain\": \"All\"}, {\"Publication date\": \"2012-01-06\", \"Training compute (FLOPs)\": 4114951520597462.5, \"Domain\": \"All\"}, {\"Publication date\": \"2012-01-11\", \"Training compute (FLOPs)\": 4192955838425968.5, \"Domain\": \"All\"}, {\"Publication date\": \"2012-01-16\", \"Training compute (FLOPs)\": 4272438830685859.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-01-21\", \"Training compute (FLOPs)\": 4353428527591091.5, \"Domain\": \"All\"}, {\"Publication date\": \"2012-01-26\", \"Training compute (FLOPs)\": 4435953490718870.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-01-31\", \"Training compute (FLOPs)\": 4520042823054982.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-02-05\", \"Training compute (FLOPs)\": 4605726179270319.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-02-10\", \"Training compute (FLOPs)\": 4693033776193529.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-02-15\", \"Training compute (FLOPs)\": 4781996403438519.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-02-21\", \"Training compute (FLOPs)\": 4872645434281641.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-02-26\", \"Training compute (FLOPs)\": 4965012836715764.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-03-02\", \"Training compute (FLOPs)\": 5059131184744700.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-03-07\", \"Training compute (FLOPs)\": 5155033669839746.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-03-12\", \"Training compute (FLOPs)\": 5252754112665388.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-03-17\", \"Training compute (FLOPs)\": 5352326974995681.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-03-22\", \"Training compute (FLOPs)\": 5453787371889745.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-03-27\", \"Training compute (FLOPs)\": 5557171084047832.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-04-01\", \"Training compute (FLOPs)\": 5662514570428569.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-04-06\", \"Training compute (FLOPs)\": 5769854981135553.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-04-11\", \"Training compute (FLOPs)\": 5879230170495631.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-04-16\", \"Training compute (FLOPs)\": 5990678710407927.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-04-21\", \"Training compute (FLOPs)\": 6104239903971461.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-04-26\", \"Training compute (FLOPs)\": 6219953799308364.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-05-01\", \"Training compute (FLOPs)\": 6337861203711862.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-05-06\", \"Training compute (FLOPs)\": 6458003698024333.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-05-11\", \"Training compute (FLOPs)\": 6580423651327992.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-05-16\", \"Training compute (FLOPs)\": 6705164235846441.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-05-21\", \"Training compute (FLOPs)\": 6832269442196323.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-05-26\", \"Training compute (FLOPs)\": 6961784094886982.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-05-31\", \"Training compute (FLOPs)\": 7093753868157156.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-06-05\", \"Training compute (FLOPs)\": 7228225302038978.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-06-11\", \"Training compute (FLOPs)\": 7365245818791677.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-06-16\", \"Training compute (FLOPs)\": 7504863739648904.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-06-21\", \"Training compute (FLOPs)\": 7647128301813711.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-06-26\", \"Training compute (FLOPs)\": 7792089675852800.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-07-01\", \"Training compute (FLOPs)\": 7939798983373629.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-07-06\", \"Training compute (FLOPs)\": 8090308315085903.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-07-11\", \"Training compute (FLOPs)\": 8243670749122292.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-07-16\", \"Training compute (FLOPs)\": 8399940369789593.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-07-21\", \"Training compute (FLOPs)\": 8559172286624802.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-07-26\", \"Training compute (FLOPs)\": 8721422653865553.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-07-31\", \"Training compute (FLOPs)\": 8886748690209325.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-08-05\", \"Training compute (FLOPs)\": 9055208698990304.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-08-10\", \"Training compute (FLOPs)\": 9226862088786968.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-08-15\", \"Training compute (FLOPs)\": 9401769394336230.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-08-20\", \"Training compute (FLOPs)\": 9579992297880514.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-08-25\", \"Training compute (FLOPs)\": 9761593650960372.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-08-30\", \"Training compute (FLOPs)\": 9946637496519866.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-09-04\", \"Training compute (FLOPs)\": 1.0135189091531324e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2012-09-09\", \"Training compute (FLOPs)\": 1.032731492998799e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2012-09-14\", \"Training compute (FLOPs)\": 1.0523082766396644e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2012-09-19\", \"Training compute (FLOPs)\": 1.0722561639607404e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2012-09-24\", \"Training compute (FLOPs)\": 1.0925821897191962e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2012-09-30\", \"Training compute (FLOPs)\": 1.1132935220287072e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2012-09-30\", \"Training compute (FLOPs)\": 8444961524789957.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-10-17\", \"Training compute (FLOPs)\": 9121988012550624.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-11-03\", \"Training compute (FLOPs)\": 9853291226591694.0, \"Domain\": \"All\"}, {\"Publication date\": \"2012-11-20\", \"Training compute (FLOPs)\": 1.0643222492974824e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2012-12-07\", \"Training compute (FLOPs)\": 1.1496481980483264e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2012-12-24\", \"Training compute (FLOPs)\": 1.2418146667054346e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-01-10\", \"Training compute (FLOPs)\": 1.3413700548242898e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-01-27\", \"Training compute (FLOPs)\": 1.4489067267604726e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-02-13\", \"Training compute (FLOPs)\": 1.5650645362936364e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-03-02\", \"Training compute (FLOPs)\": 1.6905346338204588e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-03-19\", \"Training compute (FLOPs)\": 1.8260635787598624e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-04-05\", \"Training compute (FLOPs)\": 1.9724577816850456e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-04-22\", \"Training compute (FLOPs)\": 2.1305883025015556e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-05-10\", \"Training compute (FLOPs)\": 2.3013960333683304e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-05-27\", \"Training compute (FLOPs)\": 2.4858972970915476e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-06-13\", \"Training compute (FLOPs)\": 2.6851898943453268e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-06-30\", \"Training compute (FLOPs)\": 2.900459635693766e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-07-17\", \"Training compute (FLOPs)\": 3.1329873972673584e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-08-03\", \"Training compute (FLOPs)\": 3.3841567421535224e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-08-20\", \"Training compute (FLOPs)\": 3.655462152657304e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-09-06\", \"Training compute (FLOPs)\": 3.948517922667716e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-09-23\", \"Training compute (FLOPs)\": 4.265067762853076e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-10-10\", \"Training compute (FLOPs)\": 4.606995175911063e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-10-27\", \"Training compute (FLOPs)\": 4.9763346635951096e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-11-13\", \"Training compute (FLOPs)\": 5.375283832177512e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-12-01\", \"Training compute (FLOPs)\": 5.806216468511383e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2013-12-18\", \"Training compute (FLOPs)\": 6.271696664165937e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2014-01-04\", \"Training compute (FLOPs)\": 6.774494072108677e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2014-01-21\", \"Training compute (FLOPs)\": 7.317600386392243e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2014-02-07\", \"Training compute (FLOPs)\": 7.90424714302841e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2014-02-24\", \"Training compute (FLOPs)\": 8.537924947909578e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2014-03-13\", \"Training compute (FLOPs)\": 9.2224042463977e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2014-03-30\", \"Training compute (FLOPs)\": 9.961757757638592e+16, \"Domain\": \"All\"}, {\"Publication date\": \"2014-04-16\", \"Training compute (FLOPs)\": 1.0760384707776696e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-05-03\", \"Training compute (FLOPs)\": 1.162303700575041e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-05-20\", \"Training compute (FLOPs)\": 1.2554847517617861e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-06-06\", \"Training compute (FLOPs)\": 1.3561360607615032e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-06-23\", \"Training compute (FLOPs)\": 1.464856512760717e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-07-11\", \"Training compute (FLOPs)\": 1.5822930051587562e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-07-28\", \"Training compute (FLOPs)\": 1.7091442966354864e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-08-14\", \"Training compute (FLOPs)\": 1.846165164857391e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-08-31\", \"Training compute (FLOPs)\": 1.9941708974732755e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-09-17\", \"Training compute (FLOPs)\": 2.1540421431560022e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-10-04\", \"Training compute (FLOPs)\": 2.326730151548751e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-10-21\", \"Training compute (FLOPs)\": 2.5132624332834723e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-11-07\", \"Training compute (FLOPs)\": 2.7147488738115786e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-11-24\", \"Training compute (FLOPs)\": 2.9323883372707795e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-12-11\", \"Training compute (FLOPs)\": 3.167475799884431e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2014-12-28\", \"Training compute (FLOPs)\": 3.4214100551877434e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-01-14\", \"Training compute (FLOPs)\": 3.695702036986962e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-02-01\", \"Training compute (FLOPs)\": 3.9919838095558266e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-02-18\", \"Training compute (FLOPs)\": 4.312018278602385e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-03-07\", \"Training compute (FLOPs)\": 4.657709680718582e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-03-24\", \"Training compute (FLOPs)\": 5.031114913754761e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-04-10\", \"Training compute (FLOPs)\": 5.434455775590649e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-04-27\", \"Training compute (FLOPs)\": 5.870132184043004e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-05-14\", \"Training compute (FLOPs)\": 6.340736456612777e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-05-31\", \"Training compute (FLOPs)\": 6.849068735014981e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-06-17\", \"Training compute (FLOPs)\": 7.398153646338255e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-07-04\", \"Training compute (FLOPs)\": 7.991258299851611e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-07-21\", \"Training compute (FLOPs)\": 8.631911726590744e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-08-07\", \"Training compute (FLOPs)\": 9.323925877485197e+17, \"Domain\": \"All\"}, {\"Publication date\": \"2015-08-24\", \"Training compute (FLOPs)\": 1.0071418304827145e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-09-11\", \"Training compute (FLOPs)\": 1.08788366621127e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-09-28\", \"Training compute (FLOPs)\": 1.1750985167993254e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-10-15\", \"Training compute (FLOPs)\": 1.269305319191922e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-11-01\", \"Training compute (FLOPs)\": 1.3710646131361078e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-11-18\", \"Training compute (FLOPs)\": 1.4809818764415276e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-12-05\", \"Training compute (FLOPs)\": 1.5997111276407327e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-12-22\", \"Training compute (FLOPs)\": 1.7279588174613445e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-01-08\", \"Training compute (FLOPs)\": 1.8664880322734886e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-01-25\", \"Training compute (FLOPs)\": 2.0161230345441318e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-02-11\", \"Training compute (FLOPs)\": 2.1777541672527237e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-02-28\", \"Training compute (FLOPs)\": 2.3523431515522043e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-03-16\", \"Training compute (FLOPs)\": 2.540928809074529e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-04-03\", \"Training compute (FLOPs)\": 2.744633243038828e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-04-20\", \"Training compute (FLOPs)\": 2.9646685148717527e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-05-07\", \"Training compute (FLOPs)\": 3.202343856085524e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-05-24\", \"Training compute (FLOPs)\": 3.459073458360587e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-06-10\", \"Training compute (FLOPs)\": 3.736384888078697e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-06-27\", \"Training compute (FLOPs)\": 4.0359281755492547e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-07-14\", \"Training compute (FLOPs)\": 4.359485632800594e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-07-31\", \"Training compute (FLOPs)\": 4.708982458541488e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-08-17\", \"Training compute (FLOPs)\": 5.086498193265014e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-09-03\", \"Training compute (FLOPs)\": 5.494279092760923e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-09-20\", \"Training compute (FLOPs)\": 5.93475149349709e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-10-07\", \"Training compute (FLOPs)\": 6.410536249587056e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-10-24\", \"Training compute (FLOPs)\": 6.92446432717274e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-11-11\", \"Training compute (FLOPs)\": 7.479593648873989e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-11-28\", \"Training compute (FLOPs)\": 8.079227288779779e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-12-15\", \"Training compute (FLOPs)\": 8.726933126033362e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2017-01-01\", \"Training compute (FLOPs)\": 9.426565074135996e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2017-01-18\", \"Training compute (FLOPs)\": 1.0182286012005917e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-02-04\", \"Training compute (FLOPs)\": 1.0998592553563535e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-02-21\", \"Training compute (FLOPs)\": 1.1880341803100223e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-03-10\", \"Training compute (FLOPs)\": 1.2832780255393733e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-03-27\", \"Training compute (FLOPs)\": 1.3861575012955372e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-04-13\", \"Training compute (FLOPs)\": 1.497284750579492e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-04-30\", \"Training compute (FLOPs)\": 1.6173209914654626e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-05-17\", \"Training compute (FLOPs)\": 1.7469804513954114e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-06-04\", \"Training compute (FLOPs)\": 1.8870346169104904e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-06-21\", \"Training compute (FLOPs)\": 2.0383168240844874e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-07-08\", \"Training compute (FLOPs)\": 2.2017272169326326e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-07-25\", \"Training compute (FLOPs)\": 2.3782381033670128e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-08-11\", \"Training compute (FLOPs)\": 2.5688997405402153e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-08-28\", \"Training compute (FLOPs)\": 2.7748465839499596e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-09-14\", \"Training compute (FLOPs)\": 2.99730403757975e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-10-01\", \"Training compute (FLOPs)\": 3.237595745161769e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-10-18\", \"Training compute (FLOPs)\": 3.497151466013946e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-11-04\", \"Training compute (FLOPs)\": 3.777515582209416e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-11-21\", \"Training compute (FLOPs)\": 4.080356287818893e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-12-08\", \"Training compute (FLOPs)\": 4.407475514856803e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-12-26\", \"Training compute (FLOPs)\": 4.760819654904719e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-01-12\", \"Training compute (FLOPs)\": 5.1424911403650695e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-01-29\", \"Training compute (FLOPs)\": 5.554760954121275e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-02-15\", \"Training compute (FLOPs)\": 6.0000821421550305e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-03-04\", \"Training compute (FLOPs)\": 6.481104409344076e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-03-21\", \"Training compute (FLOPs)\": 7.0006898854962274e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-04-07\", \"Training compute (FLOPs)\": 7.561930155350154e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-04-24\", \"Training compute (FLOPs)\": 8.168164653724085e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-05-11\", \"Training compute (FLOPs)\": 8.823000535536963e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-05-28\", \"Training compute (FLOPs)\": 9.530334138701614e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-06-14\", \"Training compute (FLOPs)\": 1.0294374167797095e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-07-01\", \"Training compute (FLOPs)\": 1.1119666736136475e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-07-18\", \"Training compute (FLOPs)\": 1.20111224157552e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-08-05\", \"Training compute (FLOPs)\": 1.2974045455644883e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-08-22\", \"Training compute (FLOPs)\": 1.4014165342644722e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-09-08\", \"Training compute (FLOPs)\": 1.5137670892398697e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-09-25\", \"Training compute (FLOPs)\": 1.63512470735421e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-10-12\", \"Training compute (FLOPs)\": 1.7662114783739566e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-10-29\", \"Training compute (FLOPs)\": 1.90780738148165e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-11-15\", \"Training compute (FLOPs)\": 2.0607549262415443e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-12-02\", \"Training compute (FLOPs)\": 2.225964165591336e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-12-19\", \"Training compute (FLOPs)\": 2.4044181107617835e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-01-05\", \"Training compute (FLOPs)\": 2.5971785802811218e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-01-22\", \"Training compute (FLOPs)\": 2.8053925179211396e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-02-08\", \"Training compute (FLOPs)\": 3.030298817087713e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-02-26\", \"Training compute (FLOPs)\": 3.2732356924039365e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-03-15\", \"Training compute (FLOPs)\": 3.53564864217711e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-04-01\", \"Training compute (FLOPs)\": 3.81909904927988e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-04-18\", \"Training compute (FLOPs)\": 4.125273471526093e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-05-05\", \"Training compute (FLOPs)\": 4.455993676846416e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-05-22\", \"Training compute (FLOPs)\": 4.8132274830234196e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-06-08\", \"Training compute (FLOPs)\": 5.19910046633622e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-06-25\", \"Training compute (FLOPs)\": 5.6159086090264864e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-07-12\", \"Training compute (FLOPs)\": 6.066131960547162e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-07-29\", \"Training compute (FLOPs)\": 6.552449394141879e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-08-15\", \"Training compute (FLOPs)\": 7.077754546394273e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-09-01\", \"Training compute (FLOPs)\": 7.645173034634382e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-09-18\", \"Training compute (FLOPs)\": 8.258081054732178e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-10-06\", \"Training compute (FLOPs)\": 8.920125469684032e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-10-23\", \"Training compute (FLOPs)\": 9.635245508940203e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-11-09\", \"Training compute (FLOPs)\": 1.040769620708495e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2019-11-26\", \"Training compute (FLOPs)\": 1.1242073721781562e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2019-12-13\", \"Training compute (FLOPs)\": 1.214334268134542e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2019-12-30\", \"Training compute (FLOPs)\": 1.3116865724743538e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-01-16\", \"Training compute (FLOPs)\": 1.4168435409902253e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-02-02\", \"Training compute (FLOPs)\": 1.5304308679954667e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-02-19\", \"Training compute (FLOPs)\": 1.6531244092600316e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-03-07\", \"Training compute (FLOPs)\": 1.785654203427517e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-03-24\", \"Training compute (FLOPs)\": 1.928808815814182e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-04-10\", \"Training compute (FLOPs)\": 2.083440030450174e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-04-28\", \"Training compute (FLOPs)\": 2.2504679182736108e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-05-15\", \"Training compute (FLOPs)\": 2.4308863116566106e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-06-01\", \"Training compute (FLOPs)\": 2.6257687177984543e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-06-18\", \"Training compute (FLOPs)\": 2.8362747061874464e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-07-05\", \"Training compute (FLOPs)\": 3.063656808168339e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-07-22\", \"Training compute (FLOPs)\": 3.3092679696223704e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-08-08\", \"Training compute (FLOPs)\": 3.5745696011283863e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-08-25\", \"Training compute (FLOPs)\": 3.861140273491504e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-09-11\", \"Training compute (FLOPs)\": 4.170685110417741e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-09-28\", \"Training compute (FLOPs)\": 4.505045934155427e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-10-15\", \"Training compute (FLOPs)\": 4.866212224498897e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-11-01\", \"Training compute (FLOPs)\": 5.256332956414542e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-11-18\", \"Training compute (FLOPs)\": 5.677729386645228e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-12-06\", \"Training compute (FLOPs)\": 6.132908865416335e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-12-23\", \"Training compute (FLOPs)\": 6.624579755388572e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-01-09\", \"Training compute (FLOPs)\": 7.155667546791232e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-01-26\", \"Training compute (FLOPs)\": 7.729332264216638e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-02-12\", \"Training compute (FLOPs)\": 8.348987269182212e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-03-01\", \"Training compute (FLOPs)\": 9.018319569941693e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-03-18\", \"Training compute (FLOPs)\": 9.741311759547055e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-04-04\", \"Training compute (FLOPs)\": 1.0522265712669045e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-04-21\", \"Training compute (FLOPs)\": 1.1365828182127187e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-05-08\", \"Training compute (FLOPs)\": 1.227701844771317e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-05-25\", \"Training compute (FLOPs)\": 1.3261258181124496e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-06-11\", \"Training compute (FLOPs)\": 1.4324403705624377e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-06-29\", \"Training compute (FLOPs)\": 1.5472780841697354e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-07-16\", \"Training compute (FLOPs)\": 1.6713222546303635e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-08-02\", \"Training compute (FLOPs)\": 1.8053109569645376e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-08-19\", \"Training compute (FLOPs)\": 1.950041437127498e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-09-05\", \"Training compute (FLOPs)\": 2.1063748557334557e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-09-22\", \"Training compute (FLOPs)\": 2.275241411998799e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-10-09\", \"Training compute (FLOPs)\": 2.4576458785498153e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-10-26\", \"Training compute (FLOPs)\": 2.6546735799111244e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-11-12\", \"Training compute (FLOPs)\": 2.8674968502933974e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-11-29\", \"Training compute (FLOPs)\": 3.0973820090907816e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-12-16\", \"Training compute (FLOPs)\": 3.3456968956243584e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2022-01-02\", \"Training compute (FLOPs)\": 3.6139190079107335e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2022-01-20\", \"Training compute (FLOPs)\": 3.903644293904641e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2012-09-30\", \"Training compute (FLOPs)\": 7.51257170245646e+17, \"Domain\": \"Record\"}, {\"Publication date\": \"2012-10-17\", \"Training compute (FLOPs)\": 8.172981330666134e+17, \"Domain\": \"Record\"}, {\"Publication date\": \"2012-11-03\", \"Training compute (FLOPs)\": 8.89144576276534e+17, \"Domain\": \"Record\"}, {\"Publication date\": \"2012-11-20\", \"Training compute (FLOPs)\": 9.673068437775834e+17, \"Domain\": \"Record\"}, {\"Publication date\": \"2012-12-07\", \"Training compute (FLOPs)\": 1.0523401424066522e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2012-12-24\", \"Training compute (FLOPs)\": 1.1448484857149281e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-01-10\", \"Training compute (FLOPs)\": 1.2454889844326428e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-01-27\", \"Training compute (FLOPs)\": 1.354976514099675e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-02-13\", \"Training compute (FLOPs)\": 1.4740887930006031e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-03-02\", \"Training compute (FLOPs)\": 1.6036719065155172e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-03-19\", \"Training compute (FLOPs)\": 1.7446463170730186e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-04-05\", \"Training compute (FLOPs)\": 1.89801340243594e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-04-22\", \"Training compute (FLOPs)\": 2.064862568747039e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-05-10\", \"Training compute (FLOPs)\": 2.2463789888677837e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-05-27\", \"Training compute (FLOPs)\": 2.4438520209498153e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-06-13\", \"Training compute (FLOPs)\": 2.658684367107041e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-06-30\", \"Training compute (FLOPs)\": 2.892402037160548e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-07-17\", \"Training compute (FLOPs)\": 3.146665188255732e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-08-03\", \"Training compute (FLOPs)\": 3.423279917449324e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-08-20\", \"Training compute (FLOPs)\": 3.7242110908318193e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-09-06\", \"Training compute (FLOPs)\": 4.0515963004801526e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-09-23\", \"Training compute (FLOPs)\": 4.4077610483668905e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-10-10\", \"Training compute (FLOPs)\": 4.795235264983984e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-10-27\", \"Training compute (FLOPs)\": 5.21677128006234e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-11-13\", \"Training compute (FLOPs)\": 5.675363373128924e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-12-01\", \"Training compute (FLOPs)\": 6.174269042671796e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2013-12-18\", \"Training compute (FLOPs)\": 6.717032144885973e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-01-04\", \"Training compute (FLOPs)\": 7.307508066736556e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-01-21\", \"Training compute (FLOPs)\": 7.949891111651738e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-02-07\", \"Training compute (FLOPs)\": 8.648744292846739e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-02-24\", \"Training compute (FLOPs)\": 9.409031745531394e+18, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-03-13\", \"Training compute (FLOPs)\": 1.0236153988462742e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-03-30\", \"Training compute (FLOPs)\": 1.1135986285228982e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-04-16\", \"Training compute (FLOPs)\": 1.21149203777943e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-05-03\", \"Training compute (FLOPs)\": 1.3179909888598634e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-05-20\", \"Training compute (FLOPs)\": 1.4338519714085528e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-06-06\", \"Training compute (FLOPs)\": 1.5598979760058872e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-06-23\", \"Training compute (FLOPs)\": 1.6970243400695843e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-07-11\", \"Training compute (FLOPs)\": 1.846205107701076e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-07-28\", \"Training compute (FLOPs)\": 2.0084999485445915e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-08-14\", \"Training compute (FLOPs)\": 2.18506168489506e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-08-31\", \"Training compute (FLOPs)\": 2.3771444805145022e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-09-17\", \"Training compute (FLOPs)\": 2.586112749271783e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-10-04\", \"Training compute (FLOPs)\": 2.8134508469104607e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-10-21\", \"Training compute (FLOPs)\": 3.0607736148369863e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-11-07\", \"Training compute (FLOPs)\": 3.329837850769341e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-11-24\", \"Training compute (FLOPs)\": 3.6225547876671332e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-12-11\", \"Training compute (FLOPs)\": 3.941003669778724e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2014-12-28\", \"Training compute (FLOPs)\": 4.287446522028707e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-01-14\", \"Training compute (FLOPs)\": 4.664344217738237e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-02-01\", \"Training compute (FLOPs)\": 5.074373958886872e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-02-18\", \"Training compute (FLOPs)\": 5.52044829299134e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-03-07\", \"Training compute (FLOPs)\": 6.005735801586907e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-03-24\", \"Training compute (FLOPs)\": 6.5336836075894596e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-04-10\", \"Training compute (FLOPs)\": 7.1080418610643526e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-04-27\", \"Training compute (FLOPs)\": 7.732890377466231e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-05-14\", \"Training compute (FLOPs)\": 8.41266761772855e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-05-31\", \"Training compute (FLOPs)\": 9.152202215657387e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-06-17\", \"Training compute (FLOPs)\": 9.956747277158641e+19, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-07-04\", \"Training compute (FLOPs)\": 1.0832017694256418e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-07-21\", \"Training compute (FLOPs)\": 1.1784230739451052e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-08-07\", \"Training compute (FLOPs)\": 1.282015022872947e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-08-24\", \"Training compute (FLOPs)\": 1.3947134566646538e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-09-11\", \"Training compute (FLOPs)\": 1.5173189014885466e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-09-28\", \"Training compute (FLOPs)\": 1.6507022555839185e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-10-15\", \"Training compute (FLOPs)\": 1.7958109754832034e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-11-01\", \"Training compute (FLOPs)\": 1.9536758060092388e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-11-18\", \"Training compute (FLOPs)\": 2.12541810195744e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-12-05\", \"Training compute (FLOPs)\": 2.3122577933569724e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2015-12-22\", \"Training compute (FLOPs)\": 2.515522050934296e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-01-08\", \"Training compute (FLOPs)\": 2.736654713372838e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-01-25\", \"Training compute (FLOPs)\": 2.97722654328374e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-02-11\", \"Training compute (FLOPs)\": 3.238946384686668e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-02-28\", \"Training compute (FLOPs)\": 3.5236733014290576e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-03-16\", \"Training compute (FLOPs)\": 3.833429782577613e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-04-03\", \"Training compute (FLOPs)\": 4.170416108656413e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-04-20\", \"Training compute (FLOPs)\": 4.537025980855766e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-05-07\", \"Training compute (FLOPs)\": 4.935863524081771e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-05-24\", \"Training compute (FLOPs)\": 5.369761784737599e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-06-10\", \"Training compute (FLOPs)\": 5.8418028545861065e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-06-27\", \"Training compute (FLOPs)\": 6.355339763659458e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-07-14\", \"Training compute (FLOPs)\": 6.91402029764802e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-07-31\", \"Training compute (FLOPs)\": 7.52181290914383e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-08-17\", \"Training compute (FLOPs)\": 8.183034906537532e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-09-03\", \"Training compute (FLOPs)\": 8.902383120990664e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-09-20\", \"Training compute (FLOPs)\": 9.684967269244286e+20, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-10-07\", \"Training compute (FLOPs)\": 1.053634624928292e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-10-24\", \"Training compute (FLOPs)\": 1.1462567626540692e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-11-11\", \"Training compute (FLOPs)\": 1.2470210591478619e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-11-28\", \"Training compute (FLOPs)\": 1.3566432693093765e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2016-12-15\", \"Training compute (FLOPs)\": 1.4759020680983942e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-01-01\", \"Training compute (FLOPs)\": 1.6056445816670543e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-01-18\", \"Training compute (FLOPs)\": 1.7467924047046558e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-02-04\", \"Training compute (FLOPs)\": 1.900348146760786e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-02-21\", \"Training compute (FLOPs)\": 2.0674025540632644e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-03-10\", \"Training compute (FLOPs)\": 2.249142257340483e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-03-27\", \"Training compute (FLOPs)\": 2.446858200795202e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-04-13\", \"Training compute (FLOPs)\": 2.661954811999418e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-04-30\", \"Training compute (FLOPs)\": 2.895959977911259e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-05-17\", \"Training compute (FLOPs)\": 3.1505358978519033e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-06-04\", \"Training compute (FLOPs)\": 3.4274908905451883e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-06-21\", \"Training compute (FLOPs)\": 3.728792239045888e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-07-08\", \"Training compute (FLOPs)\": 4.0565801649022755e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-07-25\", \"Training compute (FLOPs)\": 4.4131830306773124e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-08-11\", \"Training compute (FLOPs)\": 4.80113387891778e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-08-28\", \"Training compute (FLOPs)\": 5.223188425011765e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-09-14\", \"Training compute (FLOPs)\": 5.68234463176094e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-10-01\", \"Training compute (FLOPs)\": 6.181864004640427e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-10-18\", \"Training compute (FLOPs)\": 6.725294759185722e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-11-04\", \"Training compute (FLOPs)\": 7.316497024857548e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-11-21\", \"Training compute (FLOPs)\": 7.959670264475745e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-12-08\", \"Training compute (FLOPs)\": 8.659383104245244e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2017-12-26\", \"Training compute (FLOPs)\": 9.420605785757864e+21, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-01-12\", \"Training compute (FLOPs)\": 1.0248745470905459e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-01-29\", \"Training compute (FLOPs)\": 1.1149684650450025e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-02-15\", \"Training compute (FLOPs)\": 1.2129822928830934e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-03-04\", \"Training compute (FLOPs)\": 1.3196122482131901e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-03-21\", \"Training compute (FLOPs)\": 1.4356157512337704e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-04-07\", \"Training compute (FLOPs)\": 1.5618168048908342e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-04-24\", \"Training compute (FLOPs)\": 1.6991118479565244e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-05-11\", \"Training compute (FLOPs)\": 1.8484761226961096e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-05-28\", \"Training compute (FLOPs)\": 2.010970602255624e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-06-14\", \"Training compute (FLOPs)\": 2.1877495270155104e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-07-01\", \"Training compute (FLOPs)\": 2.3800686034783534e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-07-18\", \"Training compute (FLOPs)\": 2.5892939238866374e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-08-05\", \"Training compute (FLOPs)\": 2.816911669883802e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-08-22\", \"Training compute (FLOPs)\": 3.064538669303877e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-09-08\", \"Training compute (FLOPs)\": 3.3339338808683716e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-09-25\", \"Training compute (FLOPs)\": 3.6270108885677353e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-10-12\", \"Training compute (FLOPs)\": 3.9458514943283755e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-10-29\", \"Training compute (FLOPs)\": 4.292720505555629e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-11-15\", \"Training compute (FLOPs)\": 4.670081822730358e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-12-02\", \"Training compute (FLOPs)\": 5.0806159410602575e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2018-12-19\", \"Training compute (FLOPs)\": 5.527238990731474e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-01-05\", \"Training compute (FLOPs)\": 6.013123450989847e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-01-22\", \"Training compute (FLOPs)\": 6.541720685056659e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-02-08\", \"Training compute (FLOPs)\": 7.116785455996981e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-02-26\", \"Training compute (FLOPs)\": 7.742402597911829e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-03-15\", \"Training compute (FLOPs)\": 8.423016031453457e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-04-01\", \"Training compute (FLOPs)\": 9.16346033014458e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-04-18\", \"Training compute (FLOPs)\": 9.968995061694533e+22, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-05-05\", \"Training compute (FLOPs)\": 1.084534214802784e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-05-22\", \"Training compute (FLOPs)\": 1.179872650950531e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-06-08\", \"Training compute (FLOPs)\": 1.2835920282250042e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-06-25\", \"Training compute (FLOPs)\": 1.3964290922434145e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-07-12\", \"Training compute (FLOPs)\": 1.519185353898022e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-07-29\", \"Training compute (FLOPs)\": 1.6527327827226042e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-08-15\", \"Training compute (FLOPs)\": 1.798020000708275e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-09-01\", \"Training compute (FLOPs)\": 1.9560790205991964e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-09-18\", \"Training compute (FLOPs)\": 2.1280325765681572e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-10-06\", \"Training compute (FLOPs)\": 2.3151020992741437e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-10-23\", \"Training compute (FLOPs)\": 2.518616392004225e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-11-09\", \"Training compute (FLOPs)\": 2.7400210695075788e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-11-26\", \"Training compute (FLOPs)\": 2.9808888265778363e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-12-13\", \"Training compute (FLOPs)\": 3.242930609287201e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2019-12-30\", \"Training compute (FLOPs)\": 3.5280077683230154e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-01-16\", \"Training compute (FLOPs)\": 3.838145280583489e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-02-02\", \"Training compute (FLOPs)\": 4.175546133193036e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-02-19\", \"Training compute (FLOPs)\": 4.542606971826055e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-03-07\", \"Training compute (FLOPs)\": 4.9419351247018054e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-03-24\", \"Training compute (FLOPs)\": 5.376367123159657e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-04-10\", \"Training compute (FLOPs)\": 5.848988850252606e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-04-28\", \"Training compute (FLOPs)\": 6.36315746053485e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-05-15\", \"Training compute (FLOPs)\": 6.922525226856407e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-06-01\", \"Training compute (FLOPs)\": 7.531065483429995e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-06-18\", \"Training compute (FLOPs)\": 8.193100849319356e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-07-05\", \"Training compute (FLOPs)\": 8.913333933267681e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-07-22\", \"Training compute (FLOPs)\": 9.696880737493619e+23, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-08-08\", \"Training compute (FLOPs)\": 1.0549306997915439e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-08-25\", \"Training compute (FLOPs)\": 1.1476667719132179e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-09-11\", \"Training compute (FLOPs)\": 1.2485550184613787e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-09-28\", \"Training compute (FLOPs)\": 1.3583120747900948e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-10-15\", \"Training compute (FLOPs)\": 1.4777175737071e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-11-01\", \"Training compute (FLOPs)\": 1.6076196834109858e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-11-18\", \"Training compute (FLOPs)\": 1.748941132236471e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-12-06\", \"Training compute (FLOPs)\": 1.9026857630522132e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2020-12-23\", \"Training compute (FLOPs)\": 2.0699456638041382e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-01-09\", \"Training compute (FLOPs)\": 2.2519089249021593e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-01-26\", \"Training compute (FLOPs)\": 2.4498680785341786e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-02-12\", \"Training compute (FLOPs)\": 2.665229279862677e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-03-01\", \"Training compute (FLOPs)\": 2.8995222952872753e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-03-18\", \"Training compute (FLOPs)\": 3.1544113688039485e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-04-04\", \"Training compute (FLOPs)\": 3.431707043547528e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-04-21\", \"Training compute (FLOPs)\": 3.733379022517822e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-05-08\", \"Training compute (FLOPs)\": 4.061570159962368e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-05-25\", \"Training compute (FLOPs)\": 4.4186116825625714e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-06-11\", \"Training compute (FLOPs)\": 4.8070397487405334e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-06-29\", \"Training compute (FLOPs)\": 5.229613463706452e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-07-16\", \"Training compute (FLOPs)\": 5.689334478020085e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-08-02\", \"Training compute (FLOPs)\": 6.189468309163925e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-08-19\", \"Training compute (FLOPs)\": 6.733567537304679e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-09-05\", \"Training compute (FLOPs)\": 7.325497040271295e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-09-22\", \"Training compute (FLOPs)\": 7.969461446642204e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-10-09\", \"Training compute (FLOPs)\": 8.670035002470946e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-10-26\", \"Training compute (FLOPs)\": 9.432194063199258e+24, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-11-12\", \"Training compute (FLOPs)\": 1.0261352442117715e+25, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-11-29\", \"Training compute (FLOPs)\": 1.1163399866015969e+25, \"Domain\": \"Record\"}, {\"Publication date\": \"2021-12-16\", \"Training compute (FLOPs)\": 1.2144743811454764e+25, \"Domain\": \"Record\"}, {\"Publication date\": \"2022-01-02\", \"Training compute (FLOPs)\": 1.321235501873205e+25, \"Domain\": \"Record\"}, {\"Publication date\": \"2022-01-20\", \"Training compute (FLOPs)\": 1.4373817006887447e+25, \"Domain\": \"Record\"}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Run this first!\n",
        "!pip install altair_saver --quiet\n",
        "!npm install vega-lite vega-cli canvas --silent"
      ],
      "metadata": {
        "id": "dYM7QIMPfsmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c27db8-288c-462a-a26d-5c0143730381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 89 kB 4.8 MB/s \n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 958 kB 15.7 MB/s \n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 844 kB 52.8 MB/s \n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 356 kB 15.1 MB/s \n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 138 kB 45.9 MB/s \n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 55 kB 3.8 MB/s \n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 3.6 MB 47.7 MB/s \n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 58 kB 4.8 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[K\u001b[?25h[canvas] Success: \"/content/node_modules/canvas/build/Release/canvas.node\" is installed via remote\n",
            "\u001b[K\u001b[?25h+ canvas@2.9.0\n",
            "+ vega-lite@5.2.0\n",
            "+ vega-cli@5.21.0\n",
            "added 146 packages from 88 contributors and audited 146 packages in 16.613s\n",
            "\n",
            "11 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.numeric import False_\n",
        "#@title Automatically generate charts and results\n",
        "\n",
        "settings_list = [\n",
        "  # {\n",
        "  #     'chart_name' : 'fig1.png',\n",
        "  #     'table_name' : 'fig1table.csv',\n",
        "  #     'visualization_params' : {\n",
        "  #         'x_axis' : 'Publication date',\n",
        "  #         'y_axis' : 'Training compute (FLOPs)',\n",
        "  #         'date_start' : '1950-01-01',\n",
        "  #         'date_end' : '2022-02-01',\n",
        "\n",
        "  #         # Era options\n",
        "  #         'start_dl_era' : '2009-12-31',\n",
        "  #         'start_large_scale_era' : '2015-09-01',\n",
        "  #         'split_dl_era' : True,\n",
        "  #         'split_large_scale_era' : True,\n",
        "\n",
        "  #         # Data options\n",
        "  #         'citation_threshold' : 0,\n",
        "  #         'separate_categories' : False,\n",
        "  #         'other_domain_threshold' : 10,\n",
        "  #         'outliers_action' : 'remove',\n",
        "  #         'large_scale_action' : 'label',\n",
        "  #         'big_alphago_action' : 'ignore',\n",
        "  #         'record_setters_action' : 'ignore',\n",
        "  #         'low_outliers_z_value_threshold' : -2,\n",
        "  #         'high_outliers_z_value_threshold' : 0.76,\n",
        "  #         'outlier_window_size' : 2,\n",
        "\n",
        "  #         # Bootstrapping options\n",
        "  #         'bootstrap_sample_size' : 1000,\n",
        "  #         'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #         # Visualization options \n",
        "  #         'label_points' : False,\n",
        "  #         'plot_regressions' : True,\n",
        "  #         'label_eras' : True,\n",
        "  #     }\n",
        "  # },\n",
        "  # {\n",
        "  #     'chart_name' : 'fig2.png',\n",
        "  #     'table_name' : 'fig2table.csv',\n",
        "  #     'visualization_params' : {\n",
        "  #         'x_axis' : 'Publication date',\n",
        "  #         'y_axis' : 'Training compute (FLOPs)',\n",
        "  #         'date_start' : '1950-01-01',\n",
        "  #         'date_end' : '2022-02-01',\n",
        "\n",
        "  #         # Era options\n",
        "  #         'start_dl_era' : '2009-12-31',\n",
        "  #         'start_large_scale_era' : '2015-09-01',\n",
        "  #         'split_dl_era' : True,\n",
        "  #         'split_large_scale_era' : True,\n",
        "\n",
        "  #         # Data options\n",
        "  #         'citation_threshold' : 0,\n",
        "  #         'separate_categories' : True,\n",
        "  #         'other_domain_threshold' : 10,\n",
        "  #         'outliers_action' : 'remove',\n",
        "  #         'large_scale_action' : 'label',\n",
        "  #         'big_alphago_action' : 'ignore',\n",
        "  #         'record_setters_action' : 'ignore',\n",
        "  #         'low_outliers_z_value_threshold' : -2,\n",
        "  #         'high_outliers_z_value_threshold' : 0.76,\n",
        "  #         'outlier_window_size' : 2,\n",
        "\n",
        "  #         # Bootstrapping options\n",
        "  #         'bootstrap_sample_size' : 1000,\n",
        "  #         'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #         # Visualization options \n",
        "  #         'label_points' : False,\n",
        "  #         'plot_regressions' : False,\n",
        "  #         'label_eras' : True,\n",
        "  #     }\n",
        "  # },\n",
        "  # {\n",
        "  #     'chart_name' : 'fig3.png',\n",
        "  #     'table_name' : 'fig3table.csv',\n",
        "  #     'visualization_params' : {\n",
        "  #         'x_axis' : 'Publication date',\n",
        "  #         'y_axis' : 'Training compute (FLOPs)',\n",
        "  #         'date_start' : '1950-01-01',\n",
        "  #         'date_end' : '2022-02-01',\n",
        "\n",
        "  #         # Era options\n",
        "  #         'start_dl_era' : '2009-12-31',\n",
        "  #         'start_large_scale_era' : '2024-09-01',\n",
        "  #         'split_dl_era' : True,\n",
        "  #         'split_large_scale_era' : True,\n",
        "\n",
        "  #         # Data options\n",
        "  #         'citation_threshold' : 0,\n",
        "  #         'separate_categories' : False,\n",
        "  #         'other_domain_threshold' : 10,\n",
        "  #         'outliers_action' : 'remove',\n",
        "  #         'large_scale_action' : 'ignore',\n",
        "  #         'big_alphago_action' : 'ignore',\n",
        "  #         'record_setters_action' : 'ignore',\n",
        "  #         'low_outliers_z_value_threshold' : -2,\n",
        "  #         'high_outliers_z_value_threshold' : 0.76,\n",
        "  #         'outlier_window_size' : 2,\n",
        "\n",
        "  #         # Bootstrapping options\n",
        "  #         'bootstrap_sample_size' : 1000,\n",
        "  #         'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #         # Visualization options \n",
        "  #         'label_points' : False,\n",
        "  #         'plot_regressions' : True,\n",
        "  #         'label_eras' : True,\n",
        "  #     }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'fig4.png',\n",
        "  #   'table_name' : 'fig4table.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2009-12-31',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : True,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'fig5.png',\n",
        "  #   'table_name' : 'fig5table.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2015-09-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'isolate',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : True,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : False,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'fig6.png',\n",
        "  #   'table_name' : 'fig6table.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1990-01-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'label',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : False,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'fig7.png',\n",
        "  #   'table_name' : 'fig7table.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'ignore',\n",
        "  #       'large_scale_action' : 'ignore',\n",
        "  #       'big_alphago_action' : 'remove',\n",
        "  #       'record_setters_action' : 'isolate',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : True,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'fig8.png',\n",
        "  #   'table_name' : 'fig8table.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2009-12-31',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : False,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : True,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'ignore',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  #   {\n",
        "  #   'chart_name' : 'fig10.png',\n",
        "  #   'table_name' : 'fig10table.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2012-09-01',\n",
        "  #       'date_end' : '2017-12-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2012-09-01',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : False,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table1run1.png',\n",
        "  #   'table_name' : 'table1run1.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'ignore',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table1run2.png',\n",
        "  #   'table_name' : 'table1run2.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2009-12-31',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table1run3.png',\n",
        "  #   'table_name' : 'table1run3.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table4run1.png',\n",
        "  #   'table_name' : 'table4run1.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-01-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-09-01',\n",
        "  #       'start_large_scale_era' : '2022-03-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table4run2.png',\n",
        "  #   'table_name' : 'table4run2.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-01-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-09-01',\n",
        "  #       'start_large_scale_era' : '2009-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table4run3.png',\n",
        "  #   'table_name' : 'table4run3.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-01-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2012-09-30',\n",
        "  #       'start_large_scale_era' : '2022-03-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table4run4.png',\n",
        "  #   'table_name' : 'table4run4.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-01-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2012-09-30',\n",
        "  #       'start_large_scale_era' : '2012-09-30',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table5run1.png',\n",
        "  #   'table_name' : 'table5run1.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2009-12-31',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table5run2.png',\n",
        "  #   'table_name' : 'table5run2.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2015-09-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2024-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table6run1.png',\n",
        "  #   'table_name' : 'table6run1.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'ignore',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'label',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table6run2.png',\n",
        "  #   'table_name' : 'table6run2.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '1950-01-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-31',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'ignore',\n",
        "  #       'big_alphago_action' : 'remove',\n",
        "  #       'record_setters_action' : 'label',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table7run1.png',\n",
        "  #   'table_name' : 'table7run1.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2009-12-31',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-30',\n",
        "  #       'start_large_scale_era' : '2015-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : False,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : True,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'ignore',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table7run2.png', \n",
        "  #   'table_name' : 'table7run2.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2009-12-31',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-30',\n",
        "  #       'start_large_scale_era' : '2009-12-30',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : True,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table7run2.png', # struggling to replicate\n",
        "  #   'table_name' : 'table7run2.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2009-12-31',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2009-12-30',\n",
        "  #       'start_large_scale_era' : '2009-12-30',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : True,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table8run1.png', \n",
        "  #   'table_name' : 'table8run1.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2012-09-01',\n",
        "  #       'date_end' : '2017-12-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2012-09-01',\n",
        "  #       'start_large_scale_era' : '2012-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'ignore',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table8run2.png', \n",
        "  #   'table_name' : 'table8run2.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2012-09-01',\n",
        "  #       'date_end' : '2017-12-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2012-09-01',\n",
        "  #       'start_large_scale_era' : '2012-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table8run3.png', \n",
        "  #   'table_name' : 'table8run3.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2017-12-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2017-11-30',\n",
        "  #       'start_large_scale_era' : '2017-11-30',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'ignore',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table8run4.png', \n",
        "  #   'table_name' : 'table8run4.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2017-12-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2017-11-30',\n",
        "  #       'start_large_scale_era' : '2017-11-30',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table8run5.png', \n",
        "  #   'table_name' : 'table8run5.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2012-09-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2012-09-01',\n",
        "  #       'start_large_scale_era' : '2012-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'ignore',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },\n",
        "  # {\n",
        "  #   'chart_name' : 'table8run6.png', \n",
        "  #   'table_name' : 'table8run6.csv',\n",
        "  #   'visualization_params' : {\n",
        "  #       'x_axis' : 'Publication date',\n",
        "  #       'y_axis' : 'Training compute (FLOPs)',\n",
        "  #       'date_start' : '2012-09-01',\n",
        "  #       'date_end' : '2022-02-01',\n",
        "\n",
        "  #       # Era options\n",
        "  #       'start_dl_era' : '2012-09-01',\n",
        "  #       'start_large_scale_era' : '2012-09-01',\n",
        "  #       'split_dl_era' : True,\n",
        "  #       'split_large_scale_era' : True,\n",
        "\n",
        "  #       # Data options\n",
        "  #       'citation_threshold' : 0,\n",
        "  #       'separate_categories' : False,\n",
        "  #       'other_domain_threshold' : 10,\n",
        "  #       'outliers_action' : 'remove',\n",
        "  #       'large_scale_action' : 'label',\n",
        "  #       'big_alphago_action' : 'ignore',\n",
        "  #       'record_setters_action' : 'ignore',\n",
        "  #       'low_outliers_z_value_threshold' : -2,\n",
        "  #       'high_outliers_z_value_threshold' : 0.76,\n",
        "  #       'outlier_window_size' : 2,\n",
        "\n",
        "  #       # Bootstrapping options\n",
        "  #       'bootstrap_sample_size' : 1000,\n",
        "  #       'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "  #       # Visualization options \n",
        "  #       'label_points' : False,\n",
        "  #       'plot_regressions' : True,\n",
        "  #       'label_eras' : True,\n",
        "  #   }\n",
        "  # },  \n",
        "  {\n",
        "    'chart_name' : 'table8run7.png', \n",
        "    'table_name' : 'table8run7.csv',\n",
        "    'visualization_params' : {\n",
        "        'x_axis' : 'Publication date',\n",
        "        'y_axis' : 'Training compute (FLOPs)',\n",
        "        'date_start' : '2015-09-01',\n",
        "        'date_end' : '2022-02-01',\n",
        "\n",
        "        # Era options\n",
        "        'start_dl_era' : '2012-09-01',\n",
        "        'start_large_scale_era' : '2015-09-01',\n",
        "        'split_dl_era' : True,\n",
        "        'split_large_scale_era' : True,\n",
        "\n",
        "        # Data options\n",
        "        'citation_threshold' : 0,\n",
        "        'separate_categories' : False,\n",
        "        'other_domain_threshold' : 10,\n",
        "        'outliers_action' : 'remove',\n",
        "        'large_scale_action' : 'label',\n",
        "        'big_alphago_action' : 'ignore',\n",
        "        'record_setters_action' : 'ignore',\n",
        "        'low_outliers_z_value_threshold' : -2,\n",
        "        'high_outliers_z_value_threshold' : 0.76,\n",
        "        'outlier_window_size' : 2,\n",
        "\n",
        "        # Bootstrapping options\n",
        "        'bootstrap_sample_size' : 1000,\n",
        "        'adjust_for_estimate_uncertainty' : True,\n",
        "\n",
        "        # Visualization options \n",
        "        'label_points' : False,\n",
        "        'plot_regressions' : True,\n",
        "        'label_eras' : True,\n",
        "    }\n",
        "  },  \n",
        "]\n",
        "\n",
        "#@markdown Enter number from 0 to N-1, where N is the number of settings\n",
        "#@markdown Enter 'all' to show all charts and tables\n",
        "display_index = 'all' #@param\n",
        "\n",
        "!rm output -r\n",
        "!mkdir output\n",
        "\n",
        "if display_index == 'all':\n",
        "\n",
        "  \n",
        "  for settings in settings_list:\n",
        "    chart, df_results = make_visualization(**settings['visualization_params'])\n",
        "    chart.save('output/' + settings['chart_name'])\n",
        "    df_results.to_csv('output/' + settings['table_name'])\n",
        "    display(chart)\n",
        "    display(df_results)\n",
        "  \n",
        "else:\n",
        "  settings = settings_list[display_index]\n",
        "  chart, df_results = make_visualization(**settings['visualization_params'])\n",
        "  chart.save('output/' + settings['chart_name'])\n",
        "  df_results.to_csv('output/' + settings['table_name'])\n",
        "  display(chart)\n",
        "  display(df_results)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "Iwe9DNuQdNtI",
        "outputId": "dc7607c8-2192-4507-9809-9a8403da3882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'output': No such file or directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/altair/utils/core.py:221: UserWarning: I don't know how to infer vegalite type from 'empty'.  Defaulting to nominal.\n",
            "  \"Defaulting to nominal.\".format(typ)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "alt.LayerChart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-e005618590c547a6a04a4716cc91e3e2\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-e005618590c547a6a04a4716cc91e3e2\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-e005618590c547a6a04a4716cc91e3e2\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"usermeta\": {\"embedOptions\": {\"theme\": \"fivethirtyeight\"}}, \"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 20, \"titleFontSize\": 26}, \"background\": \"#FFFFFF\", \"legend\": {\"gradientLength\": 400, \"gradientThickness\": 30, \"labelFontSize\": 18, \"symbolSize\": 130, \"titleFontSize\": 20}}, \"layer\": [{\"data\": {\"name\": \"data-cadef9b80a884685373b23455e12f61b\"}, \"mark\": {\"type\": \"point\", \"filled\": false, \"size\": 120}, \"encoding\": {\"color\": {\"field\": \"Domain\", \"legend\": null, \"scale\": {\"domain\": [\"Large Scale\", \"All\"], \"range\": [\"#fc4f30\", \"#30a2da\"]}, \"type\": \"nominal\"}, \"opacity\": {\"condition\": {\"value\": 1, \"selection\": \"selector003\"}, \"value\": 0.2}, \"shape\": {\"field\": \"Domain\", \"legend\": null, \"scale\": {\"domain\": [\"Large Scale\", \"All\"], \"range\": [\"triangle\", \"circle\"]}, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"System\", \"type\": \"nominal\"}, {\"field\": \"Reference\", \"type\": \"nominal\"}, {\"field\": \"Publication date\", \"type\": \"temporal\"}, {\"field\": \"Parameters\", \"format\": \".1e\", \"type\": \"quantitative\"}, {\"field\": \"Training compute (FLOPs)\", \"format\": \".1e\", \"type\": \"quantitative\"}, {\"field\": \"Inference compute (FLOPs)\", \"format\": \".1e\", \"type\": \"quantitative\"}, {\"field\": \"Domain\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"format\": \"%Y\", \"values\": [\"2015-01-01T00:00:00\", \"2016-01-01T00:00:00\", \"2017-01-01T00:00:00\", \"2018-01-01T00:00:00\", \"2019-01-01T00:00:00\", \"2020-01-01T00:00:00\", \"2021-01-01T00:00:00\", \"2022-01-01T00:00:00\", \"2023-01-01T00:00:00\"]}, \"field\": \"Publication date\", \"scale\": {\"domain\": [\"2015-09-09T00:00:00\", \"2022-01-20T00:00:00\"], \"type\": \"time\"}, \"type\": \"temporal\"}, \"y\": {\"axis\": {\"format\": \"e\", \"grid\": true}, \"field\": \"Training compute (FLOPs)\", \"scale\": {\"domain\": [6.14929e+16, 1.35e+24], \"type\": \"log\"}, \"type\": \"quantitative\"}}, \"height\": 600, \"selection\": {\"selector003\": {\"type\": \"multi\", \"fields\": [\"Domain\"], \"bind\": \"legend\"}, \"selector004\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": {\"text\": [\"Training compute (FLOPs) of milestone Machine Learning systems over time\"], \"subtitle\": [\"n = 79\"]}, \"width\": 1100}, {\"data\": {\"name\": \"data-c3c3497522bfc668b441acdad70ac93c\"}, \"mark\": {\"type\": \"rect\", \"opacity\": 0.1}, \"encoding\": {\"color\": {\"field\": \"Era\", \"legend\": null, \"scale\": {\"domain\": [\"Pre Deep Learning Era\", \"Deep Learning Era\", \"Large Scale Era\"], \"range\": [\"#e5ae38\", \"#30a2da\", \"#fc4f30\"]}, \"type\": \"nominal\"}, \"x\": {\"field\": \"start\", \"title\": \"Publication date\", \"type\": \"temporal\"}, \"x2\": {\"field\": \"stop\"}, \"y\": {\"value\": 0}, \"y2\": {\"value\": 1000}}}, {\"data\": {\"name\": \"data-c3c3497522bfc668b441acdad70ac93c\"}, \"mark\": {\"type\": \"text\", \"align\": \"right\", \"angle\": 270, \"baseline\": \"top\", \"dx\": -20, \"dy\": 20, \"fontWeight\": \"bold\", \"size\": 18}, \"encoding\": {\"color\": {\"field\": \"Era\", \"legend\": null, \"scale\": {\"domain\": [\"Pre Deep Learning Era\", \"Deep Learning Era\", \"Large Scale Era\"], \"range\": [\"#e5ae38\", \"#30a2da\", \"#fc4f30\"]}, \"type\": \"nominal\"}, \"text\": {\"field\": \"Era\", \"type\": \"nominal\"}, \"x\": {\"field\": \"start\", \"title\": \"Publication date\", \"type\": \"temporal\"}, \"y\": {\"value\": 0}}}, {\"data\": {\"name\": \"data-d751713988987e9331980363e24189ce\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"angle\": 270, \"baseline\": \"top\", \"dx\": -20, \"dy\": 20, \"fontWeight\": \"bold\", \"size\": 18}, \"encoding\": {\"color\": {\"field\": \"Era\", \"legend\": null, \"scale\": {\"domain\": [\"Pre Deep Learning Era\", \"Deep Learning Era\", \"Large Scale Era\"], \"range\": [\"#e5ae38\", \"#30a2da\", \"#fc4f30\"]}, \"type\": \"nominal\"}, \"text\": {\"field\": \"Era\", \"type\": \"nominal\"}, \"x\": {\"field\": \"start\", \"title\": \"Publication date\", \"type\": \"temporal\"}, \"y\": {\"value\": 570}}}, {\"data\": {\"name\": \"data-abe686790c3c9f76e73d063ae5551c06\"}, \"mark\": {\"type\": \"line\", \"clip\": true, \"point\": false, \"strokeDash\": [10, 5]}, \"encoding\": {\"color\": {\"field\": \"Domain\", \"legend\": null, \"scale\": {\"domain\": [\"Large Scale\", \"All\"], \"range\": [\"#fc4f30\", \"#30a2da\"]}, \"type\": \"nominal\"}, \"opacity\": {\"condition\": {\"value\": 1, \"selection\": \"selector003\"}, \"value\": 0.2}, \"x\": {\"field\": \"Publication date\", \"type\": \"temporal\"}, \"y\": {\"field\": \"Training compute (FLOPs)\", \"type\": \"quantitative\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-cadef9b80a884685373b23455e12f61b\": [{\"System\": \"DLRM-2021\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook AI \", \"Author(s)\": \"D Mudigere, Y Hao, J Huang, A Tulloch\", \"Publication date\": \"2020-01-01T00:00:00\", \"Year\": \"2020\", \"Reference\": null, \"Link\": null, \"Citations\": 2.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1000000000000.0, \"Training compute (FLOPs)\": 3e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": \"Possibly first theoretical suggestion of NNs\", \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 300000000.0, \"Training compute times parameters\": 3e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"NEO (DL:RM-2022)\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook\", \"Author(s)\": \"D Mudigere, Y Hao, J Huang, A Tulloch\", \"Publication date\": \"2021-01-01T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models\", \"Link\": \"https://arxiv.org/abs/2104.05158\", \"Citations\": 2.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 3000000000000.0, \"Training compute (FLOPs)\": 1.1e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 366666666.6666667, \"Training compute times parameters\": 3.3e+33, \"Era\": \"Large Scale Era\"}, {\"System\": \"Primer\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Research, Brain Team\", \"Author(s)\": \"DavidR.So, WojciechMan \\u0301ke, HanxiaoLiu, ZihangDai, NoamShazeer, QuocV.Le\", \"Publication date\": \"2021-01-01T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Primer: Searching for Efficient Transformers for Language Modeling\", \"Link\": \"http://arxiv.org/abs/2109.08668\", \"Citations\": 13.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1900000000.0, \"Training compute (FLOPs)\": 7.1e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3736842105263.1577, \"Training compute times parameters\": 1.349e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaGo Fan\", \"Domain\": \"Large Scale\", \"Task\": \"Go\", \"Organization(s)\": \"Google DeepMind\", \"Author(s)\": \"TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez\", \"Publication date\": \"2015-09-09T00:00:00\", \"Year\": \"2015\", \"Reference\": \"Continuous control with deep reinforcement learning\", \"Link\": \"https://arxiv.org/abs/1509.02971\", \"Citations\": 6350.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 8210000.0, \"Training compute (FLOPs)\": 3.8e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 46285018270401.945, \"Training compute times parameters\": 3.1198e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"DeepSpeech2\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Baidu Research- Silicon Valley AI Lab\", \"Author(s)\": \"D Amodei, S Ananthanarayanan\", \"Publication date\": \"2015-12-08T00:00:00\", \"Year\": \"2015\", \"Reference\": \"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\", \"Link\": \"https://arxiv.org/abs/1512.02595\", \"Citations\": 2210.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 11.0, \"Parameters\": 38000000.0, \"Training compute (FLOPs)\": 2.6e+19, \"Inference compute (FLOPs)\": 1800000000.0, \"Training dataset\": \"Is this supposed to be in this row? \", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 684210526315.7894, \"Training compute times parameters\": 9.88e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"ResNet-152 (ImageNet)\", \"Domain\": \"All\", \"Task\": \"Image classification\", \"Organization(s)\": \"Microsoft\", \"Author(s)\": \"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\", \"Publication date\": \"2015-12-10T00:00:00\", \"Year\": \"2015\", \"Reference\": \"Deep Residual Learning for Image Recognition\", \"Link\": \"https://arxiv.org/abs/1512.03385\", \"Citations\": 85800.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 152.0, \"Parameters\": 60000000.0, \"Training compute (FLOPs)\": 1.21e+19, \"Inference compute (FLOPs)\": 22600000000.0, \"Training dataset\": \"ILSVRC 2012\", \"Training dataset size (datapoints)\": \"1.20E+06\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": \"138\", \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 201666666666.66666, \"Training compute times parameters\": 7.26e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaGo Lee\", \"Domain\": \"Large Scale\", \"Task\": \"Go\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"D Silver, A Huang, CJ Maddison, A Guez, L Sifre\", \"Publication date\": \"2016-01-27T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Mastering the game of Go with deep neural networks and tree search\", \"Link\": \"https://www.nature.com/articles/nature16961\", \"Citations\": 10800.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.9e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"2.94E+07\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"GNMT\", \"Domain\": \"Large Scale\", \"Task\": \"Translation\", \"Organization(s)\": \"Google\", \"Author(s)\": \"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \\u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean\", \"Publication date\": \"2016-09-26T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\", \"Link\": \"https://research.google/pubs/pub45610/\", \"Citations\": 4500.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 278000000.0, \"Training compute (FLOPs)\": 6.9e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 24820143884892.086, \"Training compute times parameters\": 1.9182e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"Xception\", \"Domain\": \"All\", \"Task\": \"Image classification\", \"Organization(s)\": \"Google\", \"Author(s)\": \"Fran\\u00e7ois Chollet\", \"Publication date\": \"2016-10-07T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Xception: Deep Learning with Depthwise Separable Convolutions\", \"Link\": \"https://arxiv.org/abs/1610.02357\", \"Citations\": 5840.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 22900000.0, \"Training compute (FLOPs)\": 4.4e+19, \"Inference compute (FLOPs)\": 16800000000.0, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1921397379912.6638, \"Training compute times parameters\": 1.0076e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"NASv3 (CIFAR-10)\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Brain\", \"Author(s)\": \"Barret Zoph, Quoc V. Le\", \"Publication date\": \"2016-11-05T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Neural Architecture Search with Reinforcement Learning\", \"Link\": \"https://arxiv.org/abs/1611.01578\", \"Citations\": 2970.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": 39.0, \"Parameters\": 37400000.0, \"Training compute (FLOPs)\": 2.2e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 58823529411764.7, \"Training compute times parameters\": 8.228e+28, \"Era\": \"Large Scale Era\"}, {\"System\": \"Libratus\", \"Domain\": \"All\", \"Task\": \"Poker\", \"Organization(s)\": \"Carnagie Mellon University\", \"Author(s)\": \"N Brown, T Sandholm, S Machine\", \"Publication date\": \"2017-01-01T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Libratus: The Superhuman AI for No-Limit Poker\", \"Link\": \"https://www.cs.cmu.edu/~noamb/papers/17-IJCAI-Libratus.pdf\", \"Citations\": 64.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA improvement\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.15e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 3000000.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaGo Master\", \"Domain\": \"Large Scale\", \"Task\": \"Go\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"D Silver, J Schrittwieser, K Simonyan, I Antonoglou\", \"Publication date\": \"2017-01-01T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Mastering the game of Go without human knowledge\", \"Link\": \"https://www.nature.com/articles/nature24270\", \"Citations\": 5810.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.5e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"MoE\", \"Domain\": \"All\", \"Task\": \"Language modelling / Machine translation\", \"Organization(s)\": \"Google Brain, Jagiellonian University, Cracow\", \"Author(s)\": \"N Shazeer, A Mirhoseini, K Maziarz, A Davis\", \"Publication date\": \"2017-01-23T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\", \"Link\": \"https://arxiv.org/abs/1701.06538\", \"Citations\": 687.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 8700000000.0, \"Training compute (FLOPs)\": 9.39391e+19, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 10797597701.149426, \"Training compute times parameters\": 8.1727017e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"Transformer\", \"Domain\": \"All\", \"Task\": \"Translation\", \"Organization(s)\": \"Google Brain ; Google Research\", \"Author(s)\": \"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\", \"Publication date\": \"2017-06-12T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Attention Is All You Need\", \"Link\": \"https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\", \"Citations\": 25200.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 213000000.0, \"Training compute (FLOPs)\": 7.42452e+18, \"Inference compute (FLOPs)\": 54000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 672.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 34856901408.45071, \"Training compute times parameters\": 1.58142276e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"JFT\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Research, CMU\", \"Author(s)\": \"ChenSun,AbhinavShrivastava,SaurabhSingh,andAbhinavGupta\", \"Publication date\": \"2017-08-04T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.\", \"Link\": \"https://arxiv.org/pdf/1707.02968.pdf\", \"Citations\": 1140.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 4.79e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"JFT-300M\", \"Training dataset size (datapoints)\": \"3.00E+08\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"OpenAI TI7 DOTA 1v1\", \"Domain\": \"All\", \"Task\": \"DOTA\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": null, \"Publication date\": \"2017-08-11T00:00:00\", \"Year\": \"2017\", \"Reference\": null, \"Link\": null, \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 6.05e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaGo Zero\", \"Domain\": \"Large Scale\", \"Task\": \"Go\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"D Silver, J Schrittwieser, K Simonyan, I Antonoglou\", \"Publication date\": \"2017-10-19T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Mastering the game of Go without human knowledge\", \"Link\": \"https://www.nature.com/articles/nature24270\", \"Citations\": 5810.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 46400000.0, \"Training compute (FLOPs)\": 3.41e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 7349137931034483.0, \"Training compute times parameters\": 1.58224e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaZero\", \"Domain\": \"Large Scale\", \"Task\": null, \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"D Silver, T Hubert, J Schrittwieser, I Antonoglou\", \"Publication date\": \"2017-12-05T00:00:00\", \"Year\": \"2017\", \"Reference\": \"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\", \"Link\": \"https://arxiv.org/abs/1712.01815\", \"Citations\": 1080.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 3.67e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"IMPALA\", \"Domain\": \"All\", \"Task\": \"Atari\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu\", \"Publication date\": \"2018-02-05T00:00:00\", \"Year\": \"2018\", \"Reference\": \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\", \"Link\": \"https://arxiv.org/abs/1802.01561\", \"Citations\": 675.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1600000.0, \"Training compute (FLOPs)\": 1.68e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 105000000000000.0, \"Training compute times parameters\": 2.688e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"YOLOv3\", \"Domain\": \"All\", \"Task\": \"Object detection\", \"Organization(s)\": \"University of Washington\", \"Author(s)\": \"Joseph Redmon, Ali Farhadi\", \"Publication date\": \"2018-04-08T00:00:00\", \"Year\": \"2018\", \"Reference\": \"YOLOv3: An Incremental Improvement\", \"Link\": \"https://arxiv.org/abs/1804.02767\", \"Citations\": 7710.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 106000000.0, \"Training compute (FLOPs)\": 5.09e+19, \"Inference compute (FLOPs)\": 71000000000.0, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": \"1.28E+06\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 480188679245.283, \"Training compute times parameters\": 5.3954e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"A Radford, K Narasimhan, T Salimans, I Sutskever\", \"Publication date\": \"2018-06-01T00:00:00\", \"Year\": \"2018\", \"Reference\": \"Improving Language Understanding by Generative Pre-Training\", \"Link\": \"https://openai.com/blog/language-unsupervised/\", \"Citations\": 2260.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 117000000.0, \"Training compute (FLOPs)\": 1.1e+19, \"Inference compute (FLOPs)\": 30000000000.0, \"Training dataset\": \"BooksCorpus\", \"Training dataset size (datapoints)\": \"6.25E+08\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 94017094017.09402, \"Training compute times parameters\": 1.287e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"Population-based DRL\", \"Domain\": \"All\", \"Task\": \"Capture the flag\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel\", \"Publication date\": \"2018-07-03T00:00:00\", \"Year\": \"2018\", \"Reference\": \"Human-level performance in first-person multiplayer games with population-based deep reinforcement learning\", \"Link\": \"https://arxiv.org/abs/1807.01281\", \"Citations\": 434.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 122000000.0, \"Training compute (FLOPs)\": 3.49e+19, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 286065573770.4918, \"Training compute times parameters\": 4.2578e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"BigGAN-deep 512x512\", \"Domain\": \"All\", \"Task\": \"Image generation\", \"Organization(s)\": \"Heriot-Watt University, DeepMind\", \"Author(s)\": \"A Brock, J Donahue, K Simonyan\", \"Publication date\": \"2018-09-28T00:00:00\", \"Year\": \"2018\", \"Reference\": \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\", \"Link\": \"https://arxiv.org/abs/1809.11096\", \"Citations\": 1980.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 113000000.0, \"Training compute (FLOPs)\": 3e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": \"Perplexity... sorta\", \"Milestone\": \">5000 citations\", \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 26548672566371.68, \"Training compute times parameters\": 3.39e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"BERT-Large\", \"Domain\": \"All\", \"Task\": \"Next sentence prediction\", \"Organization(s)\": \"Google AI\", \"Author(s)\": \"J Devlin, MW Chang, K Lee, K Toutanova\", \"Publication date\": \"2018-10-11T00:00:00\", \"Year\": \"2018\", \"Reference\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"Link\": \"https://arxiv.org/abs/1810.04805\", \"Citations\": 23800.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 340000000.0, \"Training compute (FLOPs)\": 2.85e+20, \"Inference compute (FLOPs)\": 79000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 838235294117.6471, \"Training compute times parameters\": 9.69e+28, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT-2\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"A Radford, J Wu, R Child, D Luan, D Amodei\", \"Publication date\": \"2019-02-14T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Language Models are Unsupervised Multitask Learners\", \"Link\": \"https://openai.com/blog/better-language-models/\", \"Citations\": 1700.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 1500000000.0, \"Training compute (FLOPs)\": 2.49e+21, \"Inference compute (FLOPs)\": 3400000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"5.00E+09\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": \"40\", \"Approach\": \"Supervised\", \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1660000000000.0, \"Training compute times parameters\": 3.735e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"ProxylessNAS\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"MIT\", \"Author(s)\": \"Han Cai, Ligeng Zhu, and Song Han\", \"Publication date\": \"2019-02-23T00:00:00\", \"Year\": \"2019\", \"Reference\": \"ProxylessNAS: Direct neural architecture search on target task and hardware\", \"Link\": \"https://arxiv.org/pdf/1812.00332.pdf\", \"Citations\": 996.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 3.70656e+19, \"Inference compute (FLOPs)\": 263000000000.0, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": \"1.28E+06\", \"Equivalent training time (hours)\": 200.0, \"Inference time (ms)\": \"5.1\", \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"MnasNet-A1 + SSDLite\", \"Domain\": \"All\", \"Task\": \"Performing image classification and object detection on mobile devices\", \"Organization(s)\": \"Google \", \"Author(s)\": \"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le\", \"Publication date\": \"2019-05-29T00:00:00\", \"Year\": \"2019\", \"Reference\": \"MnasNet: Platform-Aware Neural Architecture Search for Mobile\", \"Link\": \"https://arxiv.org/pdf/1807.11626.pdf\", \"Citations\": 1430.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 4900000.0, \"Training compute (FLOPs)\": 1.5e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"MS COCO\", \"Training dataset size (datapoints)\": \"1.18E+05\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"https://proceedings.neurips.cc/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 306122448979591.8, \"Training compute times parameters\": 7.35e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"MnasNet-A3\", \"Domain\": \"All\", \"Task\": \"Performing image classification and object detection on mobile devices\", \"Organization(s)\": \"Google \", \"Author(s)\": \"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le\", \"Publication date\": \"2019-05-29T00:00:00\", \"Year\": \"2019\", \"Reference\": \"MnasNet: Platform-Aware Neural Architecture Search for Mobile\", \"Link\": \"https://arxiv.org/pdf/1807.11626.pdf\", \"Citations\": 1430.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 5200000.0, \"Training compute (FLOPs)\": 1.5e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LessWrong\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 288461538461538.44, \"Training compute times parameters\": 7.8e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"DLRM-2020\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook AI\", \"Author(s)\": \"M Naumov, D Mudigere, HJM Shi, J Huang\", \"Publication date\": \"2019-05-31T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Deep Learning Recommendation Model for Personalization and Recommendation Systems\", \"Link\": \"https://arxiv.org/abs/1906.00091\", \"Citations\": 140.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 100000000000.0, \"Training compute (FLOPs)\": 4e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": \"Supervised\", \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 40000000.0, \"Training compute times parameters\": 4e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"ObjectNet\", \"Domain\": \"All\", \"Task\": \"Object recognition\", \"Organization(s)\": \"MIT\", \"Author(s)\": \"Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfre- und, Josh Tenenbaum, and Boris Katz\", \"Publication date\": \"2019-09-06T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models\", \"Link\": \"https://papers.nips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf\", \"Citations\": 2390.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 38000000.0, \"Training compute (FLOPs)\": 1.94e+19, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Internal data\", \"Training dataset size (datapoints)\": \"19M utterences\", \"Equivalent training time (hours)\": 108.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 510526315789.4737, \"Training compute times parameters\": 7.372e+26, \"Era\": \"Large Scale Era\"}, {\"System\": \"Megatron-LM\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"NVIDIA\", \"Author(s)\": \"M Shoeybi, M Patwary, R Puri, P LeGresley\", \"Publication date\": \"2019-09-17T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\", \"Link\": \"https://arxiv.org/abs/1909.08053\", \"Citations\": 246.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 8300000000.0, \"Training compute (FLOPs)\": 9.1e+21, \"Inference compute (FLOPs)\": 18000000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1096385542168.6747, \"Training compute times parameters\": 7.553e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"Megatron-BERT\", \"Domain\": \"Large Scale\", \"Task\": null, \"Organization(s)\": \"NVIDIA\", \"Author(s)\": \"M Shoeybi, M Patwary, R Puri, P LeGresley\", \"Publication date\": \"2019-09-17T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\", \"Link\": \"https://arxiv.org/abs/1909.08053\", \"Citations\": 246.0, \"Highly influential citations\": 47.0, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 3900000000.0, \"Training compute (FLOPs)\": 5.68e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 14564102564102.564, \"Training compute times parameters\": 2.2152e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaX-1\", \"Domain\": \"All\", \"Task\": \"Neural architecture search for computer vision\", \"Organization(s)\": \"Brown and Facebook AI Research\", \"Author(s)\": \"Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1\", \"Publication date\": \"2019-10-02T00:00:00\", \"Year\": \"2019\", \"Reference\": \"AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search\", \"Link\": \"https://arxiv.org/pdf/1903.11059.pdf\", \"Citations\": 50.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 579000000.0, \"Training compute (FLOPs)\": 7.6e+18, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 13126079447.322971, \"Training compute times parameters\": 4.4004e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"Rubik's cube\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Open AI\", \"Author(s)\": \"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang\\n\", \"Publication date\": \"2019-10-15T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Solving Rubik\\u2019s Cube with a Robot Hand\", \"Link\": \"https://openai.com/blog/solving-rubiks-cube/\", \"Citations\": 227.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 27800000.0, \"Training compute (FLOPs)\": 8.54e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 30719424460431.656, \"Training compute times parameters\": 2.37412e+28, \"Era\": \"Large Scale Era\"}, {\"System\": \"T5-3B\", \"Domain\": \"All\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Google\", \"Author(s)\": \"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu\", \"Publication date\": \"2019-10-23T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"Link\": \"https://arxiv.org/abs/1910.10683\", \"Citations\": 1540.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 3000000000.0, \"Training compute (FLOPs)\": 1.04e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Colossal Clean Crawled Corpus (C4)\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3466666666666.6665, \"Training compute times parameters\": 3.12e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"T5-11B\", \"Domain\": \"Large Scale\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Google\", \"Author(s)\": \"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu\", \"Publication date\": \"2019-10-23T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"Link\": \"https://arxiv.org/abs/1910.10683\", \"Citations\": 1540.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 11000000000.0, \"Training compute (FLOPs)\": 4.05e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Colossal Clean Crawled Corpus (C4)\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3681818181818.182, \"Training compute times parameters\": 4.4550000000000004e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaStar\", \"Domain\": \"Large Scale\", \"Task\": \"StarCraft\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Micha\\u00ebl Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,R\\u00e9mi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario W\\u00fcnsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver\", \"Publication date\": \"2019-10-30T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Grandmaster level in StarCraft II using multi-agent reinforcement learning\", \"Link\": \"https://www.nature.com/articles/s41586-019-1724-z.epdf\", \"Citations\": 1040.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 139000000.0, \"Training compute (FLOPs)\": 2.02e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1453237410071942.5, \"Training compute times parameters\": 2.8078e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"OpenAI Five\", \"Domain\": \"All\", \"Task\": \"DotA\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,\\nPrzemys\\u0142aw \\u201cPsyho\\\" D\\u0119biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal J\\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pond\\u00e9 de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang\", \"Publication date\": \"2019-12-13T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Dota 2 with Large Scale Deep Reinforcement Learning\", \"Link\": \"https://cdn.openai.com/dota-2.pdf\", \"Citations\": 349.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA improvement\", \"Hidden layers\": null, \"Parameters\": 159000000.0, \"Training compute (FLOPs)\": 1.3e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 81761006289308.17, \"Training compute times parameters\": 2.067e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"OpenAI Five\", \"Domain\": \"Large Scale\", \"Task\": \"DOTA 5v5\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"J Raiman, S Zhang, F Wolski\", \"Publication date\": \"2019-12-13T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Dota 2 with Large Scale Deep Reinforcement Learning\", \"Link\": \"https://arxiv.org/abs/1912.06680\", \"Citations\": 454.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA improvement\", \"Hidden layers\": null, \"Parameters\": 10000000.0, \"Training compute (FLOPs)\": 6.7e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 6700000000000000.0, \"Training compute times parameters\": 6.7e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"AlphaFold\", \"Domain\": \"All\", \"Task\": \"Protein folding prediction\", \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin \\u017d\\u00eddek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu & Demis Hassabis\", \"Publication date\": \"2020-01-15T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Improved protein structure prediction using potentials from deep learning\", \"Link\": \"https://www.nature.com/articles/s41586-019-1923-7\", \"Citations\": 840.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 69000000.0, \"Training compute (FLOPs)\": 1e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1449275362318.8406, \"Training compute times parameters\": 6.9e+27, \"Era\": \"Large Scale Era\"}, {\"System\": \"Meena\", \"Domain\": \"Large Scale\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Google AI\", \"Author(s)\": \"Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, Quoc V. Le\", \"Publication date\": \"2020-01-28T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Towards a Human-like Open-Domain Chatbot\", \"Link\": \"https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html\", \"Citations\": 257.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 2600000000.0, \"Training compute (FLOPs)\": 1.12e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 43076923076923.08, \"Training compute times parameters\": 2.912e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"Turing NLG\", \"Domain\": \"All\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Microsoft\", \"Author(s)\": \"C Rosset\", \"Publication date\": \"2020-02-13T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Turing-NLG: A 17-billion-parameter language model by Microsoft\", \"Link\": \"https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\", \"Citations\": 34.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 17000000000.0, \"Training compute (FLOPs)\": 1.57e+22, \"Inference compute (FLOPs)\": 36000000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 923529411764.7058, \"Training compute times parameters\": 2.669e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"ProGen\", \"Domain\": \"All\", \"Task\": \"Protein generation\", \"Organization(s)\": \"Salesforce research, Stanford\", \"Author(s)\": \"A Madani, B McCann, N Naik, NS Keskar\", \"Publication date\": \"2020-03-13T00:00:00\", \"Year\": \"2020\", \"Reference\": \"ProGen: Language Modeling for Protein Generation\", \"Link\": \"https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2\", \"Citations\": 46.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1200000000.0, \"Training compute (FLOPs)\": 2.7e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 225000000000.0, \"Training compute times parameters\": 3.24e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT-3 175B\", \"Domain\": \"Large Scale\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\", \"Publication date\": \"2020-04-28T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Language Models are Few-Shot Learners\", \"Link\": \"https://arxiv.org/abs/2005.14165\", \"Citations\": 1530.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 175000000000.0, \"Training compute (FLOPs)\": 3.14e+23, \"Inference compute (FLOPs)\": 740000000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": \"45TB\", \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1794285714285.7144, \"Training compute times parameters\": 5.495e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"Once for All\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"MIT-IBM Watson AI Lab, \", \"Author(s)\": \"Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han\", \"Publication date\": \"2020-04-29T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Once for all: Train one network and specialize it for efficient deployment.\", \"Link\": \"https://arxiv.org/pdf/1908.09791.pdf\", \"Citations\": 371.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 7700000.0, \"Training compute (FLOPs)\": 1.78e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Imagenet\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 231168831168831.16, \"Training compute times parameters\": 1.3706e+28, \"Era\": \"Large Scale Era\"}, {\"System\": \"iGPT-L\", \"Domain\": \"All\", \"Task\": \"Image completion\", \"Organization(s)\": \"Open AI\", \"Author(s)\": \"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever\", \"Publication date\": \"2020-06-17T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Generative Pretraining from Pixels\", \"Link\": \"https://openai.com/blog/image-gpt/\", \"Citations\": 182.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1360000000.0, \"Training compute (FLOPs)\": 8.91e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 6551470588235.295, \"Training compute times parameters\": 1.21176e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"iGPT-XL\", \"Domain\": \"All\", \"Task\": \"Image completion\", \"Organization(s)\": \"Open AI\", \"Author(s)\": \"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever\", \"Publication date\": \"2020-06-17T00:00:00\", \"Year\": \"2020\", \"Reference\": \"Generative Pretraining from Pixels\", \"Link\": \"https://openai.com/blog/image-gpt/\", \"Citations\": 182.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 6800000000.0, \"Training compute (FLOPs)\": 3.3e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 4852941176470.588, \"Training compute times parameters\": 2.244e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"GShard (600B)\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Brain\", \"Author(s)\": \"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\", \"Publication date\": \"2020-06-30T00:00:00\", \"Year\": \"2020\", \"Reference\": \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\", \"Link\": \"https://arxiv.org/abs/2006.16668\", \"Citations\": 91.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 600000000000.0, \"Training compute (FLOPs)\": 1.33e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 22166666666.666664, \"Training compute times parameters\": 7.98e+33, \"Era\": \"Large Scale Era\"}, {\"System\": \"GShard (dense)\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Brain\", \"Author(s)\": \"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\", \"Publication date\": \"2020-06-30T00:00:00\", \"Year\": \"2020\", \"Reference\": \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\", \"Link\": \"https://arxiv.org/abs/2006.16668\", \"Citations\": 91.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 2300000000.0, \"Training compute (FLOPs)\": 2.6e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 11304347826086.957, \"Training compute times parameters\": 5.98e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"wave2vec 2.0 LARGE\", \"Domain\": \"All\", \"Task\": \"Speech completion\", \"Organization(s)\": \"Facebook\", \"Author(s)\": \"Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli\", \"Publication date\": \"2020-10-22T00:00:00\", \"Year\": \"2020\", \"Reference\": \"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\", \"Link\": \"https://arxiv.org/pdf/2006.11477.pdf\", \"Citations\": 410.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA Improvement\", \"Hidden layers\": null, \"Parameters\": 317000000.0, \"Training compute (FLOPs)\": 4.34e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"LibriSpeech\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1369085173501.5774, \"Training compute times parameters\": 1.37578e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"CPM-Large\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Tsinghua University, BAAI\", \"Author(s)\": \"Z Zhang, X Han, H Zhou, P Ke, Y Gu, D Ye, Y Qin, Y Su\", \"Publication date\": \"2020-12-01T00:00:00\", \"Year\": \"2020\", \"Reference\": \"CPM: A Large-scale Generative Chinese Pre-trained Language Model\", \"Link\": \"https://arxiv.org/abs/2012.00413\", \"Citations\": 10.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 2600000000.0, \"Training compute (FLOPs)\": 1.8e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 692307692307.6923, \"Training compute times parameters\": 4.68e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"AraGPT2-Mega\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"American University of Beirut\", \"Author(s)\": \"W Antoun, F Baly, H Hajj\", \"Publication date\": \"2020-12-31T00:00:00\", \"Year\": \"2020\", \"Reference\": \"AraGPT2: Pre-Trained Transformer for Arabic Language Generation\", \"Link\": \"https://arxiv.org/abs/2012.15520v1\", \"Citations\": 4.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1500000000.0, \"Training compute (FLOPs)\": 2e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1333333333333.3333, \"Training compute times parameters\": 3e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"CLIP (ViT L/14@336px)\", \"Domain\": \"All\", \"Task\": \"Zero-shot image classification\", \"Organization(s)\": \"Open AI\", \"Author(s)\": \"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\", \"Publication date\": \"2021-01-05T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Learning Transferable Visual Models From Natural Language Supervision\", \"Link\": \"https://arxiv.org/abs/2103.00020\", \"Citations\": 130.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 370000000.0, \"Training compute (FLOPs)\": 1.05e+22, \"Inference compute (FLOPs)\": 110000000.0, \"Training dataset\": \"Custom image-text pairs from the internet\", \"Training dataset size (datapoints)\": \"4.00E+08\", \"Equivalent training time (hours)\": 86016.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 28378378378378.375, \"Training compute times parameters\": 3.8849999999999994e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"DALL-E\", \"Domain\": \"All\", \"Task\": \"Text-to-image\", \"Organization(s)\": \"OpenAI\", \"Author(s)\": \"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever\", \"Publication date\": \"2021-01-05T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Zero-Shot Text-to-Image Generation\", \"Link\": \"https://openai.com/blog/dall-e/\", \"Citations\": 80.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 12000000000.0, \"Training compute (FLOPs)\": 4.7e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3916666666666.667, \"Training compute times parameters\": 5.64e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"Switch\", \"Domain\": \"All\", \"Task\": \"Text autocompletion\", \"Organization(s)\": \"Google Brain\", \"Author(s)\": \"William Fedus, Barret Zoph, Noam Shazeer\", \"Publication date\": \"2021-01-11T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\", \"Link\": \"https://arxiv.org/abs/2101.03961\", \"Citations\": 80.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1600000000000.0, \"Training compute (FLOPs)\": 8.22e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 51374999999.99999, \"Training compute times parameters\": 1.3152e+35, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT-Neo\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": null, \"Author(s)\": null, \"Publication date\": \"2021-03-21T00:00:00\", \"Year\": \"2021\", \"Reference\": \"GPT-Neo\", \"Link\": \"https://www.eleuther.ai/projects/gpt-neo/\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 2700000000.0, \"Training compute (FLOPs)\": 7.9e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2925925925925.926, \"Training compute times parameters\": 2.133e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"PanGu-\\u03b1\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"PanGu-\\u03b1 team\", \"Author(s)\": \"Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi LiaoZhiwei WangXin JiangZhenzhang YangKaisheng WangXiaoda ZhangChen LiZiyan GongYifan YaoXinjing HuangJun WangJianfeng YuQi GuoYue YuYan ZhangJin WangHengtao TaoDasen YanZexuan YiFang PengFangqing JiangHan ZhangLingfeng DengYehong ZhangZhe LinChao ZhangShaojie ZhangMingyue GuoShanzhi GuGaojun FanYaowei WangXuefeng JinQun LiuYonghong Tian\", \"Publication date\": \"2021-04-25T00:00:00\", \"Year\": \"2021\", \"Reference\": \"PanGu-\\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation\", \"Link\": \"https://arxiv.org/abs/2104.12369\", \"Citations\": 5.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 207000000000.0, \"Training compute (FLOPs)\": 5.83e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 281642512077.2947, \"Training compute times parameters\": 1.2068100000000001e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"GPT-J-6B\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": null, \"Author(s)\": null, \"Publication date\": \"2021-05-01T00:00:00\", \"Year\": \"2021\", \"Reference\": null, \"Link\": \"https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 6050000000.0, \"Training compute (FLOPs)\": 1.5e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2479338842975.2065, \"Training compute times parameters\": 9.075e+31, \"Era\": \"Large Scale Era\"}, {\"System\": \"ProtT5-XXL\", \"Domain\": \"All\", \"Task\": \"Proteins\", \"Organization(s)\": \"Technical Univeristy of Munich, Med AI Technology, Google AI, NVIDIA, Oak Ridge National Laboratory\", \"Author(s)\": \"A Elnaggar, M Heinzinger, C Dallago, G Rihawi\", \"Publication date\": \"2021-05-04T00:00:00\", \"Year\": \"2021\", \"Reference\": \"ProtTrans: Towards Cracking the Language of Life\\u2019s Code Through Self-Supervised Learning\", \"Link\": \"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3\", \"Citations\": 57.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 11000000000.0, \"Training compute (FLOPs)\": 7.37e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 6700000000000.0, \"Training compute times parameters\": 8.107e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"HyperClova\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": null, \"Author(s)\": null, \"Publication date\": \"2021-05-25T00:00:00\", \"Year\": \"2021\", \"Reference\": null, \"Link\": \"https://www.navercorp.com/promotion/pressReleasesView/30546\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 204000000000.0, \"Training compute (FLOPs)\": 6.3e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 308823529411.7647, \"Training compute times parameters\": 1.2852e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"CogView\", \"Domain\": \"All\", \"Task\": \"Text-to-image\", \"Organization(s)\": \"Tsinghua University, DAMO academy Alibaba\", \"Author(s)\": \"M Ding, Z Yang, W Hong, W Zheng, C Zhou\", \"Publication date\": \"2021-05-26T00:00:00\", \"Year\": \"2021\", \"Reference\": \"CogView: Mastering Text-to-Image Generation via Transformers\", \"Link\": \"https://arxiv.org/abs/2105.13290\", \"Citations\": 1.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 4000000000.0, \"Training compute (FLOPs)\": 2.68e+22, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 6700000000000.0, \"Training compute times parameters\": 1.072e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"ViT-G/14\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google Research, Brain Team\", \"Author(s)\": \"X Zhai, A Kolesnikov, N Houlsby, L Beyer\", \"Publication date\": \"2021-06-08T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Scaling Vision Transformers\", \"Link\": \"https://arxiv.org/abs/2106.04560\", \"Citations\": 6.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 1800000000.0, \"Training compute (FLOPs)\": 3.4e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1888888888888.889, \"Training compute times parameters\": 6.12e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"HuBERT\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook AI Research\", \"Author(s)\": \"Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed\", \"Publication date\": \"2021-07-27T00:00:00\", \"Year\": \"2021\", \"Reference\": \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"Link\": \"https://arxiv.org/pdf/2106.07447.pdf\", \"Citations\": 37.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA Improvement\", \"Hidden layers\": null, \"Parameters\": 1000000000.0, \"Training compute (FLOPs)\": 5.54e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"LibriSpeech\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 5540000000000.0, \"Training compute times parameters\": 5.54e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"SEER\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Facebook AI Research, Inria\", \"Author(s)\": \"Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski\", \"Publication date\": \"2021-07-29T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Self-supervised Pretraining of Visual Features in the Wild\", \"Link\": \"https://arxiv.org/pdf/2103.01988.pdf\", \"Citations\": 39.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Important context\", \"Hidden layers\": null, \"Parameters\": 1300000000.0, \"Training compute (FLOPs)\": 4.42e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 3400000000000.0, \"Training compute times parameters\": 5.746e+30, \"Era\": \"Large Scale Era\"}, {\"System\": \"Jurassic-1-Jumbo\", \"Domain\": \"Large Scale\", \"Task\": null, \"Organization(s)\": \"AI21 Labs\", \"Author(s)\": null, \"Publication date\": \"2021-08-11T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Jurassic-1: Technical Details and Evaluation\", \"Link\": \"https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf\", \"Citations\": 9.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 178000000000.0, \"Training compute (FLOPs)\": 3.7e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2078651685393.2585, \"Training compute times parameters\": 6.586e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"M6-10T\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Alibaba\", \"Author(s)\": \"Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang\", \"Publication date\": \"2021-10-08T00:00:00\", \"Year\": \"2021\", \"Reference\": \"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining\", \"Link\": \"https://arxiv.org/abs/2110.03888\", \"Citations\": 14.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 10000000000000.0, \"Training compute (FLOPs)\": 5.53e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 122880.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 553000000.0, \"Training compute times parameters\": 5.5299999999999995e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"Megatron-Turing NLG 530B\", \"Domain\": \"Large Scale\", \"Task\": null, \"Organization(s)\": \"Microsoft\", \"Author(s)\": null, \"Publication date\": \"2021-10-11T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World\\u2019s Largest and Most Powerful Generative Language Model\", \"Link\": \"https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 530000000000.0, \"Training compute (FLOPs)\": 1.35e+24, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2547169811320.755, \"Training compute times parameters\": 7.155e+35, \"Era\": \"Large Scale Era\"}, {\"System\": \"Yuan 1.0\", \"Domain\": \"Large Scale\", \"Task\": null, \"Organization(s)\": \"Inspur\", \"Author(s)\": \"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu\", \"Publication date\": \"2021-11-10T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\", \"Link\": \"https://arxiv.org/abs/2110.04725\", \"Citations\": 4.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 245000000000.0, \"Training compute (FLOPs)\": 4.1e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 1673469387755.102, \"Training compute times parameters\": 1.0045e+35, \"Era\": \"Large Scale Era\"}, {\"System\": \"Gopher\", \"Domain\": \"Large Scale\", \"Task\": null, \"Organization(s)\": \"DeepMind\", \"Author(s)\": \"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu and Geoffrey Irving\", \"Publication date\": \"2021-12-08T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Scaling Language Models: Methods, Analysis & Insights from Training Gopher\", \"Link\": \"https://deepmind.com/blog/article/language-modelling-at-scale\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 280000000000.0, \"Training compute (FLOPs)\": 6.31e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": null, \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2253571428571.4287, \"Training compute times parameters\": 1.7668000000000003e+35, \"Era\": \"Large Scale Era\"}, {\"System\": \"LaMDA\", \"Domain\": \"Large Scale\", \"Task\": null, \"Organization(s)\": \"Google\", \"Author(s)\": \"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le\", \"Publication date\": \"2022-01-20T00:00:00\", \"Year\": \"2022\", \"Reference\": \"LaMDA: Language Models for Dialog Applications\", \"Link\": \"https://arxiv.org/abs/2201.08239\", \"Citations\": 0.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 137000000000.0, \"Training compute (FLOPs)\": 3.55e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Infiniset\", \"Training dataset size (datapoints)\": \"1.56E+09\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"OpenAI\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 2591240875912.4087, \"Training compute times parameters\": 4.8635e+34, \"Era\": \"Large Scale Era\"}, {\"System\": \"ALIGN\", \"Domain\": \"Large Scale\", \"Task\": null, \"Organization(s)\": \"Google AI\", \"Author(s)\": \"ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocV.Le,YunhsuanSung, Zhen Li, and Tom Duerig\", \"Publication date\": \"2021-06-11T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Scaling up visual and vision-language representation learning with noisy text supervision\", \"Link\": \"https://arxiv.org/pdf/2102.05918.pdf\", \"Citations\": 75.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 820000000.0, \"Training compute (FLOPs)\": 2.15e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": \"1.60E+09\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Scaling Vision Transformers\", \"Type of developer\": \"Company\", \"Training compute per parameter (FLOPs)\": 262195121951219.5, \"Training compute times parameters\": 1.763e+32, \"Era\": \"Large Scale Era\"}, {\"System\": \"Meta Pseudo Labels\", \"Domain\": \"Large Scale\", \"Task\": null, \"Organization(s)\": \"Google AI, Brain team\", \"Author(s)\": \"Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le\", \"Publication date\": \"2021-03-01T00:00:00\", \"Year\": \"2021\", \"Reference\": \"Meta pseudo labels\", \"Link\": \"https://arxiv.org/pdf/2003.10580.pdf\", \"Citations\": 131.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"SOTA Improvement\", \"Hidden layers\": null, \"Parameters\": 480000000.0, \"Training compute (FLOPs)\": 2.12e+23, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"ImageNet\", \"Training dataset size (datapoints)\": \"1.30E+08\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Scaling Vision Transformers\", \"Type of developer\": \"Company\", \"Training compute per parameter (FLOPs)\": 441666666666666.6, \"Training compute times parameters\": 1.0176e+32, \"Era\": \"Large Scale Era\"}, {\"System\": null, \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"University of Freiburg\", \"Author(s)\": \"Ilya Loshchilov and Frank Hutter\", \"Publication date\": \"2019-01-04T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Decoupled weight decay regularization.\", \"Link\": \"https://arxiv.org/pdf/1711.05101.pdf\", \"Citations\": 2060.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 36500000.0, \"Training compute (FLOPs)\": 2.47e+18, \"Inference compute (FLOPs)\": 1730000000.0, \"Training dataset\": \"CIFAR-10\", \"Training dataset size (datapoints)\": \"5.00E+04\", \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Automated Concatenation of Embeddings for Structured Prediction\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": 67671232876.712326, \"Training compute times parameters\": 9.0155e+25, \"Era\": \"Large Scale Era\"}, {\"System\": null, \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Tel Aviv University, MIT\", \"Author(s)\": \"Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson.\", \"Publication date\": \"2019-04-04T00:00:00\", \"Year\": \"2019\", \"Reference\": \"Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.\", \"Link\": \"https://arxiv.org/pdf/1902.09492.pdf\", \"Citations\": 129.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 2.56e+18, \"Inference compute (FLOPs)\": 3660000000000.0, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Automated Concatenation of Embeddings for Structured Prediction\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"R-FCN\", \"Domain\": \"All\", \"Task\": \"Object detection\", \"Organization(s)\": \"Microsoft research, Tsinghua university\", \"Author(s)\": \"Jifeng Dai, Y. Li, Kaiming He, and Jian Sun\", \"Publication date\": \"2016-06-21T00:00:00\", \"Year\": \"2016\", \"Reference\": \"R-fcn: Object detection via region-based fully convolutional networks.\", \"Link\": \"https://arxiv.org/pdf/1605.06409.pdf\", \"Citations\": 4490.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 6.14929e+16, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"PASCAL VOC (2007 and 2012 vesrions) + MS COCO\", \"Training dataset size (datapoints)\": \"9.44E+04\", \"Equivalent training time (hours)\": 12.06567222, \"Inference time (ms)\": \"170\", \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"Dynamic Head: Unifying Object Detection Heads with Attentions\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"ALBERT-xxlarge\", \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Google research, Toyota Technological Institute at Chicago\", \"Author(s)\": \"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut\", \"Publication date\": \"2020-02-09T00:00:00\", \"Year\": \"2020\", \"Reference\": \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\", \"Link\": \"https://arxiv.org/pdf/1909.11942.pdf\", \"Citations\": 2180.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": 235000000.0, \"Training compute (FLOPs)\": 2.54373e+21, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 17408.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\", \"Type of developer\": null, \"Training compute per parameter (FLOPs)\": 10824382978723.404, \"Training compute times parameters\": 5.9777655000000005e+29, \"Era\": \"Large Scale Era\"}, {\"System\": \"Part-of-sentence tagging model\", \"Domain\": \"All\", \"Task\": \"POS tagging\", \"Organization(s)\": \"University of Toronto\", \"Author(s)\": \"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton\", \"Publication date\": \"2016-07-21T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Layer Normalization.\", \"Link\": \"https://arxiv.org/pdf/1607.06450.pdf\", \"Citations\": 4130.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 1.45411e+17, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 12.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": \"Named Entity Recognition model\", \"Domain\": \"All\", \"Task\": \"Named Entity Recognition model\", \"Organization(s)\": \"University of Toronto\", \"Author(s)\": \"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton\", \"Publication date\": \"2016-07-21T00:00:00\", \"Year\": \"2016\", \"Reference\": \"Layer Normalization.\", \"Link\": \"https://arxiv.org/pdf/1607.06450.pdf\", \"Citations\": 4130.0, \"Highly influential citations\": null, \"Inclusion criteria\": \"Highly cited\", \"Hidden layers\": null, \"Parameters\": null, \"Training compute (FLOPs)\": 9.69408e+16, \"Inference compute (FLOPs)\": null, \"Training dataset\": null, \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": 8.0, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": null, \"Training compute times parameters\": null, \"Era\": \"Large Scale Era\"}, {\"System\": null, \"Domain\": \"All\", \"Task\": null, \"Organization(s)\": \"Tsinghua University, Princeton, Mila- Quebec AI, University de Montreal, HEC, CIFAR\", \"Author(s)\": \"Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang.\", \"Publication date\": \"2020-11-23T00:00:00\", \"Year\": \"2020\", \"Reference\": \"KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.\", \"Link\": \"https://arxiv.org/pdf/1911.06136.pdf\", \"Citations\": 96.0, \"Highly influential citations\": null, \"Inclusion criteria\": null, \"Hidden layers\": null, \"Parameters\": 110000000.0, \"Training compute (FLOPs)\": 1.24e+20, \"Inference compute (FLOPs)\": null, \"Training dataset\": \"Wikipedia+BookCorpus\", \"Training dataset size (datapoints)\": null, \"Equivalent training time (hours)\": null, \"Inference time (ms)\": null, \"Training dataset size (GB)\": null, \"Approach\": null, \"Training objective\": null, \"Milestone\": null, \"Training cost (2020 USD)\": null, \"Source\": \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\", \"Type of developer\": \"Academia\", \"Training compute per parameter (FLOPs)\": 1127272727272.7273, \"Training compute times parameters\": 1.364e+28, \"Era\": \"Large Scale Era\"}], \"data-c3c3497522bfc668b441acdad70ac93c\": [{\"Era\": \"Large Scale Era\", \"start\": \"2015-09-09T00:00:00\", \"stop\": \"2022-01-20T00:00:00\"}], \"data-d751713988987e9331980363e24189ce\": [], \"data-abe686790c3c9f76e73d063ae5551c06\": [{\"Publication date\": \"2015-09-09\", \"Training compute (FLOPs)\": 2.1245906884427466e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-09-20\", \"Training compute (FLOPs)\": 2.2265487622163238e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-10-02\", \"Training compute (FLOPs)\": 2.3333997543596216e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-10-14\", \"Training compute (FLOPs)\": 2.44537847364537e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-10-25\", \"Training compute (FLOPs)\": 2.562730997210463e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-11-06\", \"Training compute (FLOPs)\": 2.685715211297806e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-11-18\", \"Training compute (FLOPs)\": 2.8146013780017695e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-11-29\", \"Training compute (FLOPs)\": 2.949672729163993e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-12-11\", \"Training compute (FLOPs)\": 3.091226088765906e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2015-12-23\", \"Training compute (FLOPs)\": 3.239572525245967e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-01-03\", \"Training compute (FLOPs)\": 3.3950380350595215e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-01-15\", \"Training compute (FLOPs)\": 3.5579642590806984e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-01-27\", \"Training compute (FLOPs)\": 3.7287092333406556e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-02-07\", \"Training compute (FLOPs)\": 3.9076481758678953e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-02-19\", \"Training compute (FLOPs)\": 4.095174311214158e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-03-02\", \"Training compute (FLOPs)\": 4.291699734595151e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-03-13\", \"Training compute (FLOPs)\": 4.4976563174485857e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-03-25\", \"Training compute (FLOPs)\": 4.713496656539377e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-04-06\", \"Training compute (FLOPs)\": 4.939695068532407e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-04-17\", \"Training compute (FLOPs)\": 5.176748632287577e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-04-29\", \"Training compute (FLOPs)\": 5.425178281268535e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-05-11\", \"Training compute (FLOPs)\": 5.685529948272131e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-05-23\", \"Training compute (FLOPs)\": 5.958375765157083e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-06-03\", \"Training compute (FLOPs)\": 6.244315320074299e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-06-15\", \"Training compute (FLOPs)\": 6.543976975155854e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-06-27\", \"Training compute (FLOPs)\": 6.858019247328801e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-07-08\", \"Training compute (FLOPs)\": 7.187132255384143e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-07-20\", \"Training compute (FLOPs)\": 7.53203923662109e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-08-01\", \"Training compute (FLOPs)\": 7.893498136130705e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-08-12\", \"Training compute (FLOPs)\": 8.272303272437647e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-08-24\", \"Training compute (FLOPs)\": 8.669287082974258e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-09-05\", \"Training compute (FLOPs)\": 9.085321953492345e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-09-16\", \"Training compute (FLOPs)\": 9.521322135094307e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-09-28\", \"Training compute (FLOPs)\": 9.978245753369166e+18, \"Domain\": \"All\"}, {\"Publication date\": \"2016-10-10\", \"Training compute (FLOPs)\": 1.045709691382426e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2016-10-21\", \"Training compute (FLOPs)\": 1.0958927908564533e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2016-11-02\", \"Training compute (FLOPs)\": 1.1484841528684376e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2016-11-14\", \"Training compute (FLOPs)\": 1.2035993487614278e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2016-11-25\", \"Training compute (FLOPs)\": 1.261359496098228e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2016-12-07\", \"Training compute (FLOPs)\": 1.321891524812168e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2016-12-19\", \"Training compute (FLOPs)\": 1.385328456145241e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2016-12-31\", \"Training compute (FLOPs)\": 1.4518096949524021e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-01-11\", \"Training compute (FLOPs)\": 1.5214813360808012e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-01-23\", \"Training compute (FLOPs)\": 1.5944964853800032e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-02-04\", \"Training compute (FLOPs)\": 1.6710155961774076e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-02-15\", \"Training compute (FLOPs)\": 1.7512068218843849e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-02-27\", \"Training compute (FLOPs)\": 1.835246385509391e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-03-11\", \"Training compute (FLOPs)\": 1.923318966921374e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-03-22\", \"Training compute (FLOPs)\": 2.015618108670063e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-04-03\", \"Training compute (FLOPs)\": 2.112346641347183e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-04-15\", \"Training compute (FLOPs)\": 2.2137171292559405e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-04-26\", \"Training compute (FLOPs)\": 2.319952337579416e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-05-08\", \"Training compute (FLOPs)\": 2.4312857218744775e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-05-20\", \"Training compute (FLOPs)\": 2.547961941131193e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-05-31\", \"Training compute (FLOPs)\": 2.6702373954063016e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-06-12\", \"Training compute (FLOPs)\": 2.7983807892537782e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-06-24\", \"Training compute (FLOPs)\": 2.9326737222452273e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-07-05\", \"Training compute (FLOPs)\": 3.0734113077731365e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-07-17\", \"Training compute (FLOPs)\": 3.2209028215849062e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-07-29\", \"Training compute (FLOPs)\": 3.375472381393324e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-08-10\", \"Training compute (FLOPs)\": 3.537459659211419e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-08-21\", \"Training compute (FLOPs)\": 3.707220627696738e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-09-02\", \"Training compute (FLOPs)\": 3.8851283424989544e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-09-14\", \"Training compute (FLOPs)\": 4.071573761995106e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-09-25\", \"Training compute (FLOPs)\": 4.2669666064889954e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-10-07\", \"Training compute (FLOPs)\": 4.471736258563753e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-10-19\", \"Training compute (FLOPs)\": 4.686332706625923e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-10-30\", \"Training compute (FLOPs)\": 4.911227533874631e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-11-11\", \"Training compute (FLOPs)\": 5.146914954487699e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-11-23\", \"Training compute (FLOPs)\": 5.3939128997929296e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-12-04\", \"Training compute (FLOPs)\": 5.652764156346461e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-12-16\", \"Training compute (FLOPs)\": 5.924037558801147e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2017-12-28\", \"Training compute (FLOPs)\": 6.208329239909863e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-01-08\", \"Training compute (FLOPs)\": 6.506263940493718e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-01-20\", \"Training compute (FLOPs)\": 6.818496382476126e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-02-01\", \"Training compute (FLOPs)\": 7.1457127074704376e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-02-12\", \"Training compute (FLOPs)\": 7.488631984764451e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-02-24\", \"Training compute (FLOPs)\": 7.84800779135358e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-03-08\", \"Training compute (FLOPs)\": 8.2246298681403e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-03-19\", \"Training compute (FLOPs)\": 8.619325855192215e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-03-31\", \"Training compute (FLOPs)\": 9.032963110689332e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-04-12\", \"Training compute (FLOPs)\": 9.466450616911453e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-04-24\", \"Training compute (FLOPs)\": 9.920740977696394e+19, \"Domain\": \"All\"}, {\"Publication date\": \"2018-05-05\", \"Training compute (FLOPs)\": 1.0396832511958072e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-05-17\", \"Training compute (FLOPs)\": 1.0895771447341548e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-05-29\", \"Training compute (FLOPs)\": 1.141865421960539e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-06-09\", \"Training compute (FLOPs)\": 1.1966629881764972e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-06-21\", \"Training compute (FLOPs)\": 1.254090262945181e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-07-03\", \"Training compute (FLOPs)\": 1.3142734446929851e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-07-14\", \"Training compute (FLOPs)\": 1.3773447880664267e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-07-26\", \"Training compute (FLOPs)\": 1.443442894531669e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-08-07\", \"Training compute (FLOPs)\": 1.5127130169709924e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-08-18\", \"Training compute (FLOPs)\": 1.585307378900857e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-08-30\", \"Training compute (FLOPs)\": 1.661385508947752e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-09-11\", \"Training compute (FLOPs)\": 1.741114591456135e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-09-22\", \"Training compute (FLOPs)\": 1.824669833847758e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-10-04\", \"Training compute (FLOPs)\": 1.912234851689731e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-10-16\", \"Training compute (FLOPs)\": 2.0040020721437996e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-10-27\", \"Training compute (FLOPs)\": 2.100173156873443e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-11-08\", \"Training compute (FLOPs)\": 2.200959445187891e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-11-20\", \"Training compute (FLOPs)\": 2.306582418453203e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-12-02\", \"Training compute (FLOPs)\": 2.4172741868373606e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-12-13\", \"Training compute (FLOPs)\": 2.5332779993373127e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2018-12-25\", \"Training compute (FLOPs)\": 2.65484877838772e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-01-06\", \"Training compute (FLOPs)\": 2.782253679989237e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-01-17\", \"Training compute (FLOPs)\": 2.9157726808510323e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-01-29\", \"Training compute (FLOPs)\": 3.055699193622813e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-02-10\", \"Training compute (FLOPs)\": 3.2023407116913417e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-02-21\", \"Training compute (FLOPs)\": 3.3560194848883586e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-03-05\", \"Training compute (FLOPs)\": 3.5170732276499644e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-03-17\", \"Training compute (FLOPs)\": 3.6858558612079824e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-03-28\", \"Training compute (FLOPs)\": 3.862738291249727e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-04-09\", \"Training compute (FLOPs)\": 4.0481092230765674e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-04-21\", \"Training compute (FLOPs)\": 4.242376015760509e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-05-02\", \"Training compute (FLOPs)\": 4.445965577346598e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-05-14\", \"Training compute (FLOPs)\": 4.6593253029613915e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-05-26\", \"Training compute (FLOPs)\": 4.882924058033896e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-06-06\", \"Training compute (FLOPs)\": 5.117253208618413e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-06-18\", \"Training compute (FLOPs)\": 5.36282770115505e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-06-30\", \"Training compute (FLOPs)\": 5.620187194157032e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-07-12\", \"Training compute (FLOPs)\": 5.8898972440287923e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-07-23\", \"Training compute (FLOPs)\": 6.172550548036776e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-08-04\", \"Training compute (FLOPs)\": 6.468768246613416e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-08-16\", \"Training compute (FLOPs)\": 6.779201288469496e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-08-27\", \"Training compute (FLOPs)\": 7.104531861014896e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-09-08\", \"Training compute (FLOPs)\": 7.445474889517082e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-09-20\", \"Training compute (FLOPs)\": 7.802779608112603e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-10-01\", \"Training compute (FLOPs)\": 8.177231206366582e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-10-13\", \"Training compute (FLOPs)\": 8.569652554711825e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-10-25\", \"Training compute (FLOPs)\": 8.980906012679138e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-11-05\", \"Training compute (FLOPs)\": 9.411895324067613e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-11-17\", \"Training compute (FLOPs)\": 9.863567602883746e+20, \"Domain\": \"All\"}, {\"Publication date\": \"2019-11-29\", \"Training compute (FLOPs)\": 1.0336915414696196e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2019-12-10\", \"Training compute (FLOPs)\": 1.0832978957747548e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2019-12-22\", \"Training compute (FLOPs)\": 1.1352848348953063e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-01-03\", \"Training compute (FLOPs)\": 1.1897666019386896e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-01-14\", \"Training compute (FLOPs)\": 1.2468629224860908e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-01-26\", \"Training compute (FLOPs)\": 1.306699267683271e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-02-07\", \"Training compute (FLOPs)\": 1.369407129983083e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-02-18\", \"Training compute (FLOPs)\": 1.4351243120976302e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-03-01\", \"Training compute (FLOPs)\": 1.503995229815129e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-03-13\", \"Training compute (FLOPs)\": 1.5761712293762472e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-03-25\", \"Training compute (FLOPs)\": 1.6518109200511242e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-04-05\", \"Training compute (FLOPs)\": 1.7310805226952542e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-04-17\", \"Training compute (FLOPs)\": 1.814154235011264e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-04-29\", \"Training compute (FLOPs)\": 1.9012146143756773e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-05-10\", \"Training compute (FLOPs)\": 1.9924529790052734e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-05-22\", \"Training compute (FLOPs)\": 2.0880698283724817e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-06-03\", \"Training compute (FLOPs)\": 2.1882752838344125e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-06-14\", \"Training compute (FLOPs)\": 2.2932895503657313e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-06-26\", \"Training compute (FLOPs)\": 2.403343400475767e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-07-08\", \"Training compute (FLOPs)\": 2.518678681319229e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-07-19\", \"Training compute (FLOPs)\": 2.6395488461932494e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-07-31\", \"Training compute (FLOPs)\": 2.76621951149038e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-08-12\", \"Training compute (FLOPs)\": 2.898969040410733e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-08-23\", \"Training compute (FLOPs)\": 3.0380891546507894e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-09-04\", \"Training compute (FLOPs)\": 3.183885575507567e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-09-16\", \"Training compute (FLOPs)\": 3.336678695695349e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-09-27\", \"Training compute (FLOPs)\": 3.496804283397979e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-10-09\", \"Training compute (FLOPs)\": 3.664614220172095e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-10-21\", \"Training compute (FLOPs)\": 3.8404772741921e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-11-02\", \"Training compute (FLOPs)\": 4.0247799106461547e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-11-13\", \"Training compute (FLOPs)\": 4.217927140964708e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-11-25\", \"Training compute (FLOPs)\": 4.4203434129408105e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-12-07\", \"Training compute (FLOPs)\": 4.6324735433576567e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-12-18\", \"Training compute (FLOPs)\": 4.854783695547016e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2020-12-30\", \"Training compute (FLOPs)\": 5.087762403812066e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-01-11\", \"Training compute (FLOPs)\": 5.331921646969873e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-01-22\", \"Training compute (FLOPs)\": 5.587797973463619e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-02-03\", \"Training compute (FLOPs)\": 5.85595368038794e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-02-15\", \"Training compute (FLOPs)\": 6.136978049286297e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-02-26\", \"Training compute (FLOPs)\": 6.43148864094976e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-03-10\", \"Training compute (FLOPs)\": 6.740132652676323e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-03-22\", \"Training compute (FLOPs)\": 7.063588340392329e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-04-02\", \"Training compute (FLOPs)\": 7.402566509238477e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-04-14\", \"Training compute (FLOPs)\": 7.757812075550581e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-04-26\", \"Training compute (FLOPs)\": 8.130105703771327e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-05-07\", \"Training compute (FLOPs)\": 8.520265522167963e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-05-19\", \"Training compute (FLOPs)\": 8.929148920464509e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-05-31\", \"Training compute (FLOPs)\": 9.35765443419097e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-06-12\", \"Training compute (FLOPs)\": 9.806723719063233e+21, \"Domain\": \"All\"}, {\"Publication date\": \"2021-06-23\", \"Training compute (FLOPs)\": 1.0277343620538909e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-07-05\", \"Training compute (FLOPs)\": 1.0770548342185917e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-07-17\", \"Training compute (FLOPs)\": 1.128742171851582e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-07-28\", \"Training compute (FLOPs)\": 1.182909959677737e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-08-09\", \"Training compute (FLOPs)\": 1.2396772332953783e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-08-21\", \"Training compute (FLOPs)\": 1.299168740766014e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-09-01\", \"Training compute (FLOPs)\": 1.3615152167389357e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-09-13\", \"Training compute (FLOPs)\": 1.4268536697753715e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-09-25\", \"Training compute (FLOPs)\": 1.4953276833905248e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-10-06\", \"Training compute (FLOPs)\": 1.5670877316177551e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-10-18\", \"Training compute (FLOPs)\": 1.642291509653261e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-10-30\", \"Training compute (FLOPs)\": 1.721104280418852e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-11-10\", \"Training compute (FLOPs)\": 1.8036992377240689e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-11-22\", \"Training compute (FLOPs)\": 1.8902578868498444e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-12-04\", \"Training compute (FLOPs)\": 1.9809704434546172e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-12-15\", \"Training compute (FLOPs)\": 2.0760362515256597e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2021-12-27\", \"Training compute (FLOPs)\": 2.175664221492195e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2022-01-08\", \"Training compute (FLOPs)\": 2.2800732892707424e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2022-01-20\", \"Training compute (FLOPs)\": 2.3894928974389656e+22, \"Domain\": \"All\"}, {\"Publication date\": \"2015-09-09\", \"Training compute (FLOPs)\": 4.1128886895007975e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2015-09-20\", \"Training compute (FLOPs)\": 4.2183028040594253e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2015-10-02\", \"Training compute (FLOPs)\": 4.3264187022957667e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2015-10-14\", \"Training compute (FLOPs)\": 4.437305631440606e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2015-10-25\", \"Training compute (FLOPs)\": 4.5510346135446896e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2015-11-06\", \"Training compute (FLOPs)\": 4.667678490957941e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2015-11-18\", \"Training compute (FLOPs)\": 4.787311972993115e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2015-11-29\", \"Training compute (FLOPs)\": 4.910011683786673e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2015-12-11\", \"Training compute (FLOPs)\": 5.03585621136656e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2015-12-23\", \"Training compute (FLOPs)\": 5.164926157970915e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-01-03\", \"Training compute (FLOPs)\": 5.297304191709846e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-01-15\", \"Training compute (FLOPs)\": 5.433075099470828e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-01-27\", \"Training compute (FLOPs)\": 5.572325841267772e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-02-07\", \"Training compute (FLOPs)\": 5.715145605893128e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-02-19\", \"Training compute (FLOPs)\": 5.861625868088596e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-03-02\", \"Training compute (FLOPs)\": 6.011860447086554e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-03-13\", \"Training compute (FLOPs)\": 6.16594556674245e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-03-25\", \"Training compute (FLOPs)\": 6.32397991714782e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-04-06\", \"Training compute (FLOPs)\": 6.486064717826827e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-04-17\", \"Training compute (FLOPs)\": 6.652303782578665e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-04-29\", \"Training compute (FLOPs)\": 6.822803585983573e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-05-11\", \"Training compute (FLOPs)\": 6.997673331585947e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-05-23\", \"Training compute (FLOPs)\": 7.177025021823201e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-06-03\", \"Training compute (FLOPs)\": 7.360973529775829e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-06-15\", \"Training compute (FLOPs)\": 7.549636672758306e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-06-27\", \"Training compute (FLOPs)\": 7.743135287765744e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-07-08\", \"Training compute (FLOPs)\": 7.941593308843956e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-07-20\", \"Training compute (FLOPs)\": 8.1451378465161e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-08-01\", \"Training compute (FLOPs)\": 8.353899269172709e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-08-12\", \"Training compute (FLOPs)\": 8.568011286553537e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-08-24\", \"Training compute (FLOPs)\": 8.787611035403573e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-09-05\", \"Training compute (FLOPs)\": 9.012839167326664e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-09-16\", \"Training compute (FLOPs)\": 9.243839938854565e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-09-28\", \"Training compute (FLOPs)\": 9.480761303822093e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-10-10\", \"Training compute (FLOPs)\": 9.723755008137892e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-10-21\", \"Training compute (FLOPs)\": 9.972976687048024e+21, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-11-02\", \"Training compute (FLOPs)\": 1.022858596469823e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-11-14\", \"Training compute (FLOPs)\": 1.0490746556451647e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-11-25\", \"Training compute (FLOPs)\": 1.0759626373704584e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-12-07\", \"Training compute (FLOPs)\": 1.1035397631521774e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-12-19\", \"Training compute (FLOPs)\": 1.1318236958805012e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2016-12-31\", \"Training compute (FLOPs)\": 1.1608325511498615e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-01-11\", \"Training compute (FLOPs)\": 1.1905849088631992e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-01-23\", \"Training compute (FLOPs)\": 1.2210998251299732e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-02-04\", \"Training compute (FLOPs)\": 1.2523968444688054e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-02-15\", \"Training compute (FLOPs)\": 1.2844960123279265e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-02-27\", \"Training compute (FLOPs)\": 1.3174178879268496e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-03-11\", \"Training compute (FLOPs)\": 1.3511835574218737e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-03-22\", \"Training compute (FLOPs)\": 1.3858146474072247e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-04-03\", \"Training compute (FLOPs)\": 1.4213333387765602e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-04-15\", \"Training compute (FLOPs)\": 1.4577623809181545e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-04-26\", \"Training compute (FLOPs)\": 1.4951251062974344e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-05-08\", \"Training compute (FLOPs)\": 1.5334454453891406e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-05-20\", \"Training compute (FLOPs)\": 1.572747942016949e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-05-31\", \"Training compute (FLOPs)\": 1.6130577690608695e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-06-12\", \"Training compute (FLOPs)\": 1.6544007445932577e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-06-24\", \"Training compute (FLOPs)\": 1.6968033484016997e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-07-05\", \"Training compute (FLOPs)\": 1.740292738962763e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-07-17\", \"Training compute (FLOPs)\": 1.7848967708227033e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-07-29\", \"Training compute (FLOPs)\": 1.8306440124505294e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-08-10\", \"Training compute (FLOPs)\": 1.8775637645306538e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-08-21\", \"Training compute (FLOPs)\": 1.9256860787256886e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-09-02\", \"Training compute (FLOPs)\": 1.9750417769278875e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-09-14\", \"Training compute (FLOPs)\": 2.025662471004512e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-09-25\", \"Training compute (FLOPs)\": 2.0775805830411158e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-10-07\", \"Training compute (FLOPs)\": 2.130829366103139e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-10-19\", \"Training compute (FLOPs)\": 2.1854429255359187e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-10-30\", \"Training compute (FLOPs)\": 2.241456240824971e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-11-11\", \"Training compute (FLOPs)\": 2.298905187972907e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-11-23\", \"Training compute (FLOPs)\": 2.3578265624956428e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-12-04\", \"Training compute (FLOPs)\": 2.4182581029800583e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-12-16\", \"Training compute (FLOPs)\": 2.480238515275342e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2017-12-28\", \"Training compute (FLOPs)\": 2.543807497253721e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-01-08\", \"Training compute (FLOPs)\": 2.609005764253724e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-01-20\", \"Training compute (FLOPs)\": 2.6758750751610945e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-02-01\", \"Training compute (FLOPs)\": 2.744458259149924e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-02-12\", \"Training compute (FLOPs)\": 2.8147992431084557e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-02-24\", \"Training compute (FLOPs)\": 2.8869430797791616e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-03-08\", \"Training compute (FLOPs)\": 2.9609359766207886e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-03-19\", \"Training compute (FLOPs)\": 3.036825325398217e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-03-31\", \"Training compute (FLOPs)\": 3.1146597325266798e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-04-12\", \"Training compute (FLOPs)\": 3.194489050225906e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-04-24\", \"Training compute (FLOPs)\": 3.2763644084242233e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-05-05\", \"Training compute (FLOPs)\": 3.3603382475332417e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-05-17\", \"Training compute (FLOPs)\": 3.4464643520083322e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-05-29\", \"Training compute (FLOPs)\": 3.5347978848248814e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-06-09\", \"Training compute (FLOPs)\": 3.6253954227811394e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-06-21\", \"Training compute (FLOPs)\": 3.7183149927643866e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-07-03\", \"Training compute (FLOPs)\": 3.8136161088866125e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-07-14\", \"Training compute (FLOPs)\": 3.9113598106294255e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-07-26\", \"Training compute (FLOPs)\": 4.011608701928195e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-08-07\", \"Training compute (FLOPs)\": 4.114426991256383e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-08-18\", \"Training compute (FLOPs)\": 4.219880532779787e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-08-30\", \"Training compute (FLOPs)\": 4.328036868501463e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-09-11\", \"Training compute (FLOPs)\": 4.438965271556678e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-09-22\", \"Training compute (FLOPs)\": 4.5527367905458856e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-10-04\", \"Training compute (FLOPs)\": 4.669424295077435e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-10-16\", \"Training compute (FLOPs)\": 4.789102522402194e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-10-27\", \"Training compute (FLOPs)\": 4.911848125315566e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-11-08\", \"Training compute (FLOPs)\": 5.037739721238976e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-11-20\", \"Training compute (FLOPs)\": 5.166857942562804e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-12-02\", \"Training compute (FLOPs)\": 5.2992854883004345e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-12-13\", \"Training compute (FLOPs)\": 5.43510717706755e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2018-12-25\", \"Training compute (FLOPs)\": 5.574410001397415e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-01-06\", \"Training compute (FLOPs)\": 5.71728318344684e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-01-17\", \"Training compute (FLOPs)\": 5.863818232146791e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-01-29\", \"Training compute (FLOPs)\": 6.014109001856271e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-02-10\", \"Training compute (FLOPs)\": 6.168251752402408e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-02-21\", \"Training compute (FLOPs)\": 6.326345210782174e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-03-05\", \"Training compute (FLOPs)\": 6.488490634370546e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-03-17\", \"Training compute (FLOPs)\": 6.654791875828927e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-03-28\", \"Training compute (FLOPs)\": 6.825355449541301e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-04-09\", \"Training compute (FLOPs)\": 7.000290599881701e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-04-21\", \"Training compute (FLOPs)\": 7.179709371192592e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-05-02\", \"Training compute (FLOPs)\": 7.363726679534679e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-05-14\", \"Training compute (FLOPs)\": 7.5524603862738e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-05-26\", \"Training compute (FLOPs)\": 7.746031373584286e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-06-06\", \"Training compute (FLOPs)\": 7.944563621889463e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-06-18\", \"Training compute (FLOPs)\": 8.148184289254961e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-06-30\", \"Training compute (FLOPs)\": 8.357023792814827e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-07-12\", \"Training compute (FLOPs)\": 8.571215892318247e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-07-23\", \"Training compute (FLOPs)\": 8.790897775819785e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-08-04\", \"Training compute (FLOPs)\": 9.016210147530464e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-08-16\", \"Training compute (FLOPs)\": 9.247297317908512e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-08-27\", \"Training compute (FLOPs)\": 9.48430729615473e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-09-08\", \"Training compute (FLOPs)\": 9.72739188493448e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-09-20\", \"Training compute (FLOPs)\": 9.976706777684376e+22, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-10-01\", \"Training compute (FLOPs)\": 1.0232411658251965e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-10-13\", \"Training compute (FLOPs)\": 1.0494670303254324e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-10-25\", \"Training compute (FLOPs)\": 1.0763650686890771e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-11-05\", \"Training compute (FLOPs)\": 1.1039525088604067e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-11-17\", \"Training compute (FLOPs)\": 1.1322470203392525e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-11-29\", \"Training compute (FLOPs)\": 1.1612667254957256e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-12-10\", \"Training compute (FLOPs)\": 1.191030211179618e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2019-12-22\", \"Training compute (FLOPs)\": 1.2215565406276568e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-01-03\", \"Training compute (FLOPs)\": 1.2528652656710264e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-01-14\", \"Training compute (FLOPs)\": 1.2849764392554496e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-01-26\", \"Training compute (FLOPs)\": 1.3179106282859581e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-02-07\", \"Training compute (FLOPs)\": 1.3516889268095273e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-02-18\", \"Training compute (FLOPs)\": 1.3863329695092636e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-03-01\", \"Training compute (FLOPs)\": 1.421864945572051e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-03-13\", \"Training compute (FLOPs)\": 1.4583076128947742e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-03-25\", \"Training compute (FLOPs)\": 1.4956843126726783e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-04-05\", \"Training compute (FLOPs)\": 1.5340189843310927e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-04-17\", \"Training compute (FLOPs)\": 1.5733361808687504e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-04-29\", \"Training compute (FLOPs)\": 1.613661084585635e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-05-10\", \"Training compute (FLOPs)\": 1.6550195232089657e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-05-22\", \"Training compute (FLOPs)\": 1.697437986432071e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-06-03\", \"Training compute (FLOPs)\": 1.7409436428821704e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-06-14\", \"Training compute (FLOPs)\": 1.7855643575344723e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-06-26\", \"Training compute (FLOPs)\": 1.8313287095378298e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-07-08\", \"Training compute (FLOPs)\": 1.878266010533728e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-07-19\", \"Training compute (FLOPs)\": 1.926406323422531e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-07-31\", \"Training compute (FLOPs)\": 1.9757804816345233e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-08-12\", \"Training compute (FLOPs)\": 2.026420108854535e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-08-23\", \"Training compute (FLOPs)\": 2.0783576392902723e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-09-04\", \"Training compute (FLOPs)\": 2.131626338448614e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-09-16\", \"Training compute (FLOPs)\": 2.1862603244378344e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-09-27\", \"Training compute (FLOPs)\": 2.2422945898152536e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-10-09\", \"Training compute (FLOPs)\": 2.2997650240038726e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-10-21\", \"Training compute (FLOPs)\": 2.3587084362841444e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-11-02\", \"Training compute (FLOPs)\": 2.4191625793655212e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-11-13\", \"Training compute (FLOPs)\": 2.481166173558937e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-11-25\", \"Training compute (FLOPs)\": 2.544758931594477e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-12-07\", \"Training compute (FLOPs)\": 2.609981584036479e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-12-18\", \"Training compute (FLOPs)\": 2.67687590539214e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2020-12-30\", \"Training compute (FLOPs)\": 2.7454847408461e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-01-11\", \"Training compute (FLOPs)\": 2.8158520337245375e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-01-22\", \"Training compute (FLOPs)\": 2.8880228536177385e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-02-03\", \"Training compute (FLOPs)\": 2.9620434252669533e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-02-15\", \"Training compute (FLOPs)\": 3.0379611581625203e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-02-26\", \"Training compute (FLOPs)\": 3.115824676902697e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-03-10\", \"Training compute (FLOPs)\": 3.19568385234314e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-03-22\", \"Training compute (FLOPs)\": 3.27758983354557e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-04-02\", \"Training compute (FLOPs)\": 3.361595080532087e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-04-14\", \"Training compute (FLOPs)\": 3.447753397878122e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-04-26\", \"Training compute (FLOPs)\": 3.536119969176573e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-05-07\", \"Training compute (FLOPs)\": 3.62675139240847e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-05-19\", \"Training compute (FLOPs)\": 3.719705716149581e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-05-31\", \"Training compute (FLOPs)\": 3.815042476779047e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-06-12\", \"Training compute (FLOPs)\": 3.912822736596467e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-06-23\", \"Training compute (FLOPs)\": 4.0131091229643034e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-07-05\", \"Training compute (FLOPs)\": 4.1159658683715754e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-07-17\", \"Training compute (FLOPs)\": 4.221458851606331e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-07-28\", \"Training compute (FLOPs)\": 4.329655639933341e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-08-09\", \"Training compute (FLOPs)\": 4.440625532406345e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-08-21\", \"Training compute (FLOPs)\": 4.5544396041997316e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-09-01\", \"Training compute (FLOPs)\": 4.671170752162216e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-09-13\", \"Training compute (FLOPs)\": 4.790893741512183e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-09-25\", \"Training compute (FLOPs)\": 4.913685253715076e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-10-06\", \"Training compute (FLOPs)\": 5.039623935586632e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-10-18\", \"Training compute (FLOPs)\": 5.168790449674946e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-10-30\", \"Training compute (FLOPs)\": 5.301267525935154e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-11-10\", \"Training compute (FLOPs)\": 5.437140014707197e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-11-22\", \"Training compute (FLOPs)\": 5.576494941044188e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-12-04\", \"Training compute (FLOPs)\": 5.7194215604908695e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-12-15\", \"Training compute (FLOPs)\": 5.866011416204807e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2021-12-27\", \"Training compute (FLOPs)\": 6.016358397636265e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2022-01-08\", \"Training compute (FLOPs)\": 6.170558800614979e+23, \"Domain\": \"Large Scale\"}, {\"Publication date\": \"2022-01-20\", \"Training compute (FLOPs)\": 6.328711389076527e+23, \"Domain\": \"Large Scale\"}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-eb8e079f-7c48-4898-8715-9055215c7d21\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Trend</th>\n",
              "      <th>n</th>\n",
              "      <th>Scale (start / end)</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Doubling time</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>All Large Scale Era</td>\n",
              "      <td>60</td>\n",
              "      <td>3e+18 / 2e+22</td>\n",
              "      <td>0.6 OOMs/year [0.5 ; 0.6 ; 0.8]</td>\n",
              "      <td>5.8 months [4.5 ; 5.8 ; 8.0]</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Large Scale Large Scale Era</td>\n",
              "      <td>19</td>\n",
              "      <td>4e+21 / 6e+23</td>\n",
              "      <td>0.3 OOMs/year [0.1 ; 0.3 ; 0.5]</td>\n",
              "      <td>10.7 months [7.8 ; 10.7 ; 27.2]</td>\n",
              "      <td>0.66</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb8e079f-7c48-4898-8715-9055215c7d21')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb8e079f-7c48-4898-8715-9055215c7d21 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb8e079f-7c48-4898-8715-9055215c7d21');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                         Trend   n  ...                    Doubling time    R2\n",
              "0          All Large Scale Era  60  ...     5.8 months [4.5 ; 5.8 ; 8.0]  0.48\n",
              "1  Large Scale Large Scale Era  19  ...  10.7 months [7.8 ; 10.7 ; 27.2]  0.66\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scratchpad\n",
        "t_step = pd.DateOffset(months=1)\n",
        "window_size = pd.DateOffset(months=18)\n",
        "vars = []\n",
        "for i_date in pd.date_range(date_start, date_end, freq=t_step):\n",
        "  # Select data from the period \n",
        "  mask = (i_date - window_size <= df['Publication date']) & (df['Publication date'] < i_date + t_step)\n",
        "  step_df = df[mask]\n",
        "  var = step_df[y_axis].apply(lambda x: np.log10(x)).var()\n",
        "  vars.append({\n",
        "      'Date' : i_date,\n",
        "      'Rolling variance': var,\n",
        "      })\n",
        "\n",
        "vars_df = pd.DataFrame(vars)\n",
        "\n",
        "alt.Chart(vars_df).mark_line().encode(x='Date', y='Rolling variance')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "2E5zrsEgOSUV",
        "outputId": "8ed30a03-57e7-421f-b544-483569e35379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-b1073b43c9d741329ef164c976b39691\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-b1073b43c9d741329ef164c976b39691\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-b1073b43c9d741329ef164c976b39691\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"usermeta\": {\"embedOptions\": {\"theme\": \"fivethirtyeight\"}}, \"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-ea75403ae1d15038ae46d830dc52d14e\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"field\": \"Date\", \"type\": \"temporal\"}, \"y\": {\"field\": \"Rolling variance\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-ea75403ae1d15038ae46d830dc52d14e\": [{\"Date\": \"2010-03-01T00:00:00\", \"Rolling variance\": null}, {\"Date\": \"2010-04-01T00:00:00\", \"Rolling variance\": null}, {\"Date\": \"2010-05-01T00:00:00\", \"Rolling variance\": 0.09107773234810326}, {\"Date\": \"2010-06-01T00:00:00\", \"Rolling variance\": 0.09107773234810326}, {\"Date\": \"2010-07-01T00:00:00\", \"Rolling variance\": 0.09107773234810326}, {\"Date\": \"2010-08-01T00:00:00\", \"Rolling variance\": 0.09107773234810326}, {\"Date\": \"2010-09-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2010-10-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2010-11-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2010-12-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-01-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-02-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-03-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-04-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-05-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-06-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-07-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-08-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-09-01T00:00:00\", \"Rolling variance\": 1.4090825207032573}, {\"Date\": \"2011-10-01T00:00:00\", \"Rolling variance\": 1.2649252061156542}, {\"Date\": \"2011-11-01T00:00:00\", \"Rolling variance\": 1.2649252061156542}, {\"Date\": \"2011-12-01T00:00:00\", \"Rolling variance\": 0.7880321458413302}, {\"Date\": \"2012-01-01T00:00:00\", \"Rolling variance\": 0.7880321458413302}, {\"Date\": \"2012-02-01T00:00:00\", \"Rolling variance\": 0.5095592586516436}, {\"Date\": \"2012-03-01T00:00:00\", \"Rolling variance\": 0.5095592586516436}, {\"Date\": \"2012-04-01T00:00:00\", \"Rolling variance\": null}, {\"Date\": \"2012-05-01T00:00:00\", \"Rolling variance\": null}, {\"Date\": \"2012-06-01T00:00:00\", \"Rolling variance\": 0.02190912815084107}, {\"Date\": \"2012-07-01T00:00:00\", \"Rolling variance\": 0.02190912815084107}, {\"Date\": \"2012-08-01T00:00:00\", \"Rolling variance\": 0.02190912815084107}, {\"Date\": \"2012-09-01T00:00:00\", \"Rolling variance\": 1.338593952989245}, {\"Date\": \"2012-10-01T00:00:00\", \"Rolling variance\": 1.338593952989245}, {\"Date\": \"2012-11-01T00:00:00\", \"Rolling variance\": 1.338593952989245}, {\"Date\": \"2012-12-01T00:00:00\", \"Rolling variance\": 1.338593952989245}, {\"Date\": \"2013-01-01T00:00:00\", \"Rolling variance\": 1.0656826684882748}, {\"Date\": \"2013-02-01T00:00:00\", \"Rolling variance\": 1.0656826684882748}, {\"Date\": \"2013-03-01T00:00:00\", \"Rolling variance\": 1.0656826684882748}, {\"Date\": \"2013-04-01T00:00:00\", \"Rolling variance\": 1.0656826684882748}, {\"Date\": \"2013-05-01T00:00:00\", \"Rolling variance\": 1.0656826684882748}, {\"Date\": \"2013-06-01T00:00:00\", \"Rolling variance\": 1.0656826684882748}, {\"Date\": \"2013-07-01T00:00:00\", \"Rolling variance\": 1.0656826684882748}, {\"Date\": \"2013-08-01T00:00:00\", \"Rolling variance\": 1.0656826684882748}, {\"Date\": \"2013-09-01T00:00:00\", \"Rolling variance\": 1.1970706085016214}, {\"Date\": \"2013-10-01T00:00:00\", \"Rolling variance\": 0.8998859520519682}, {\"Date\": \"2013-11-01T00:00:00\", \"Rolling variance\": 0.9670024469022482}, {\"Date\": \"2013-12-01T00:00:00\", \"Rolling variance\": 1.5616290681525524}, {\"Date\": \"2014-01-01T00:00:00\", \"Rolling variance\": 1.6833314814839415}, {\"Date\": \"2014-02-01T00:00:00\", \"Rolling variance\": 1.6833314814839415}, {\"Date\": \"2014-03-01T00:00:00\", \"Rolling variance\": 1.6833314814839415}, {\"Date\": \"2014-04-01T00:00:00\", \"Rolling variance\": 1.8240961992240048}, {\"Date\": \"2014-05-01T00:00:00\", \"Rolling variance\": 1.8240961992240048}, {\"Date\": \"2014-06-01T00:00:00\", \"Rolling variance\": 1.6964850913145433}, {\"Date\": \"2014-07-01T00:00:00\", \"Rolling variance\": 1.6964850913145433}, {\"Date\": \"2014-08-01T00:00:00\", \"Rolling variance\": 1.9669559683724567}, {\"Date\": \"2014-09-01T00:00:00\", \"Rolling variance\": 1.9414672448517707}, {\"Date\": \"2014-10-01T00:00:00\", \"Rolling variance\": 1.9414672448517707}, {\"Date\": \"2014-11-01T00:00:00\", \"Rolling variance\": 1.9414672448517707}, {\"Date\": \"2014-12-01T00:00:00\", \"Rolling variance\": 1.7736663701418536}, {\"Date\": \"2015-01-01T00:00:00\", \"Rolling variance\": 1.7736663701418536}, {\"Date\": \"2015-02-01T00:00:00\", \"Rolling variance\": 1.7736663701418536}, {\"Date\": \"2015-03-01T00:00:00\", \"Rolling variance\": 1.7736663701418536}, {\"Date\": \"2015-04-01T00:00:00\", \"Rolling variance\": 1.7736663701418536}, {\"Date\": \"2015-05-01T00:00:00\", \"Rolling variance\": 1.8905657779336826}, {\"Date\": \"2015-06-01T00:00:00\", \"Rolling variance\": 1.931099309925581}, {\"Date\": \"2015-07-01T00:00:00\", \"Rolling variance\": 0.6351029650589144}, {\"Date\": \"2015-08-01T00:00:00\", \"Rolling variance\": 0.6351029650589144}, {\"Date\": \"2015-09-01T00:00:00\", \"Rolling variance\": 1.3987408590114938}, {\"Date\": \"2015-10-01T00:00:00\", \"Rolling variance\": 1.3987408590114938}, {\"Date\": \"2015-11-01T00:00:00\", \"Rolling variance\": 1.3987408590114938}, {\"Date\": \"2015-12-01T00:00:00\", \"Rolling variance\": 1.1753849004977333}, {\"Date\": \"2016-01-01T00:00:00\", \"Rolling variance\": 1.7628039550482704}, {\"Date\": \"2016-02-01T00:00:00\", \"Rolling variance\": 1.7628039550482704}, {\"Date\": \"2016-03-01T00:00:00\", \"Rolling variance\": 1.7628039550482704}, {\"Date\": \"2016-04-01T00:00:00\", \"Rolling variance\": 2.6287618813749374}, {\"Date\": \"2016-05-01T00:00:00\", \"Rolling variance\": 2.6287618813749374}, {\"Date\": \"2016-06-01T00:00:00\", \"Rolling variance\": 3.036302375405645}, {\"Date\": \"2016-07-01T00:00:00\", \"Rolling variance\": 2.8641230323609532}, {\"Date\": \"2016-08-01T00:00:00\", \"Rolling variance\": 2.8641230323609532}, {\"Date\": \"2016-09-01T00:00:00\", \"Rolling variance\": 3.6106599028988215}, {\"Date\": \"2016-10-01T00:00:00\", \"Rolling variance\": 3.2463456739953256}, {\"Date\": \"2016-11-01T00:00:00\", \"Rolling variance\": 3.3800819879439423}, {\"Date\": \"2016-12-01T00:00:00\", \"Rolling variance\": 3.3800819879439423}, {\"Date\": \"2017-01-01T00:00:00\", \"Rolling variance\": 3.9089674552187588}, {\"Date\": \"2017-02-01T00:00:00\", \"Rolling variance\": 3.9089674552187588}, {\"Date\": \"2017-03-01T00:00:00\", \"Rolling variance\": 3.9089674552187588}, {\"Date\": \"2017-04-01T00:00:00\", \"Rolling variance\": 4.214911843955178}, {\"Date\": \"2017-05-01T00:00:00\", \"Rolling variance\": 4.214911843955178}, {\"Date\": \"2017-06-01T00:00:00\", \"Rolling variance\": 3.9318890836393963}, {\"Date\": \"2017-07-01T00:00:00\", \"Rolling variance\": 4.655748144797744}, {\"Date\": \"2017-08-01T00:00:00\", \"Rolling variance\": 4.188871633076435}, {\"Date\": \"2017-09-01T00:00:00\", \"Rolling variance\": 4.188871633076435}, {\"Date\": \"2017-10-01T00:00:00\", \"Rolling variance\": 4.878128132050498}, {\"Date\": \"2017-11-01T00:00:00\", \"Rolling variance\": 4.878128132050498}, {\"Date\": \"2017-12-01T00:00:00\", \"Rolling variance\": 4.92234226640693}, {\"Date\": \"2018-01-01T00:00:00\", \"Rolling variance\": 4.217014163440407}, {\"Date\": \"2018-02-01T00:00:00\", \"Rolling variance\": 2.033119768750748}, {\"Date\": \"2018-03-01T00:00:00\", \"Rolling variance\": 2.033119768750748}, {\"Date\": \"2018-04-01T00:00:00\", \"Rolling variance\": 2.141353927112601}, {\"Date\": \"2018-05-01T00:00:00\", \"Rolling variance\": 2.1654984571127978}, {\"Date\": \"2018-06-01T00:00:00\", \"Rolling variance\": 2.527503417587163}, {\"Date\": \"2018-07-01T00:00:00\", \"Rolling variance\": 2.4453837616185563}, {\"Date\": \"2018-08-01T00:00:00\", \"Rolling variance\": 2.494174976565871}, {\"Date\": \"2018-09-01T00:00:00\", \"Rolling variance\": 2.3030734784094538}, {\"Date\": \"2018-10-01T00:00:00\", \"Rolling variance\": 2.0759624858394483}, {\"Date\": \"2018-11-01T00:00:00\", \"Rolling variance\": 2.0759624858394483}, {\"Date\": \"2018-12-01T00:00:00\", \"Rolling variance\": 2.0759624858394483}, {\"Date\": \"2019-01-01T00:00:00\", \"Rolling variance\": 2.264464822491652}, {\"Date\": \"2019-02-01T00:00:00\", \"Rolling variance\": 2.027558930057902}, {\"Date\": \"2019-03-01T00:00:00\", \"Rolling variance\": 2.4261633432501184}, {\"Date\": \"2019-04-01T00:00:00\", \"Rolling variance\": 2.58312506938728}, {\"Date\": \"2019-05-01T00:00:00\", \"Rolling variance\": 1.6914862102900838}, {\"Date\": \"2019-06-01T00:00:00\", \"Rolling variance\": 1.6914862102900838}, {\"Date\": \"2019-07-01T00:00:00\", \"Rolling variance\": 1.297767449616926}, {\"Date\": \"2019-08-01T00:00:00\", \"Rolling variance\": 1.297767449616926}, {\"Date\": \"2019-09-01T00:00:00\", \"Rolling variance\": 1.9235101823373557}, {\"Date\": \"2019-10-01T00:00:00\", \"Rolling variance\": 2.40026949296171}, {\"Date\": \"2019-11-01T00:00:00\", \"Rolling variance\": 2.4936857286726375}, {\"Date\": \"2019-12-01T00:00:00\", \"Rolling variance\": 2.5813696125830305}, {\"Date\": \"2020-01-01T00:00:00\", \"Rolling variance\": 2.5063041171074905}, {\"Date\": \"2020-02-01T00:00:00\", \"Rolling variance\": 2.384613560822586}, {\"Date\": \"2020-03-01T00:00:00\", \"Rolling variance\": 2.29943804161665}, {\"Date\": \"2020-04-01T00:00:00\", \"Rolling variance\": 2.4417941556898874}, {\"Date\": \"2020-05-01T00:00:00\", \"Rolling variance\": 2.526042094849467}, {\"Date\": \"2020-06-01T00:00:00\", \"Rolling variance\": 2.305326770203123}, {\"Date\": \"2020-07-01T00:00:00\", \"Rolling variance\": 2.305326770203123}, {\"Date\": \"2020-08-01T00:00:00\", \"Rolling variance\": 2.0782557287992476}, {\"Date\": \"2020-09-01T00:00:00\", \"Rolling variance\": 2.113122661531653}, {\"Date\": \"2020-10-01T00:00:00\", \"Rolling variance\": 2.05425274952755}, {\"Date\": \"2020-11-01T00:00:00\", \"Rolling variance\": 1.773369377865406}, {\"Date\": \"2020-12-01T00:00:00\", \"Rolling variance\": 1.502234915933664}, {\"Date\": \"2021-01-01T00:00:00\", \"Rolling variance\": 1.354634457276118}, {\"Date\": \"2021-02-01T00:00:00\", \"Rolling variance\": 1.354634457276118}, {\"Date\": \"2021-03-01T00:00:00\", \"Rolling variance\": 1.360856385655839}, {\"Date\": \"2021-04-01T00:00:00\", \"Rolling variance\": 1.2514616206628593}, {\"Date\": \"2021-05-01T00:00:00\", \"Rolling variance\": 0.9507296313920921}, {\"Date\": \"2021-06-01T00:00:00\", \"Rolling variance\": 0.9596303511101716}, {\"Date\": \"2021-07-01T00:00:00\", \"Rolling variance\": 0.9326217451935803}, {\"Date\": \"2021-08-01T00:00:00\", \"Rolling variance\": 0.8220725284473313}, {\"Date\": \"2021-09-01T00:00:00\", \"Rolling variance\": 0.8700133633728911}, {\"Date\": \"2021-10-01T00:00:00\", \"Rolling variance\": 0.8979173325603761}, {\"Date\": \"2021-11-01T00:00:00\", \"Rolling variance\": 0.9134170676166065}, {\"Date\": \"2021-12-01T00:00:00\", \"Rolling variance\": 0.9739873124109827}, {\"Date\": \"2022-01-01T00:00:00\", \"Rolling variance\": 1.1231717076107957}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6WygvBc8uGuq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}